commit fc2bd991cca680fb9579913765f2d7cf82130c75
Author: Supriya Rao <supriyar@fb.com>
Date:   Wed Oct 28 15:46:23 2020 -0700

    [quant] Fix flaky test test_histogram_observer_against_reference (#46957)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/46957
    
    Possibly due to use of large tensor in hypothesis. Reducing the size to see if it helps
    
    Test Plan:
    python test/test_quantization.py TestRecordHistogramObserver.test_histogram_observer_against_reference
    
    Imported from OSS
    
    Reviewed By: jerryzh168
    
    Differential Revision: D24580137
    
    fbshipit-source-id: f44ab059796fba97cccb12353c13803bf49214a1

commit cb8b8bb143365da049c03d8a01079e3f5806f37d
Author: Supriya Rao <supriyar@fb.com>
Date:   Tue Oct 27 16:13:58 2020 -0700

    [quant] Fix flaky test test_histogram_observer_against_reference
    Summary:
    Possibly due to use of large tensor in hypothesis. Reducing the size to see if it helps
    
    Test Plan:
    python test/test_quantization.py TestRecordHistogramObserver.test_histogram_observer_against_reference
    
    Reviewers:
    
    Subscribers:
    
    Tasks:
    
    Tags:
    
    ghstack-source-id: 58dc6343895119e189b27a48ef1f9372c96d18d6
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/46957

commit 4549628f1529cd5da401b4692db2d6bc7b036089
Author: Supriya Rao <supriyar@fb.com>
Date:   Tue Oct 27 16:13:46 2020 -0700

    [quant] Fix flaky test test_histogram_observer_against_reference
    Summary:
    Possibly due to use of large tensor in hypothesis. Reducing the size to see if it helps
    
    Test Plan:
    python test/test_quantization.py TestRecordHistogramObserver.test_histogram_observer_against_reference
    
    Reviewers:
    
    Subscribers:
    
    Tasks:
    
    Tags:
    
    [ghstack-poisoned]

commit 115bbf9945dfef34d036c07bd329d800ede0c51b
Author: Dmytro Dzhulgakov <dzhulgakov@fb.com>
Date:   Tue Oct 27 16:01:28 2020 -0700

    [caffe2] Disable running full grad check in tests by default
    
    Summary:
    We've been seeing a lot of Hypothesis timeouts and from profiling a few of the failing tests one of the contributing factors is really slow grad checker. In short, it launches the whole op for each of the input elements so the overall complexity is O(numel^2) at least.
    
    This applies a very unscientific hack to just run grad check on the first and last few elements. It's not ideal, but it's better than flaky tests. One can still explicitly opt in with the env var.
    
    Reviewed By: malfet
    
    Differential Revision: D23336220
    
    fbshipit-source-id: f04d8d43c6aa1590c2f3e72fc7ccc6aa674e49d2

commit 151f31ba278b39a790044b0e1f60d14aeb71ad6f
Author: Jeff Daily <jeff.daily@amd.com>
Date:   Tue Oct 27 14:19:32 2020 -0700

    remove event not ready assertion from TestCuda.test_copy_non_blocking (#46857)
    
    Summary:
    It is incorrect to assume that a newly recorded event will immediately query as False.
    This test is flaky on ROCm due to this incorrect assumption.
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/46857
    
    Reviewed By: albanD
    
    Differential Revision: D24565581
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 0e9ba02cf52554957b29dbeaa5093696dc914b67

commit fe5c7bd58154c78cd731ca5899c2c48ca64e1f5d
Author: Xiang Gao <qasdfgtyuiop@gmail.com>
Date:   Fri Oct 23 10:55:55 2020 -0700

    Fix some flaky tests

commit f5e70a750464fc3b022cecb57f1bfe19242a3884
Author: Rong Rong <rongr@fb.com>
Date:   Tue Oct 6 17:29:57 2020 -0700

    fix test flakiness caused by sys.getrefcount(None) (#45876)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45876
    
    sys.getrefcount() can be flaky before/after scope() call
    
    Test Plan: buck test mode/opt-asan //caffe2/test:others -- 'test_none_names_refcount \(test_namedtensor\.TestNamedTensor\)' --run-disabled
    
    Reviewed By: malfet
    
    Differential Revision: D24123724
    
    fbshipit-source-id: 4af0b150222cfb92dd0776a42fcab44d896a772a

commit bf85642c4c6e257587c50be8de108199be5d0396
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Mon Oct 5 19:59:52 2020 -0700

    Remove lock from GraphTask::set_exception_without_signal. (#45867)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45867
    
    In most cases the lock ordering was hold a lock in local autograd and
    then hold a lock in DistAutogradContext.
    
    In case of `set_exception_without_signal` the lock order was in reverse and as
    a result we saw potential deadlock issues in our TSAN tests. To fix this, I
    removed the lock and instead just used std::atomic exchange.
    
    In addition to this, I fixed TestE2E to ensure that we use the appropriate
    timeout.
    
    TestE2EProcessGroup was flaky for these two reasons and now is fixed.
    ghstack-source-id: 113592709
    
    Test Plan: waitforbuildbot.
    
    Reviewed By: albanD
    
    Differential Revision: D24120962
    
    fbshipit-source-id: 12447b84ceae772b91e9a183c90d1e6340f44e66

commit 7eef68a929810e5d53efbe3d61ad7d0c875439f0
Merge: 386e1eb 2cf1b3c
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:09:04 2020 -0700

    Update on "[RPC tests] Have GDB print the stack traces of all threads upon deadlock"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with deadlocks, where the processes stop, don't print any logs, and are eventually killed by the test runner, leaving few hints as to where they were and why they weren't progressing. One useful piece of information to understand deadlocks comes from TensorPipe's verbose logging trace, which we enabled in an earlier PR in this stack. Another useful tool is having the stack traces of all the threads at the time the deadlock occurred. One easy way of obtaining it is to use GDB, which is what this PR here does. (If GDB isn't installed it fails graciously).
    
    Differential Revision: [D23963479](https://our.internmc.facebook.com/intern/diff/D23963479/)
    
    [ghstack-poisoned]

commit 41973746dc192f7ca620eb8d626031bbc8fd9c66
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:09:04 2020 -0700

    [RPC tests] Have GDB print the stack traces of all threads upon deadlock
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45442
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with deadlocks, where the processes stop, don't print any logs, and are eventually killed by the test runner, leaving few hints as to where they were and why they weren't progressing. One useful piece of information to understand deadlocks comes from TensorPipe's verbose logging trace, which we enabled in an earlier PR in this stack. Another useful tool is having the stack traces of all the threads at the time the deadlock occurred. One easy way of obtaining it is to use GDB, which is what this PR here does. (If GDB isn't installed it fails graciously).
    ghstack-source-id: 113114959
    
    Differential Revision: [D23963479](https://our.internmc.facebook.com/intern/diff/D23963479/)

commit 2cf1b3ca490460622eee22f3146af1b05dfcd19c
Merge: f031b24 4f5c814
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:09:01 2020 -0700

    Update on "[RPC tests] Log backtrace in case of SEGFAULT"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with segfaults, which cause immediate crashes without any error message or backtrace being printed, which makes them very hard to pinpoint and to associate to the code at fault.
    
    There exists however a nice library which is (practically?) default on Linux which when (pre-)loaded into an executable installs a segfault handler which, when triggered, prints the stack trace, the content of registries, and other information. This should help us automatically gather more information from segfaults.
    
    Differential Revision: [D23963478](https://our.internmc.facebook.com/intern/diff/D23963478/)
    
    [ghstack-poisoned]

commit 0ad64fc35cf3b890607bb572ecf1657ffbf7e99b
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:09:01 2020 -0700

    [RPC tests] Log backtrace in case of SEGFAULT
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45441
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with segfaults, which cause immediate crashes without any error message or backtrace being printed, which makes them very hard to pinpoint and to associate to the code at fault.
    
    There exists however a nice library which is (practically?) default on Linux which when (pre-)loaded into an executable installs a segfault handler which, when triggered, prints the stack trace, the content of registries, and other information. This should help us automatically gather more information from segfaults.
    ghstack-source-id: 113114957
    
    Differential Revision: [D23963478](https://our.internmc.facebook.com/intern/diff/D23963478/)

commit 4f5c8145a2363bdd0e7011ff25da513d90e0ef28
Merge: 5edcb08 642a7f8
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:08:58 2020 -0700

    Update on "[RPC tests] Print verbose logging for TensorPipe"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One very useful piece of information which we'd love to have for the TensorPipe agent is its verbose logs. They are divided into many levels and printing all of them is probably quite impractical as that would create a bloat in the logs and a slowdown of the tests. A hopefully decent compromise is to only print the first level, which gives details about the interface between TensorPipe and PyTorch: which methods of TensorPipe are called by PyTorch, and which callbacks are invoked. This should hopefully not be too big, and would at least help "bisect" the issue into whether it's in TensorPipe or in the agent.
    
    Differential Revision: [D23963480](https://our.internmc.facebook.com/intern/diff/D23963480/)
    
    [ghstack-poisoned]

commit f0a49013fe123b66c84c9f92ae329a2ec7cd5660
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 04:08:58 2020 -0700

    [RPC tests] Print verbose logging for TensorPipe
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45440
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One very useful piece of information which we'd love to have for the TensorPipe agent is its verbose logs. They are divided into many levels and printing all of them is probably quite impractical as that would create a bloat in the logs and a slowdown of the tests. A hopefully decent compromise is to only print the first level, which gives details about the interface between TensorPipe and PyTorch: which methods of TensorPipe are called by PyTorch, and which callbacks are invoked. This should hopefully not be too big, and would at least help "bisect" the issue into whether it's in TensorPipe or in the agent.
    ghstack-source-id: 113114956
    
    Differential Revision: [D23963480](https://our.internmc.facebook.com/intern/diff/D23963480/)

commit 386e1ebea12a7b284fe24f618afe7015bdfee28b
Merge: ac6a38d f031b24
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 02:39:42 2020 -0700

    Update on "[RPC tests] Have GDB print the stack traces of all threads upon deadlock"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with deadlocks, where the processes stop, don't print any logs, and are eventually killed by the test runner, leaving few hints as to where they were and why they weren't progressing. One useful piece of information to understand deadlocks comes from TensorPipe's verbose logging trace, which we enabled in an earlier PR in this stack. Another useful tool is having the stack traces of all the threads at the time the deadlock occurred. One easy way of obtaining it is to use GDB, which is what this PR here does. (If GDB isn't installed it fails graciously).
    
    Differential Revision: [D23963479](https://our.internmc.facebook.com/intern/diff/D23963479/)
    
    [ghstack-poisoned]

commit f031b24e9fbba75825cf424ae199d9d2e563f620
Merge: e641309 5edcb08
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 02:39:39 2020 -0700

    Update on "[RPC tests] Log backtrace in case of SEGFAULT"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with segfaults, which cause immediate crashes without any error message or backtrace being printed, which makes them very hard to pinpoint and to associate to the code at fault.
    
    There exists however a nice library which is (practically?) default on Linux which when (pre-)loaded into an executable installs a segfault handler which, when triggered, prints the stack trace, the content of registries, and other information. This should help us automatically gather more information from segfaults.
    
    Differential Revision: [D23963478](https://our.internmc.facebook.com/intern/diff/D23963478/)
    
    [ghstack-poisoned]

commit 5edcb08a0d16a18816869cda7fe29d71768d11c8
Merge: 0c91b85 39b5b64
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Tue Sep 29 02:39:36 2020 -0700

    Update on "[RPC tests] Print verbose logging for TensorPipe"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One very useful piece of information which we'd love to have for the TensorPipe agent is its verbose logs. They are divided into many levels and printing all of them is probably quite impractical as that would create a bloat in the logs and a slowdown of the tests. A hopefully decent compromise is to only print the first level, which gives details about the interface between TensorPipe and PyTorch: which methods of TensorPipe are called by PyTorch, and which callbacks are invoked. This should hopefully not be too big, and would at least help "bisect" the issue into whether it's in TensorPipe or in the agent.
    
    Differential Revision: [D23963480](https://our.internmc.facebook.com/intern/diff/D23963480/)
    
    [ghstack-poisoned]

commit ac6a38d669d5019b9bf2637220dc5f0b63fe27b1
Merge: 8d23d9a e641309
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 10:18:59 2020 -0700

    Update on "[RPC tests] Have GDB print the stack traces of all threads upon deadlock"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with deadlocks, where the processes stop, don't print any logs, and are eventually killed by the test runner, leaving few hints as to where they were and why they weren't progressing. One useful piece of information to understand deadlocks comes from TensorPipe's verbose logging trace, which we enabled in an earlier PR in this stack. Another useful tool is having the stack traces of all the threads at the time the deadlock occurred. One easy way of obtaining it is to use GDB, which is what this PR here does. (If GDB isn't installed it fails graciously).
    
    Differential Revision: [D23963479](https://our.internmc.facebook.com/intern/diff/D23963479/)
    
    [ghstack-poisoned]

commit e641309f3c430e2c4328878d5ef13e9c3929e1d8
Merge: 435e3f2 0c91b85
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 10:18:56 2020 -0700

    Update on "[RPC tests] Log backtrace in case of SEGFAULT"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with segfaults, which cause immediate crashes without any error message or backtrace being printed, which makes them very hard to pinpoint and to associate to the code at fault.
    
    There exists however a nice library which is (practically?) default on Linux which when (pre-)loaded into an executable installs a segfault handler which, when triggered, prints the stack trace, the content of registries, and other information. This should help us automatically gather more information from segfaults.
    
    Differential Revision: [D23963478](https://our.internmc.facebook.com/intern/diff/D23963478/)
    
    [ghstack-poisoned]

commit 0c91b855ac5f581450e63e9aa540f0b5f70fb476
Merge: 0fcb8da cf7f0f7
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 10:18:53 2020 -0700

    Update on "[RPC tests] Print verbose logging for TensorPipe"
    
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One very useful piece of information which we'd love to have for the TensorPipe agent is its verbose logs. They are divided into many levels and printing all of them is probably quite impractical as that would create a bloat in the logs and a slowdown of the tests. A hopefully decent compromise is to only print the first level, which gives details about the interface between TensorPipe and PyTorch: which methods of TensorPipe are called by PyTorch, and which callbacks are invoked. This should hopefully not be too big, and would at least help "bisect" the issue into whether it's in TensorPipe or in the agent.
    
    Differential Revision: [D23963480](https://our.internmc.facebook.com/intern/diff/D23963480/)
    
    [ghstack-poisoned]

commit 8d23d9a1be512b167d1f58ee3fa6bf57b8d5a2c3
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 09:33:08 2020 -0700

    [RPC tests] Have GDB print the stack traces of all threads upon deadlock
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with deadlocks, where the processes stop, don't print any logs, and are eventually killed by the test runner, leaving few hints as to where they were and why they weren't progressing. One useful piece of information to understand deadlocks comes from TensorPipe's verbose logging trace, which we enabled in an earlier PR in this stack. Another useful tool is having the stack traces of all the threads at the time the deadlock occurred. One easy way of obtaining it is to use GDB, which is what this PR here does. (If GDB isn't installed it fails graciously).
    
    Differential Revision: [D23963479](https://our.internmc.facebook.com/intern/diff/D23963479/)
    
    [ghstack-poisoned]

commit 435e3f23b7f331c8cb5d155dd58252759c2bd3d0
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 09:32:57 2020 -0700

    [RPC tests] Log backtrace in case of SEGFAULT
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One notable instance of this occurs with segfaults, which cause immediate crashes without any error message or backtrace being printed, which makes them very hard to pinpoint and to associate to the code at fault.
    
    There exists however a nice library which is (practically?) default on Linux which when (pre-)loaded into an executable installs a segfault handler which, when triggered, prints the stack trace, the content of registries, and other information. This should help us automatically gather more information from segfaults.
    
    Differential Revision: [D23963478](https://our.internmc.facebook.com/intern/diff/D23963478/)
    
    [ghstack-poisoned]

commit 0fcb8da17b515ae5471bb8b78fdb9187a62ee4f5
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Mon Sep 28 09:32:46 2020 -0700

    [RPC tests] Print verbose logging for TensorPipe
    
    Some of our tests are flaky but when they fail they print very little information which makes it hard to diagnose the issue. One very useful piece of information which we'd love to have for the TensorPipe agent is its verbose logs. They are divided into many levels and printing all of them is probably quite impractical as that would create a bloat in the logs and a slowdown of the tests. A hopefully decent compromise is to only print the first level, which gives details about the interface between TensorPipe and PyTorch: which methods of TensorPipe are called by PyTorch, and which callbacks are invoked. This should hopefully not be too big, and would at least help "bisect" the issue into whether it's in TensorPipe or in the agent.
    
    Differential Revision: [D23963480](https://our.internmc.facebook.com/intern/diff/D23963480/)
    
    [ghstack-poisoned]

commit bee1d448e76837e7ffc066fcad576ccb98e92ee1
Author: Rohan Varma <rvarm1@fb.com>
Date:   Thu Sep 24 15:55:35 2020 -0700

    Fix test_rpc_profiling_remote_record_function (#45162)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45162
    
    This test was flaky because it was not able to validate that the
    overall record_function's CPU times are greater than the sum of its children.
    It turns out that this is a general bug in the profiler that can be reproduced
    without RPC, see https://github.com/pytorch/pytorch/issues/45160. Hence,
    removing this from the test and replacing it by just validating the expected
    children.
    
    Ran the test 1000 times and they all passed.
    ghstack-source-id: 112632327
    
    Test Plan: CI
    
    Reviewed By: mrshenli
    
    Differential Revision: D23851854
    
    fbshipit-source-id: 5d9023acd17800a6668ba4849659d8cc902b8d6c

commit 3f5eee666cfbb5cbc5ced32915658e00b39b40e9
Author: Gao, Xiang <qasdfgtyuiop@gmail.com>
Date:   Thu Sep 24 10:23:46 2020 -0700

    Adjust TF32 tests (#44240)
    
    Summary:
    - The thresholds of some tests are bumped up. Depending on the random generator, sometimes these tests fail with things like 0.0059 is not smaller than 0.005. I ran `test_nn.py` and `test_torch.py` for 10+ times to check these are no longer flaky.
    - Add `tf32_on_and_off` to new `matrix_exp` tests.
    - Disable TF32 on test suites other than `test_nn.py` and `test_torch.py`
    
    cc: ptrblck
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/44240
    
    Reviewed By: mruberry
    
    Differential Revision: D23882498
    
    Pulled By: ngimel
    
    fbshipit-source-id: 44a9ec08802c93a2efaf4e01d7487222478b6df8

commit 8507ea22b21842f93a7d17ddfe737f134642375c
Author: Taylor Robie <taylorrobie@fb.com>
Date:   Thu Sep 24 09:36:46 2020 -0700

    replace timer test with a mocked variant (#45173)
    
    Summary:
    I noticed that the recently introduced adaptive_autorange tests occasionally timeout CI, and I've been meaning to improve the Timer tests for a while. This PR allows unit tests to swap the measurement portion of `Timer` with a deterministic mock so we can thoroughly test behavior without having to worry about flaky CI measurements. It also means that the tests can be much more detailed and still finish very quickly.
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/45173
    
    Test Plan: You're lookin' at it.
    
    Reviewed By: ezyang
    
    Differential Revision: D23873548
    
    Pulled By: robieta
    
    fbshipit-source-id: 26113e5cea0cbf46909b9bf5e90c878c29e87e88

commit 4bbb6adff590a0d813589a2e856db786308f2a61
Author: Nick Gibson <nickg@fb.com>
Date:   Mon Sep 21 09:25:53 2020 -0700

    [NNC] fix SyncThreads insertion and reenable CudaSharedMem test (#44909)
    
    Summary:
    A previous fix for masking Cuda dimensions (https://github.com/pytorch/pytorch/issues/44733) changed the behaviour of inserting thread synchronization barriers in the Cuda CodeGen, causing the CudaSharedMemReduce_1 to be flaky and ultimately disabled.
    
    The issue is working out where these barriers must be inserted - solving this optimally is very hard, and I think not possible without dependency analysis we don't have, so I've changed our logic to be quite pessimistic. We'll insert barriers before and after any blocks that have thread dimensions masked (even between blocks that have no data dependencies). This should be correct, but it's an area we could improve performance. To address this somewhat I've added a simplifier pass that removes obviously unnecessary syncThreads.
    
    To avoid this test being flaky again, I've added a check against the generated code to ensure there is a syncThread in the right place.
    
    Also fixed a couple of non-functional but clarity issues in the generated code: fixed the missing newline after Stores in the CudaPrinter, and prevented the PrioritizeLoad mutator from pulling out loads contained within simple Let statements (such as those produced by the Registerizer).
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/44909
    
    Reviewed By: agolynski
    
    Differential Revision: D23800565
    
    Pulled By: nickgg
    
    fbshipit-source-id: bddef1f40d8d461da965685f01d00b468d8a2c2f

commit 21a1b9c7cff40d46fc4f9ea391e702d4052ffa41
Author: Rong Rong <rongr@fb.com>
Date:   Fri Sep 18 18:49:52 2020 -0700

    skip more nccl tests that causes flaky timeouts on rocm build (#44996)
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44996
    
    Reviewed By: malfet
    
    Differential Revision: D23797564
    
    Pulled By: walterddr
    
    fbshipit-source-id: 4d60f76bb8ae54bb04a9f4143a68623933461b2a

commit a40ef25e301ca6a50a955fec1d763de8570b9483
Author: Bert Maher <bertrand@fb.com>
Date:   Thu Sep 17 07:48:39 2020 -0700

    [te] Disable flaky test CudaSharedMemReduce_1 (#44862)
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44862
    
    Test Plan: Imported from OSS
    
    Reviewed By: ZolotukhinM
    
    Differential Revision: D23753831
    
    Pulled By: bertmaher
    
    fbshipit-source-id: d7d524ac34e4ca208df022a5730c2d11b3068f12

commit c4e5ab6ff29de7905d296eadc0ee9383cd64382f
Author: Mikhail Zolotukhin <mvz@fb.com>
Date:   Wed Aug 26 18:33:50 2020 -0700

    [TensorExpr] Disable a flaky test. (#43678)
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43678
    
    Test Plan: Imported from OSS
    
    Reviewed By: Krovatkin
    
    Differential Revision: D23363651
    
    Pulled By: ZolotukhinM
    
    fbshipit-source-id: 9557fbfda28633cea169836b02d034e9c950bc71

commit 6797f81e6f8ac856ceac4c4d03b5f42dd4efa3ce
Author: albanD <desmaison.alban@gmail.com>
Date:   Thu Aug 13 18:48:31 2020 -0400

    remove test flakyness as it's not used anymore

commit f9a766bb39e65efe438730adb8e047117f439f7c
Author: Sean Lynch <swlynch@fb.com>
Date:   Thu Aug 20 08:36:20 2020 -0700

    Increase deadline time for load_save tests (#43205)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/43205
    
    A number of tests that forward to `TestLoadSaveBase.load_save` are all marked as flaky due to them regularly taking much longer to start up than hypothesis' default timeout of 200ms. This diff fixes the problem by removing the timeout for `load_save`. This is alright as these tests aren't meant to be testing the performance of these operators.
    
    I would set the deadline to 60s if I could however it appears the that caffe2 github CI uses a different version of hypothesis that doesn't allow using `dateutil.timedelta` so instead of trying to figure out an approach that works on both I've just removed the deadline time.
    
    I've also tagged all existing tasks WRT these failures.
    
    Differential Revision: D23175752
    
    fbshipit-source-id: 324f9ff034df1ac4874797f04f50067149a6ba48

commit 3bf2978497cb1f03d40de94f793d86b11fd12316
Author: Hector Yuen <hyz@fb.com>
Date:   Tue Aug 11 14:10:43 2020 -0700

    remove deadline enforcement for hypothesis (#42871)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/42871
    
    old version of hypothesis.testing was not enforcing deadlines
    after the library got updated, default deadline=200ms, but even with 1s or
    more, tests are flaky. Changing deadline to non-enforced which is the same
    behavior as the old version
    
    Test Plan: tested fakelowp/tests
    
    Reviewed By: hl475
    
    Differential Revision: D23059033
    
    fbshipit-source-id: 79b6aec39a2714ca5d62420c15ca9c2c1e7a8883

commit c30bc6d4d7b54296179d717c15cd7507ba520bae
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Thu Aug 6 02:11:57 2020 -0700

    Update TensorPipe submodule (#42522)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/42522
    
    Main changes:
    - Consolidated CMake files to have a single entry point, rather than having a specialized one for PyTorch.
    - Changed the way the preprocessor flags are provided, and changed their name.
    
    There were a few instances in PyTorch's CMake files where we were directly adding TensorPipe's source directory as an include path, which however doesn't contain the auto-generated header we now added. We fix that by adding the `tensorpipe` CMake target as a dependency, so that the include paths defined by TensorPipe are used, which contain that auto-generated header. So instead we link those targets to the tensorpipe target in order for them to pick up the correct include directories.
    
    I'm turning off SHM and CMA for now because they have never been covered by the CI. I'll enable them in a separate PR so that if they turn out to be flaky we can revert that change without reverting this one.
    
    Test Plan: CI
    
    Reviewed By: malfet
    
    Differential Revision: D22959472
    
    fbshipit-source-id: 1959a41c4a66ef78bf0f3bd5e3964969a2a1bf67

commit d7516ccfac08c389aeb1cf741ba816cf58518ac7
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Enroll TensorPipe in missing test suites (#40823)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40823
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    As it is now easier to spot that the TensorPipe agent wasn't being run on some test suite, we fix that. We keep this change for last so that if those tests turn out to be flaky and must be reverted this won't affect the rest of the stack.
    ghstack-source-id: 109229469
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22309432
    
    fbshipit-source-id: c433a6a49a7b6737e0df4cd953f3dfde290f20b8

commit 2e7b464c43a67701dcb77ecb1df67d56751648b8
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Remove global TEST_CONFIG (#40822)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40822
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This is the last step of removing TEST_CONFIG. As there was no one left using it, there is really not much to it.
    ghstack-source-id: 109229471
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22307778
    
    fbshipit-source-id: 0d9498d9367eec671e0a964ce693015f73c5638c

commit e7c7eaab82c960b9b880f9e9e2d6c33a137d9b69
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Move some functions to methods of fixture (#40821)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40821
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This change continues the work towards removing TEST_CONFIG, by taking a few functions that were accepting the agent name (as obtained from TEST_CONFIG) and then did a bunch of if/elses on it, and replace them by new abstract methods on the fixtures, so that these functions become "decentralized".
    ghstack-source-id: 109229472
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22307776
    
    fbshipit-source-id: 9e1f6edca79aacf0bcf9d83d50ce9e0d2beec0dd

commit 2acef69ce3e072c511aeb585f729dfb753444411
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Make generic fixture an abstract base class (#40820)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40820
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    Now that no one is using the generic fixture anymore (i.e., the fixture that looks up the agent's name in the global TEST_CONFIG) we can make it abstract, i.e., have its methods become no-ops and add decorators that will require all subclasses to provide new implementations of those methods. This is a first step towards removing TEST_CONFIG.
    ghstack-source-id: 109229475
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22307777
    
    fbshipit-source-id: e52abd915c37894933545eebdfdca3ecb9559926

commit a94039fce598538cb8b75e107f9c7926f09d7ba1
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Avoid decorators to skip tests (#40819)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40819
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This diff removes the two decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which were used to skip tests. They were only used to prevent the TensorPipe agent from running tests that were using the process group agent's options. The converse (preventing the PG agent from using the TP options) is achieved by having those tests live in a `TensorPipeAgentRpcTest` class. So here we're doing the same for process group, by moving those tests to a `ProcessGroupAgentRpcTest` class.
    ghstack-source-id: 109229473
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22283179
    
    fbshipit-source-id: b9315f9fd67f35e88fe1843faa161fc53a4133c4

commit 935fcc9580948b6b80b5d7931290a8afa985a492
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Merge process group tests into single entry point (#40818)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40818
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This diff does the changes described above for the process group agent. It defines a fixture for it (instead of using the generic fixture in its default behavior) and then merges all the entry points into a single script. Note that after this change there won't be anymore a "vanilla" RPC test: all test scripts now specify what agent they are using. This puts all agents on equal standing.
    ghstack-source-id: 109229474
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22283182
    
    fbshipit-source-id: 7e3626bbbf37d88b892077a03725f0598576b370

commit b93c7c54eb6bcd9826d83b7e65a33d7830234815
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Merge tests for faulty agent into single script (#40817)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40817
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This diff does the changes described above for the faulty agent, which is its own strange beast. It merges all the test entry points (i.e., the combinations of agent, suite and fork/spawn) into a single file. It also modifies the test suites that are intended to be run only on the faulty agent, which used to inherit from its fixture, to inherit from the generic fixture, as they will be mixed in with the faulty fixture at the very end, inside the entry point script.
    ghstack-source-id: 109229477
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22283178
    
    fbshipit-source-id: 72659efe6652dac8450473642a578933030f2c74

commit edf6c4bc4dbd4e42c18033a7d9df9c2025ab5805
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Wed Aug 5 15:01:13 2020 -0700

    [RPC tests] Merge TensorPipe tests into single entry point (#40816)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40816
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This diff does the changes described above for the TensorPipe agent. It fixes its fixture (making it inherit from the generic fixture) and merges all the entry point scripts into a single one, so that it's easier to have a clear overview of all the test suites which we run on TensorPipe (you'll notice that many are missing: the JIT ones, the remote module one, ...).
    ghstack-source-id: 109229476
    
    Test Plan: Sandcastle and CircleCI
    
    Reviewed By: pritamdamania87
    
    Differential Revision: D22283180
    
    fbshipit-source-id: d5e9f9f4e6d4bfd6fbcae7ae56eed63d2567a02f

commit 2335430086fd1e23db161f5f0782d8b04885702d
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Thu Jul 30 02:31:12 2020 -0700

    Update TensorPipe submodule (#42225)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/42225
    
    Main changes:
    - Consolidated CMake files to have a single entry point, rather than having a specialized one for PyTorch.
    - Changed the way the preprocessor flags are provided, and changed their name.
    
    There were a few instances in PyTorch's CMake files where we were directly adding TensorPipe's source directory as an include path, which however doesn't contain the auto-generated header we now added. We fix that by adding the `tensorpipe` CMake target as a dependency, so that the include paths defined by TensorPipe are used, which contain that auto-generated header.
    
    I'm turning off SHM and CMA for now because they have never been covered by the CI. I'll enable them in a separate PR so that if they turn out to be flaky we can revert that change without reverting this one.
    
    Test Plan: CircleCI is all green.
    
    Reviewed By: beauby
    
    Differential Revision: D22812445
    
    fbshipit-source-id: e6d824bb28f5afe75fd765de0430968174f3531f

commit 8deb4fe809ca956276e8d6edaa184de7118be58f
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Tue Jul 28 18:36:52 2020 -0700

    Fix flaky NCCL error handling tests. (#42149)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/42149
    
    Some of these tests were flaky since we could kill the process in some
    way without cleaning up the ProcessGroup. This resulted in issues where the
    FileStore didn't clean up appropriately resulting in other processes in the
    group to crash.
    
    Fixed this by explicitly deleting the process_group before we bring a process
    down forcibly.
    ghstack-source-id: 108629057
    
    Test Plan: waitforbuildbot
    
    Reviewed By: mrshenli
    
    Differential Revision: D22785042
    
    fbshipit-source-id: c31d0f723badbc23b7258e322f75b57e0a1a42cf

commit 1f11e930d0e537be0edec4bbfa52909cfe2585cc
Author: lcskrishna <lollachaitanya@gmail.com>
Date:   Tue Jul 21 08:53:13 2020 -0700

    [ROCm] skip test_streams on rocm. (#41697)
    
    Summary:
    Skipping the test test_streams as it is flaky on rocm.
    cc: jeffdaily  sunway513
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/41697
    
    Reviewed By: zhangguanheng66
    
    Differential Revision: D22644600
    
    Pulled By: malfet
    
    fbshipit-source-id: b1b16d496e58a91c44c40d640851fd62a5d7393d

commit b5e32528d04e711f60ae2a7bfdd4b3b749f1a9b5
Author: Rohan Varma <rvarm1@fb.com>
Date:   Thu Jul 16 11:17:31 2020 -0700

    Fix flaky test_udf_remote_message_delay_timeout_to_self (#41217)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/41217
    
    Fixes this flaky test. Due to the possibility of callback
    finishCreatingOwnerRRef running after request_callback has processed and
    created the owner RRef, we could actually end up with 0 owners on the node,
    since the callback removes from the owners_ map. In this case, shutdown is fine
    since there are no owners. On the other hand, if the callback runs first, there
    will be 1 owner which we will delete in shutdown when we detect it has no
    forks. So either way, shutdown works fine and we don't need to enforce there to
    be 1 owner.
    ghstack-source-id: 107883497
    
    Test Plan: Ran the test 500 times with TSAN.
    
    Reviewed By: ezyang
    
    Differential Revision: D22469806
    
    fbshipit-source-id: 02290d6d5922f91a9e2d5ede21d1cf1c4598cb46

commit 563b60b8901c23a5239f2533744d2d4b03a11ca4
Author: Alexander Grund <alexander.grund@tu-dresden.de>
Date:   Wed Jul 15 10:50:40 2020 -0700

    Fix flaky test_stream_event_nogil due to missing event sync (#41398)
    
    Summary:
    The test asserts that the stream is "ready" but doesn't wait for the
    event to be "executed" which makes it fail on some platforms where the
    `query` call occurs "soon enough".
    
    Fixes https://github.com/pytorch/pytorch/issues/38807
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/41398
    
    Reviewed By: zhangguanheng66
    
    Differential Revision: D22540012
    
    Pulled By: ezyang
    
    fbshipit-source-id: 6f56d951e48133ce4f6a9a54534298b7d2877c80

commit fd0329029f8b6ff3955d5b1b3e3f22ff9ab22cb0
Author: Rohan Varma <rvarm1@fb.com>
Date:   Tue Jul 14 19:21:27 2020 -0700

    Fix flaky profiler and test_callback_simple RPC tests (#41287)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/41287
    
    Profiler tests that test profiling with builtin functions and `test_callback_simple` test has been broken for a while. This diff fixes that by preferring c10 ops to non-c10 ops in our operation matching logic.
    
    The result of this is that these ops go through the c10 dispatch and thus have profiling enabled. For `test_callback_simple` this results in the effect that we choose `aten::add.Tensor` over `aten::add.Int` which fixes the type issue.
    
    Test Plan:
    Ensured that the tests are no longer flaky by running them a bunch
    of times.
    
    Reviewed By: vincentqb
    
    Differential Revision: D22489197
    
    fbshipit-source-id: 8452b93e4d45703453f77d968350c0d32f3f63fe

commit 05207b7371bbaa072c2d8f3469984312b21d50ca
Author: Eli Uriegas <eliuriegas@fb.com>
Date:   Tue Jul 14 14:46:41 2020 -0700

    .circleci: Re-split postnightly into its own thing (#41354)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/41354
    
    The nightly pipeline has the potential to be flaky and thus the html
    pages have the potential not to be updated.
    
    This should actually be done as an automatic lambda job that runs
    whenever the S3 bucket updates but this is intermediate step in order to
    get there.
    
    Closes https://github.com/pytorch/pytorch/issues/40998
    
    Signed-off-by: Eli Uriegas <eliuriegas@fb.com>
    
    Test Plan: Imported from OSS
    
    Reviewed By: ezyang
    
    Differential Revision: D22530283
    
    Pulled By: seemethere
    
    fbshipit-source-id: 0d80b7751ede83e6dd466690cc0a0ded68f59c5d

commit f083cea22789db8f80876033c15cf239e41399d5
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Fri Jul 3 06:16:29 2020 -0700

    [RPC tests] Fix file descriptor leak (#40913)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40913
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    Once we start merging multiple test suites in a single file (which we'll happen in the next diffs in the stack) the OSX tests on CircleCI start failing due to "too many open files". This indicates a file descriptor leak. I then managed to repro it on Linux too by lowering the limit on open file descriptors (`ulimit -n 500`). Each test method that unittest runs is run on a new instance of the Testcase class. With our multiprocessing wrappers, this instance contains a list of child processes. Even after these processes are terminated, it appears they still hold some open file descriptor (for example a pipe to communicate with the subprocess). It also appears unittest is keeping these Testcase instances alive until the entire suite completes, which I suspect is what leads to this "leak" of file descriptors. Based on that guess, in this diff I am resetting the list of subprocesses during shutdown, and this seems to fix the problem.
    ghstack-source-id: 107045908
    
    Test Plan: Sandcastle and CircleCI
    
    Differential Revision: D22356784
    
    fbshipit-source-id: c93bb9db60fde72cae0b0c735a50c17e427580a6

commit f9a71d3de4d82da9fe77ff73a2f3097a8c2dcb74
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Fri Jul 3 06:16:29 2020 -0700

    [RPC tests] Align ddp_under_dist_autograd test with others (#40815)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40815
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This prepares the stack by aligning the `ddp_under_dist_autograd` test to the other ones, so that later changes will be more consistent and thus easier to follow. It does so by moving the `skipIf` decorators and the `setUp` methods from the base test suite to the entry point scripts.
    ghstack-source-id: 107045911
    
    Test Plan: Sandcastle and CircleCI
    
    Differential Revision: D22287535
    
    fbshipit-source-id: ab0c9eb774b21d81e0ebd3078df958dbb4bfa0c7

commit d0f2079b5e0247c38e731f5e7e7dba835aef644f
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Fri Jul 3 02:44:32 2020 -0700

    [RPC tests] Remove world_size and init_method from TensorPipe fixture (#40814)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40814
    
    Summary of the entire stack:
    --
    
    This diff is part of an attempt to refactor the RPC tests. They currently suffer from several problems:
    - Several ways to specify the agent to use: there exists one "generic" fixture that uses the global variable TEST_CONFIG to look up the agent name, and is used for process group and Thrift, and then there are separate fixtures for the flaky agent and the TensorPipe one.
    - These two ways lead to having two separate decorators (`requires_process_group_agent` and `@_skip_if_tensorpipe_agent`) which must both be specified, making it unclear what the effect of each of them is and what happens if only one is given.
    - Thrift must override the TEST_CONFIG global variable before any other import (in order for the `requires_process_group_agent` decorator to work correctly) and for that it must use a "trap" file, which makes it even harder to track which agent is being used, and which is specific to Buck, and thus cannot be used in OSS by other agents.
    - Even if the TensorPipe fixture doesn't use TEST_CONFIG, it still needs to set it to the right value for other parts of the code to work. (This is done in `dist_init`).
    - There are a few functions in dist_utils.py that return some properties of the agent (e.g., a regexp to match against the error it returns in case of shutdown). These functions are effectively chained if/elses on the various agents, which has the effect of "leaking" some part of the Thrift agent into OSS.
    - Each test suite (RPC, dist autograd/dist optimizer, their JIT versions, remote module, ...) must be run on each agent (or almost; the faulty one is an exception) in both fork and spawn mode. Each of these combinations is a separate file, which leads to a proliferation of scripts.
    - There is no "master list" of what combinations make sense and should be run. Therefore it has happened that when adding new tests or new agents we forgot to enroll them into the right tests. (TensorPipe is still missing a few tests, it turns out).
    - All of these tiny "entry point" files contain almost the same duplicated boilerplate. This makes it very easy to get the wrong content into one of them due to a bad copy-paste.
    
    This refactoring aims to address these problems by:
    - Avoiding global state, defaults/override, traps, if/elses, ... and have a single way to specify the agent, based on an abstract base class and several concrete subclasses which can be "mixed in" to any test suite.
    - Instead of enabling/disabling tests using decorators, the tests that are specific to a certain agent are now in a separate class (which is a subclass of the "generic" test suite) so that they are only picked up by the agent they apply to.
    - Instead of having one separate entry point script for each combination, it uses one entry point for each agent, and in that script it provides a list of all the test suites it wants to run on that agent. And it does that by trying to deduplicate the boilerplate as much as possible. (In fact, the various agent-suite combinations could be grouped in any way, not necessarily by agent as I did here).
    
    It provides further advantages:
    - It puts all the agents on equal standing, by not having any of them be the default, making it thus easier to migrate from process group to TensorPipe.
    - It will make it easier to add more versions of the TensorPipe tests (e.g., one that disables the same-machine backends in order to test the TCP-based ones) without a further duplication of entry points, of boilerplate, ...
    
    Summary of this commit
    --
    This prepares the stack by simplifying the TensorPipe fixture. A comment says that the TensorPipe fixture cannot subclass the generic fixture class as that would lead to a diamond class hierarchy which Python doesn't support (whereas in fact it does), and therefore it copies over two properties that are defined on the generic fixture. However, each class that uses the TensorPipe fixture also inherits from the generic fixture, so there's no need to redefine those properties. And, in fact, by not redefining it we save ourselves some trouble when the TensorPipe fixture would end up overriding another override.
    ghstack-source-id: 107045914
    
    Test Plan: Sandcastle and CircleCI
    
    Differential Revision: D22287533
    
    fbshipit-source-id: 254c38b36ba51c9d852562b166027abacbbd60ef

commit b4b8f5b9d4ec4895de38bc5cd9645b5dc5e06e9a
Author: Nikita Shulga <nshulga@fb.com>
Date:   Wed Jul 1 13:36:50 2020 -0700

    Release GIL during DDP construction. (#40877)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40495
    
    As part of debugging flaky ddp_under_dist_autograd tests, I realized
    we were running into the following deadlock.
    
    1) Rank 0 would go into DDP construction, hold GIL and wait for broadcast in
    DDP construction.
    2) Rank 3 is a little slower and performs an RRef fetch call before the DDP
    construction.
    3) The RRef fetch call is done on Rank 0 and tries to acquire GIL.
    4) We now have a deadlock since Rank 0 is waiting for Rank 3 to enter the
    collective and Rank 3 is waiting for Rank 0 to release GIL.
    ghstack-source-id: 106534442
    
    Test Plan:
    1) Ran ddp_under_dist_autograd 500 times.
    2) waitforbuildbot
    
    Differential Revision: D22205180
    
    fbshipit-source-id: 6afd55342e801b9edb9591ff25158a244a8ea66a
    
    Co-authored-by: Pritam Damania <pritam.damania@fb.com>

commit fc8bca094cc23a2394214c5cdbc8392a3d279e8c
Author: Jeff Daily <jeff.daily@amd.com>
Date:   Fri Jun 26 09:43:41 2020 -0700

    skip_if_rocm test_rnn in test_c10d_spawn.py (#40577)
    
    Summary:
    Test was added a few months back in https://github.com/pytorch/pytorch/issues/36503 but recently became flaky for ROCm.
    
    CC ezyang xw285cornell sunway513
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40577
    
    Differential Revision: D22258196
    
    Pulled By: ezyang
    
    fbshipit-source-id: 8a22b0c17b536b3d42d0382f7737df0f8823ba08

commit 82e9318a16f2a5d38bb367f4817e53ae246eacab
Author: Ilia Cherniavskii <iliacher@fb.com>
Date:   Wed Jun 24 18:16:22 2020 -0700

    Adjust CUDA memory leak test (#40504)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40504
    
    Make CUDA mem liek test not flaky
    
    Test Plan: python test/test_profiler.py
    
    Differential Revision: D22215527
    
    Pulled By: ilia-cher
    
    fbshipit-source-id: 5f1051896342ac50cd3a21ea86ce7487b5f82a19

commit ea06db9466fcdc1b94b44df795056b2af7501346
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Wed Jun 24 16:51:42 2020 -0700

    Release GIL during DDP construction. (#40495)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40495
    
    As part of debugging flaky ddp_under_dist_autograd tests, I realized
    we were running into the following deadlock.
    
    1) Rank 0 would go into DDP construction, hold GIL and wait for broadcast in
    DDP construction.
    2) Rank 3 is a little slower and performs an RRef fetch call before the DDP
    construction.
    3) The RRef fetch call is done on Rank 0 and tries to acquire GIL.
    4) We now have a deadlock since Rank 0 is waiting for Rank 3 to enter the
    collective and Rank 3 is waiting for Rank 0 to release GIL.
    ghstack-source-id: 106534442
    
    Test Plan:
    1) Ran ddp_under_dist_autograd 500 times.
    2) waitforbuildbot
    
    Differential Revision: D22205180
    
    fbshipit-source-id: 6afd55342e801b9edb9591ff25158a244a8ea66a

commit 89ef8f8141c49cc7d96db716a6ef9c11e5655cf2
Author: Jeff Daily <jeff.daily@amd.com>
Date:   Thu Jun 18 15:13:27 2020 -0700

    add test_openmp to ROCM_BLACKLIST (#40204)
    
    Summary:
    This test is flaky for rocm platform.  Add to blacklist until it can be further reviewed.
    
    CC ezyang xw285cornell sunway513
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40204
    
    Differential Revision: D22108295
    
    Pulled By: xw285cornell
    
    fbshipit-source-id: 802444a7b41260edcb6ce393237784f3e6c52a74

commit f4ffe99da5ad7c1c7cf3a2254efc0100fb54304f
Author: Rohan Varma <rvarm1@fb.com>
Date:   Wed Jun 17 15:46:59 2020 -0700

    Fix flaky rref timeout test (#40141)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40141
    
    This rref timeout test could be flaky because we could end up processing `RRefUserDelete` messages on the owner node before processing the to_here message. This would result in a hang in `ProcessGroupAgent::sync()` that eventually results in a timeout.
    
    The rough sequence of what happens is:
    0) Node 0 creates RRef on node 1 with rpc.remote() call
    1) rref.to_here() is called with a timeout. Because of delay injection, the processing of this message can be delayed (this is also technically possible in applications without delay injection)
    2) At some point, callbacks corresponding to rpc.remote() runs and confirms the rref, adding it as a confirmed user
    3) RPC shutdown starts, as part of which we send out RRef user deletes. In this case, 0 sends an RRef user delete to 1, and node 1 removes the owner from the `owners_` field.
    4) The `to_here()` message is finally processed by node 1. But since we have deleted the `owner_`, while processing this message we create a future that will be complete when the owner exists (this is to account for the case of to_here() arriving here rpc.remote). But this future will never complete, since the owner is already deleted, so we hang indefnitely
    
    As a workaround for now, we can force `to_here()` to run before RPC shutdown by adding a blocking `to_here()` call with no timeout.
    
    A more robust, longer-term fix would be to detect if an owner has been previously deleted (such as by an RRefUserDelete). Then, we know that the future corresponding to owner creation on the remote end will never completee, and then we error out when processing a `to_here()`.
    ghstack-source-id: 106036796
    
    Differential Revision: D22084735
    
    fbshipit-source-id: fe7265a4fe201c4d6d2f480f64fe085cd59dbfb2

commit 34e28ede57f1011798a0c7b11f91de7508ed0a2c
Author: Ilia Cherniavskii <iliacher@fb.com>
Date:   Wed Jun 17 15:38:25 2020 -0700

    Fix flaky test (#40175)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/40175
    
    Check that there is an increasing memory usage in the test
    
    Test Plan: CI
    
    Differential Revision: D22098192
    
    Pulled By: ilia-cher
    
    fbshipit-source-id: bbdbc71f66baf18514332a98d8927441c61ebc16

commit 408e158df9c3d3fbac00bd07fbac9c43fb971240
Author: Michael Suo <suo@fb.com>
Date:   Sun Mar 22 18:46:45 2020 -0700

    skip ctc_loss test on Windows (#35069)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35069
    
    It is flaky on Windows only, so disable for now:
    https://github.com/pytorch/pytorch/issues/34870
    
    Test Plan: Imported from OSS
    
    Differential Revision: D20544736
    
    Pulled By: suo
    
    fbshipit-source-id: 49e35a4b4f0d1d20157769a4dff22cb4fe86770c

commit f58cc4b444c99f6c9260fa1e10bdad6befec0237
Author: Luca Wehrstedt <lcw@fb.com>
Date:   Thu May 28 10:42:29 2020 -0700

    [RPC] Fix flaky test by waiting for async rref calls (#39012)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/39012
    
    The `test_rref_context_debug_info` test was flaky with the TensorPipe agent, and I think the issue is the test itself.
    
    What was happening is that on line 1826 the test was clearing a global variable on the remote side which was holding a rref. Even though the RPC call that unset the global variable was synchronous, the messages that the rref context needs to send around to delete that rref are asynchronous. Therefore, sometimes, when we reached line 1845 we saw the following check fail:
    ```
            self.assertEqual(2, int(info["num_owner_rrefs"]))
    ```
    because `num_owner_rrefs` was still 3, as the deletion hadn't yet been processed.
    
    The only way I found to fix it is to add a synchronization step where we wait for all the futures from the rref context to complete. Since we must wait for this to happen on all workers, we synchronize with a barrier.
    ghstack-source-id: 104810738
    
    Test Plan: The test isn't flaky anymore.
    
    Differential Revision: D21716070
    
    fbshipit-source-id: e5a97e520c5b10b67c335abf2dc7187ee6227643

commit 7492e98c7f5014dec690acab86d6d5b50a6a5f87
Author: Omkar Salpekar <osalpekar@fb.com>
Date:   Tue May 19 13:30:49 2020 -0700

    [Tensorpipe Agent] RPC, RRef tests for Tensorpipe Agent (#38444)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/38444
    
    This enables the RPC/RRef test suites to run with the Tensorpipe RPC Agent. This creates a new fixture to ensure the backend/options used are Tensorpipe, as well as a decorator to skip tests that Tensorpipe currently cannot support due to missing functionality.
    
    One small note: the decorator function is a class method of the test class so we can check whether `self.rpc_backend` is tensorpipe. In the class-scope, the `TEST_CONFIG.rpc_backend_name` string is set to Tensorpipe, but outside the class scope, it is PGA, possibly due to importing dist_utils which sets this config to PGA by default. The cleanest solution would be to refactor the backend selection to be more uniform (since currently every backend is set slightly differently), but that would be a longer-term fix.
    ghstack-source-id: 104321885
    
    Test Plan:
    Note: A couple of these tests will fail right now due to missing features. I've skipped the ones that regularly fail, but there will be some flaky tests that still fail occasionally.
    
    The decorator `@_skip_if_tensorpipe_agent` skips the tests that fail with the Tensorpipe Agent. Remove this decorator from above the tests once they are fixed.
    
    Differential Revision: D21412016
    
    fbshipit-source-id: 1e801ac5ccaf87974dd4df92d556895b01468bf3

commit 288dd33770287ce3c904eb86a019dd4435b85303
Author: Vasiliy Kuznetsov <vasiliy@fb.com>
Date:   Wed May 6 18:57:40 2020 -0700

    quant: remove hypothesis and int32 from layernorm test (#37947)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/37947
    
    The current test is flaky, removing two potential causes of flakiness.
    
    Test Plan:
    CI
    
    Imported from OSS
    
    Differential Revision: D21434861
    
    fbshipit-source-id: 82ea5762f3bb07a12052cde29729d73e95da8ddd

commit 9a37e75fde2e095a915583edc1687c25ae8685da
Author: vasiliy <vasiliy@fb.com>
Date:   Wed May 6 12:08:20 2020 -0700

    quant: remove hypothesis and int32 from layernorm test
    
    Summary:
    
    The current test is flaky, removing two potential causes of flakiness.
    
    Test Plan: CI
    
    Reviewers:
    
    Subscribers:
    
    Tasks:
    
    Tags:
    
    [ghstack-poisoned]

commit 25e6129c5258eb966c8e843ab43e7b4ebaa1b8de
Author: Vasiliy Kuznetsov <vasiliy@fb.com>
Date:   Tue May 5 12:15:54 2020 -0700

    quant BN tests: remove qint32 (#37832)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/37832
    
    These tests were flaky and qint32 support is not a priority at
    the moment, turning it off to improve test quality.
    
    Test Plan:
    ```
    python test/test_quantization.py TestQuantizedOps.test_batch_norm2d
    python test/test_quantization.py TestQuantizedOps.test_batch_norm2d_relu
    python test/test_quantization.py TestQuantizedOps.test_batch_norm3d
    ```
    
    Imported from OSS
    
    Differential Revision: D21404980
    
    fbshipit-source-id: 04f4308bc5d6e1a278c60985971d03c10a851915

commit 61970d0b1eb21c6edc6f51642c72f9f50d3b0e6c
Author: Xiang Gao <qasdfgtyuiop@gmail.com>
Date:   Tue Apr 28 11:29:14 2020 -0700

    Squashed commit of the following:
    
    commit 4cf03b29b7d39374ec2c424fd85c1815b4b27eb6
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Tue Apr 28 10:16:53 2020 -0700
    
        replace hypot with sqrt
    
    commit 4cc4837dae4d14efd133b51fcc796a1f2d14771f
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 13:00:03 2020 -0700
    
        revert white space change
    
    commit 0683bf1b743babd8507d985b7a1096962301a184
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:57:14 2020 -0700
    
        fix copy
    
    commit b9e08bd42580d21acc2263e0212eb599612d7521
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:39:27 2020 -0700
    
        remove include of complex
    
    commit 13d3df4816bec7a23469e088df79b717c209373a
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:38:32 2020 -0700
    
        fix scalar constructor
    
    commit 2f5293d4bee8f1af7513adda922e8fe6961a2774
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:27:18 2020 -0700
    
        resolve review nits
    
    commit 8bf035de3430a8af7837f43c392bd2e48136d2a6
    Merge: a15f4d546c 201ba13911
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:23:15 2020 -0700
    
        Merge branch 'master' of github.com:pytorch/pytorch into hacking-dispatch
    
    commit a15f4d546c5c3e5633fbad9c1d6d83a321ba86d0
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:16:44 2020 -0700
    
        revert all wrap changes
    
    commit aac470d29e00f3bceedd2ec36c224ab4e75b34ab
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 12:06:54 2020 -0700
    
        fix
    
    commit 3c5dd3130ef249222eb78f526c6ac59fb237de9d
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 11:35:38 2020 -0700
    
        revert white space change
    
    commit 285d7c7d63f19d4a476511555505e734c0d47223
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 11:19:57 2020 -0700
    
        fix warning
    
    commit 38fe795e80097bd7ce5638e2203978db4a8fefdb
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 11:17:27 2020 -0700
    
        USE_C10_COMPLEX
    
    commit 201ba139115adeecae4f094a9c9790200e53ff99
    Author: Parth Agarwal <iparthagarwal@gmail.com>
    Date:   Mon Apr 27 11:11:35 2020 -0700
    
        Correct $ANDROID_HOME string empty check (#37064)
    
        Summary:
        Updated file to correct shell code to test whether $ANDROID_HOME env variable is empty or not.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37064
    
        Differential Revision: D21181787
    
        Pulled By: IvanKobzarev
    
        fbshipit-source-id: 40c1d79d0fb730c7f68aa7472ce9b2398e91f2a2
    
    commit 805c417ec94ad13b3d974a6f23d85bf69e9ffdb5
    Author: Xiao Wang <24860335+xwang233@users.noreply.github.com>
    Date:   Mon Apr 27 10:59:32 2020 -0700
    
        Implement avg_pool2d kernel for channels_last (#35855)
    
        Summary:
        Implement avg_pool2d for channels_last. This will close https://github.com/pytorch/pytorch/issues/34996.
    
        Performance compared with **avg_pool2d** contiguous can be found at https://github.com/xwang233/code-snippet/blob/ed6617c6bc48dac5757d9a1ca6f5db5a68e5d01b/avg-pool2d-channels-last/avg-pool2d-naive.ipynb
    
        cc csarofeen ptrblck
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/35855
    
        Differential Revision: D21187360
    
        Pulled By: VitalyFedyunin
    
        fbshipit-source-id: b654b56168bc3982be306b634c7ed2f92018a9e5
    
    commit ec8006cc1635a088aae36aa9263bc85140d9aa6e
    Author: mattip <matti.picus@gmail.com>
    Date:   Mon Apr 27 10:58:01 2020 -0700
    
        [ONNX] fix provider_version and add consistency test (#36797)
    
        Summary:
        forward port the test from pr gh-36795, xref issue gh-32561
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36797
    
        Differential Revision: D21257034
    
        Pulled By: ezyang
    
        fbshipit-source-id: d217da0e74f00a433c904defc0bf3eb5f594fd5e
    
    commit 0048243f70f37a3ae74725fb21c88704d3ab62bb
    Author: Lukas Koestler <lkskstlr@gmail.com>
    Date:   Mon Apr 27 10:46:07 2020 -0700
    
        Check compiler -v to determine compiler (fix #33701) (#37293)
    
        Summary:
        As described in the issue (https://github.com/pytorch/pytorch/issues/33701) the compiler check
        	for building cpp extensions does not work with ccache.
        	In this case we check compiler -v to determine which
        	compiler is actually used and check it.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37293
    
        Differential Revision: D21256913
    
        Pulled By: ezyang
    
        fbshipit-source-id: 5483a10cc2dbcff98a7f069ea9dbc0c12b6502dc
    
    commit 6d409481b38692926930278002b50d2075396557
    Author: Gao, Xiang <qasdfgtyuiop@gmail.com>
    Date:   Mon Apr 27 10:29:07 2020 -0700
    
        Add overloads of std:: math functions for c10::complex (#35725)
    
        Summary:
        Issue: https://github.com/pytorch/pytorch/issues/35284
    
        ~This depends on and contains https://github.com/pytorch/pytorch/pull/35524. Please review after the dependency gets merged and I will rebase to get a clean diff.~
    
        The implementation of most functions follow the pattern
    
        ```C++
        template<typename T>
        C10_HOST_DEVICE c10::complex<T> some_function(c10::complex<T> x) {
        #if defined(__CUDACC__) || defined(__HIPCC__)
          return static_cast<c10::complex<T>>(thrust::some_function(static_cast<thrust::complex<T>>(x)));
        #else
          return static_cast<c10::complex<T>>(std::some_function(static_cast<std::complex<T>>(x)));
        #endif
        }
        ```
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/35725
    
        Differential Revision: D21256854
    
        Pulled By: ezyang
    
        fbshipit-source-id: 2112ba6b79923450feafd7ebdc7184a3eaecadb6
    
    commit a08a9f3b8222bc438ebdac86ecc44c1793d83c6b
    Author: Ryad ZENINE <r.zenine@gmail.com>
    Date:   Mon Apr 27 10:19:23 2020 -0700
    
        Enable uint8 upsampling 2 (#35029)
    
        Summary:
        Hi everyone,
    
        This is a supper small PR to enable `unit8` support for `nearest` up-sampling in `cpu` and `cuda`.
        This works enables us to move forward with the support of 'uint8' images in 'torchvision`.
    
        See impacted issues :
        https://github.com/pytorch/vision/issues/1375
        https://github.com/pytorch/vision/issues/1179#issuecomment-558197607
    
        Note: I wanted to add a unit test to ensure we have the expected behavior. I could not locate the `upsampling` unit tests for `nearest`. I can add the test if you point me to the right location.
    
        Thanks
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/35029
    
        Reviewed By: cpuhrsch
    
        Differential Revision: D21227144
    
        Pulled By: fmassa
    
        fbshipit-source-id: 33c4b5188dedd8f7f872e9d797e2a9b58ee7315c
    
    commit 5c9d1e48242587a9b1958df2d2efea3472072f4f
    Author: Xingying Cheng <xcheng16@fb.com>
    Date:   Mon Apr 27 10:16:59 2020 -0700
    
        Propagate module lints for mobile scripted module. (#37046)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37046
        ghstack-source-id: 102669259
    
        Creating a python api entry to generate mobile model lints which takes a scripted module as argument and returns a map of module lints.
    
        The initial version is to create placeholder which included module bundled input as the first lint instance. More lints will be added in the future.
    
        Test Plan: python test/test_optimizer.py
    
        Reviewed By: dreiss
    
        Differential Revision: D21164648
    
        fbshipit-source-id: 9e8f4e19d74b5464a55cc73b9dc18f358c5947d6
    
    commit 5b9f7f7b0e205a6d8d5f2e61f558eee378f0ce40
    Author: Mo Zhou <cdluminate@gmail.com>
    Date:   Mon Apr 27 09:34:52 2020 -0700
    
        [cmake] Add USE_SYSTEM_{GLOO,FP16,PTHREADPOOL,PSIMD,FXDIV,BENCHMARK} options (#14699) (#37277)
    
        Summary:
        These options are disabled by default, and are supposed to be used by
        linux distro developers. With the existing shortcut option
        USE_SYSTEM_LIBS toggled, these new options will be enabled as well.
    
        Additionally, when USE_SYSTEM_LIBS is toggled, setup.py should
        no longer check the existence of git submodules.
    
        ezyang
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37277
    
        Differential Revision: D21256999
    
        Pulled By: ezyang
    
        fbshipit-source-id: 84f97d008db5a5e41a289cb7bce94906de3c52cf
    
    commit 3a0ff3cd2f04fcf3d4f6d152ab0772f048375cb6
    Author: peterjc123 <peterghost86@gmail.com>
    Date:   Mon Apr 27 08:28:56 2020 -0700
    
        Generate environment restore script for Windows build jobs (#37319)
    
        Summary:
        for better debugging purposes
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37319
    
        Differential Revision: D21257011
    
        Pulled By: ezyang
    
        fbshipit-source-id: 41c7f1aa440f3ea626536b64392cca32f7c32dd3
    
    commit 007163407cd68a5131c159c2944bfb772ec913d4
    Author: Mo Zhou <cdluminate@gmail.com>
    Date:   Mon Apr 27 08:14:39 2020 -0700
    
        [cmake] Support "Generic" BLAS (#14699) (#37276)
    
        Summary:
        The "Generic" BLAS refers to the Netlib BLAS. This option is meaningful
        to the Debian family due to the "update-alternatives" mechanism, which
        enables the user to switch the libblas.so providers between different
        implementations at runtime, such as ATLAS, OpenBLAS, and Intel MKL.
        Such, building against generic BLAS provides much flexibility.
    
        This new option is not documented in setup.py because it's only supposed
        to be used by linux distro (especially Debian family) developersonly.
    
        ezyang
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37276
    
        Differential Revision: D21256877
    
        Pulled By: ezyang
    
        fbshipit-source-id: 55a5356653a1cfc763a5699b04afe5938f2007ec
    
    commit 22ac071d9a173ea2358dd7c88a3a47fb1e2a2fe1
    Author: Pavel Izmailov <izmailovpavel@gmail.com>
    Date:   Mon Apr 27 07:39:50 2020 -0700
    
        Add SWA to PyTorch mainline (#35032)
    
        Summary:
        This PR is based on the issue https://github.com/pytorch/pytorch/issues/29994#issue-524418771 and the discussion in the previous version of the PR https://github.com/pytorch/pytorch/pull/30559. Specifically, I followed the interface outlined in this [comment](https://github.com/pytorch/pytorch/pull/30559#issuecomment-574864768).
    
        ## Structure
        - `torch/optim/swa_utils.py` contains the implementation of  `AveragedModel` class, `SWALR` learning rate scheduler and `update_bn` utility
        - `test/test_optim.py` contains unit tests for the three components of SWA
        - `torch/optim/swa_utils.pyi` describes the interface of `torch/optim/swa_utils.py`
    
        The new implementation consists of
        - `AveragedModel` class; this class creates a copy of a given model and allows to compute running averages of the parameters.
        - `SWALR` learning rate scheduler; after a certain number of epochs switches to a constant learning rate; this scheduler is supposed to be chained with other schedulers.
        - `update_bn` utility; updates the Batch Normalization activation statistics for a given model and dataloader; this utility is meant to be applied to `AveragedModel` instances.
    
        For `update_bn` I simplified the implementation compared to the [original PR](https://github.com/pytorch/pytorch/pull/30559) according to the sugestions by vadimkantorov.
    
        ## Example
        ```python
        loader, optimizer, model = ...
        swa_model = torch.optim.swa_utils.AveragedModel(model)
        # You can use custom averaging functions with `avg_fun` parameter
        ema_avg = lambda p_avg, p, n_avg: 0.1 * p_avg + 0.9 * p
        ema_model = torch.optim.swa_utils.AveragedModel(model,
                                            avg_function=ema_avg)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                            T_max=300)
        swa_start = 160
        swa_scheduler = SWALR(optimizer, start_epoch=swa_start, swa_lr=0.05)
    
        for i in range(300):
             for input, target in loader:
                 optimizer.zero_grad()
                 loss_fn(model(input), target).backward()
                 optimizer.step()
                 scheduler.step()
                 swa_scheduler.step()
    
             if i > swa_start:
                 swa_model.update_parameters(model)
    
        # Update bn statistics for the swa_model at the end
        torch.optim.swa_utils.update_bn(loader, swa_model)
        ```
    
        UPDATED:
        ```python3
        loader, optimizer, model, loss_fn = ...
        swa_model = torch.optim.swa_utils.AveragedModel(model)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
        swa_start = 160
        swa_scheduler = SWALR(optimizer, swa_lr=0.05)
        for i in range(300):
             for input, target in loader:
                 optimizer.zero_grad()
                 loss_fn(model(input), target).backward()
                 optimizer.step()
             if i > swa_start:
                 swa_model.update_parameters(model)
                 swa_scheduler.step()
             else:
                 scheduler.step()
    
        # Update bn statistics for the swa_model at the end
        torch.optim.swa_utils.update_bn(loader, swa_model)
        ```
    
        Fixes https://github.com/pytorch/pytorch/issues/29994
        cc soumith vincentqb andrewgordonwilson vadimkantorov
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/35032
    
        Differential Revision: D21079606
    
        Pulled By: vincentqb
    
        fbshipit-source-id: e07f5e821f72ada63789814c2dcbdc31f0160c37
    
    commit 828d590b06109f1ed1ab5d5e7fc6601aae4af198
    Author: Jeff Daily <jeff.daily@amd.com>
    Date:   Mon Apr 27 06:48:05 2020 -0700
    
        [ROCm] Update to ROCm 3.3 (#37247)
    
        Summary:
        CC ezyang .
    
        ROCm 3.3 packages went live on 2020-04-01.  Tag 376 was pushed on 2020-04-15, so it should be based on ROCm 3.3.
    
        The upgrade to ROCm 3.3 is required as part of the effort to stabilize ROCm CI.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37247
    
        Differential Revision: D21256198
    
        Pulled By: ezyang
    
        fbshipit-source-id: 92ac21c0122eda360ec279d2c3d462c3e6bf4646
    
    commit f41742ff2fd5c9507c037dc120d75f6f191a87b1
    Author: Wanchao Liang <wanchaol@users.noreply.github.com>
    Date:   Sun Apr 26 22:18:55 2020 -0700
    
        [autograd] remove spinning for dist engine (#36606)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36606
    
        This PR refactor the continuation logic of the async mode on autograd
        engine, to avoid launch spinning works. To achieve that:
        1. remove the continuation logic in
        execute_graph_task_with_continuiation
        2. separate the usage of execute_graph_task between dist_engine and
        local engine, now dist_engine universally use
        `execute_graph_task_until_ready_queue_empty` (a better name appreciated
        here).
        3. remove enqueue_blocked_task_on_cpu
        4. remove the async mode in `execute_with_graph_task` as we don't need
        to use it in dist_engine
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21032731
    
        Pulled By: wanchaol
    
        fbshipit-source-id: 708ea3bc14815bdc151b56afa15eb85b4ac0f4b1
    
    commit ed9ec3c96fdc9656c5bac144887c312a0168469e
    Author: Wanchao Liang <wanchaol@users.noreply.github.com>
    Date:   Sun Apr 26 22:18:55 2020 -0700
    
        [autograd] refactor some functions (#37061)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37061
    
        This PR refactors:
        1. `set_device` to make it out of Engine
        2. put `graph_task_completed` into GraphTask
        3. put `mark_graph_task_completed` into GraphTask
    
        This also make the distributed engine easy to call those functions.
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21188688
    
        Pulled By: wanchaol
    
        fbshipit-source-id: f56106e6ed7d966cfa4d962781c7865cc3c5321d
    
    commit 47fec01c45c696e247aff9e910f29a9586ae0869
    Author: lixinyu <lixinyu@devgpu175.prn2.facebook.com>
    Date:   Sun Apr 26 10:57:53 2020 -0700
    
        Fix cpp extension compile failure on some envs (#37221)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37221
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21226873
    
        Pulled By: glaringlee
    
        fbshipit-source-id: 0a390bbeaf153ee5ec355943f92c2dbcc5e04b59
    
    commit b428f454e13f6e8055124ea19c32b554017137d0
    Author: Mike Ruberry <mruberry@fb.com>
    Date:   Sun Apr 26 04:25:28 2020 -0700
    
        Revert D18927220: if_constexpr for C++14
    
        Test Plan: revert-hammer
    
        Differential Revision:
        D18927220
    
        Original commit changeset: 19a135e00af6
    
        fbshipit-source-id: a1b8755a27903b98b742881b3ecce4f5e99543b2
    
    commit b64fc3c4b5d927928770f9b343eb845123367084
    Author: Mike Ruberry <38511765+mruberry@users.noreply.github.com>
    Date:   Sat Apr 25 21:16:50 2020 -0700
    
        Changes warnings generated in cpp to show point of Python origination (#36052)
    
        Summary:
        Today in PyTorch, warnings triggered in C++ are printed to Python users like this:
    
        `../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.`
    
        This may be unhelpful to Python users, who have complained it's difficult to relate these messages back to their programs. After this PR, warnings that go through the PyWarningHandler and allow it to add context print like this:
    
        ```
        test/test_torch.py:16463: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:81.)
          cpu_result = getattr(cpu_tensor, op_str)(*cpu_args)
        ```
    
        This relates the warning back to the user's program. The information about the cpp file and line number is preserved in the body of the warning message.
    
        Some warnings, like those generated in the JIT, already account for a user's Python context, and so they specify that they should be printed verbatim and are unaffected by this change. Warnings originating in Python and warnings that go through c10's warning handler, which prints to cerr, are also unaffected.
    
        A test is added to test_torch.py for this behavior. The test relies on uint8 indexing being deprecated and its warning originating from its current header file, which is an unfortunate dependency. We could implement a `torch.warn` function, instead.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36052
    
        Differential Revision: D20887740
    
        Pulled By: mruberry
    
        fbshipit-source-id: d3515c6658a387acb7fccaf83f23dbb452f02847
    
    commit f8ec51bd865bb488dc0c30f1e970c5dc49ce4727
    Author: Peter Bell <peterbell10@live.co.uk>
    Date:   Sat Apr 25 20:55:28 2020 -0700
    
        Ensure DataParallel replicas can be saved (#37307)
    
        Summary:
        Fixes https://github.com/pytorch/pytorch/issues/37182
    
        The `zero_grad` wrapper from `_replicate_for_data_parallel` can't be pickled. So instead, I set an attribute `_is_replica = True` and check for this in `Module.zero_grad`.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37307
    
        Differential Revision: D21246119
    
        Pulled By: mrshenli
    
        fbshipit-source-id: 4755786d48a20bc247570ba672de9dd526914ce1
    
    commit 2b050371b4cecd9c12b5f763e6867ff1c1019aab
    Author: Omkar Salpekar <osalpekar@fb.com>
    Date:   Sat Apr 25 20:11:33 2020 -0700
    
        Make listenLoopInternal non-virtual (#37265)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37265
    
        In PGA, `listenLoopInternal` should not be virtual - PGA doesn't have any child classes that override this. Re-arranged some comments for `listenLoop` as well.
        ghstack-source-id: 102880792
    
        Test Plan: Sandcastle/CI
    
        Differential Revision: D21238761
    
        fbshipit-source-id: 5ec5058bc462182cf970faca9a734c11c7be2a32
    
    commit d98ea604f4c31f86b2afe1afd96f283ef77c4da2
    Author: Omkar Salpekar <osalpekar@fb.com>
    Date:   Sat Apr 25 19:22:51 2020 -0700
    
        Improve Error Message for Dist Autograd Context Cleanup Failure (#37255)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37255
    
        Improved error message logged when Distributed Autograd Context cleanup fails - added node information and underlying error. The previous error message also assumed that the cause of the error was due to too many RPC's failing, but this is not necessarily the case.
        ghstack-source-id: 102867620
    
        Test Plan: Ensuring Sandcastle/CI tests pass. Verified the correct message is logged when this code path is executed in `test_backward_node_failure` and `test_backward_node_failure_python_udf` .
    
        Differential Revision: D20950664
    
        fbshipit-source-id: 267318187b7ef386930753c9679a5dfab6d87018
    
    commit b198796a2810ebd7fdefec3389c17be47ba6a6ce
    Author: Zafar <cc.rafaz@zafar.cc>
    Date:   Sat Apr 25 18:19:03 2020 -0700
    
        [quant] quantized reflection_pad1d (#36450)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36450
    
        Test Plan: Imported from OSS
    
        Differential Revision: D20984967
    
        Pulled By: z-a-f
    
        fbshipit-source-id: 4731f16ba05a6aa57636d9ab85f12dfdeebcf08d
    
    commit 7604f470ed083d55c6a25bee3f995c7e71ea488f
    Author: Yinghai Lu <yinghai@fb.com>
    Date:   Sat Apr 25 18:05:21 2020 -0700
    
        Add weight info in debug_ssa_net (#37262)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37262
    
        It's convenient to have weights info in the debug_ssa_net so that we can tell what is weight and what is primary inputs. We can get their shape and size info with some post-processing script easily.
    
        Reviewed By: ChunliF
    
        Differential Revision: D21237537
    
        fbshipit-source-id: 1fadc605283ef2eed78c44494e062a16ccf135ab
    
    commit 92e91cee8dc9d78314308ace125022835fcbc0c9
    Author: Ksenija Stanojevic <ksenija.stanojevic@gmail.com>
    Date:   Sat Apr 25 17:54:57 2020 -0700
    
        ONNX Export Support for CrossEntropyLoss (#34830)
    
        Summary:
        Add ONNX export support for torch.nn.CrossEntropyLoss.
    
        This PR makes following changes:
        1. Updates nll_loss export
        2. Makes a post pass for SoftmaxCrossEntropy
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/34830
    
        Reviewed By: hl475
    
        Differential Revision: D21230712
    
        Pulled By: houseroad
    
        fbshipit-source-id: c81911a41968e23813ba10274340ce4d8ba1ed78
    
    commit 205c6ffbc5febd27b810c37e1bfae50b9655f8e4
    Author: Zafar <cc.rafaz@zafar.cc>
    Date:   Sat Apr 25 17:04:23 2020 -0700
    
        [quant] Generalizing _calculate_dynamic_qparams in quantized test (#36449)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36449
    
        Test Plan: Imported from OSS
    
        Differential Revision: D20984966
    
        Pulled By: z-a-f
    
        fbshipit-source-id: 17437297adae813bc5c6fa43c6c7514f72ce2f6c
    
    commit ca39f99d48a6fc43384a86ecf745df40f038d21f
    Author: Haixin Liu <haixin@fb.com>
    Date:   Sat Apr 25 16:44:13 2020 -0700
    
        [Pytorch Numeric Suite] Add module level comparison (#37242)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37242
    
        Add module level comparison API.
        ghstack-source-id: 102853727
    
        Test Plan: buck test mode/dev caffe2/test:quantization -- 'test_compare_model_stub'
    
        Reviewed By: raghuramank100
    
        Differential Revision: D21232277
    
        fbshipit-source-id: de707eea101a66a37869129460274c56e4e07db2
    
    commit a04022c656516c08e3719628f39ac47a9328155a
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Sat Apr 25 15:53:00 2020 -0700
    
        Use `std::chrono::high_resolution_clock` for profiling on Mac (#37280)
    
        Summary:
        According to Darwin man-page:
            `CLOCK_REALTIME` the system's real time (i.e. wall time) clock, expressed as the amount of time since the Epoch.  This is the same as the value returned by `gettimeofday`(2).
    
        I.e. its returns timestamp with microsecond resolution, as can be obvserved by running following small program:
        ```
        #include <sys/time.h>
        #include <stdint.h>
        #include <stdbool.h>
        #include <stdio.h>
    
        bool conseq_time(clockid_t c) {
          struct timespec t1, t2;
          clock_gettime(c, &t1);
          clock_gettime(c, &t2);
          printf("t1={.tv_sec=%ld, .tv_nsec=%ld}\n", t1.tv_sec, t1.tv_nsec);
          printf("t2={.tv_sec=%ld, .tv_nsec=%ld}\n", t2.tv_sec, t2.tv_nsec);
          bool rc = t1.tv_sec == t2.tv_sec && t1.tv_nsec == t2.tv_nsec;
          printf("Two timestamps are %sequal\n", rc ? "" : "not ");
          return rc;
        }
    
        int main(void) {
          printf("using CLOCK_REALTIME\n");
          conseq_time(CLOCK_REALTIME);
          printf("using CLOCK_MONOTONIC_RAW\n");
          conseq_time(CLOCK_MONOTONIC_RAW);
          return 0;
        }
        ```
        which if compiled outputs something like:
        ```
        using CLOCK_REALTIME
        t1={.tv_sec=107519, .tv_nsec=860315000}
        t2={.tv_sec=107519, .tv_nsec=860315000}
        Two timestamps are equal
        using CLOCK_MONOTONIC_RAW
        t1={.tv_sec=107520, .tv_nsec=954297363}
        t2={.tv_sec=107520, .tv_nsec=954297426}
        Two timestamps are not equal
        ```
    
        But why do it, if all this platform specific logic is already nicely abstracted in `std::chrono::`:
        https://github.com/llvm/llvm-project/blob/master/libcxx/src/chrono.cpp#L117
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37280
    
        Differential Revision: D21246608
    
        Pulled By: malfet
    
        fbshipit-source-id: 6beada30657a2720000e34214b1348112e55be50
    
    commit 59052e39b8daa12a7243ac9e0bbd6714a4fdb861
    Author: Zafar <cc.rafaz@zafar.cc>
    Date:   Sat Apr 25 15:50:38 2020 -0700
    
        [quant] qtensor resize (#36442)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36442
    
        Test Plan: Imported from OSS
    
        Differential Revision: D20984080
    
        Pulled By: z-a-f
    
        fbshipit-source-id: 7fcf24bd2f92f038b670f510118b012d8c7acc74
    
    commit bf860a4ebafbfcb75e61a8603bded72f6d0b0970
    Author: Mike Ruberry <38511765+mruberry@users.noreply.github.com>
    Date:   Sat Apr 25 15:34:39 2020 -0700
    
        Adds missing documentation . (#37295)
    
        Summary:
        Fixes torch.isclose documentation missing a `.`.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37295
    
        Differential Revision: D21245426
    
        Pulled By: mruberry
    
        fbshipit-source-id: 88ce57ed68c2eac6aa83932780a6ba30e9fa69ea
    
    commit 34284c127930dc12d612c47cab44cf09b432b522
    Author: Raghuraman Krishnamoorthi <raghuraman@fb.com>
    Date:   Sat Apr 25 14:50:40 2020 -0700
    
        Fix NaN error in dynamic quantization in qLinear, re-enable test_quantized_rnn (#36009)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36009
    
        When scale is very small (less than float eps, but greater than minimum double precision value), computation of reciprocal of scale in floating point precision within FBGEMM returns inf, while QuantUtils does not. Changed computation in QuantUtils to occur with floating point precision to re-enable tests.
        ghstack-source-id: 102896302
    
        Test Plan:
        buck test caffe2/test:quantization -- 'test_quantized_rnn \(quantization\.test_quantization\.PostTrainingDynamicQuantTest\)' --print-passing-details --run-disabled
        Summary (total time 59.91s):
          PASS: 1
          FAIL: 0
          SKIP: 0
          FATAL: 0
          TIMEOUT: 0
          OMIT: 0
    
        Differential Revision: D20853000
    
        fbshipit-source-id: 948a888f5516b3ba9c6efb7de31ef2cc9d431991
    
    commit 84a31fb4e7fb1b5dbe9e42f5e1e30be4a0440189
    Author: Mike Ruberry <mruberry@fb.com>
    Date:   Sat Apr 25 14:20:33 2020 -0700
    
        Revert D18927221: Boxing uses if_constexpr instead of SFINAE
    
        Test Plan: revert-hammer
    
        Differential Revision:
        D18927221
    
        Original commit changeset: 70d99025b45e
    
        fbshipit-source-id: a4b650bbb6d76dda6086d88eb554f3c3077b0f76
    
    commit c90955e3d12391bb7ad22fb9a22eba8f768267a4
    Author: James Reed <jamesreed@fb.com>
    Date:   Sat Apr 25 13:53:12 2020 -0700
    
        [profiler] Sort by end interval as well when parsing CPU trace (#37297)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37297
    
        Test Plan: Imported from OSS
    
        Reviewed By: ngimel
    
        Differential Revision: D21245463
    
        Pulled By: jamesr66a
    
        fbshipit-source-id: 8d307eaa32fa960b93dfd9a3b0b4c767fd903094
    
    commit ea741f829e825e8ff87ed67cd80a71d65fbb9c73
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Sat Apr 25 13:51:10 2020 -0700
    
        Add `--repeat` option to python unit-test (#37281)
    
        Summary:
        This would run same testsuite (or individual test) multiple time
        Useful for detecting flaky tests
    
        Example usage: `python test_autograd.py TestAutograd.test_profiler -v --repeat=100`
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37281
    
        Differential Revision: D21244442
    
        Pulled By: malfet
    
        fbshipit-source-id: 3ecafec7ae87bc1e418aa28151bbc472ef37a713
    
    commit 44345ad08c0aefcae400b948635f980c907f0f49
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Sat Apr 25 13:50:50 2020 -0700
    
        Do not define C10_IOS on Mac (#37283)
    
        Summary:
        Because MacOS is not iOS
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37283
    
        Test Plan: CI
    
        Differential Revision: D21244398
    
        Pulled By: malfet
    
        fbshipit-source-id: b822e216e83887e2f2961b5c5384eaf749629f61
    
    commit cb27067b321dacbc8fd94d9a4b85c62d4244edbf
    Author: Negin Raoof <neginmr@utexas.edu>
    Date:   Sat Apr 25 12:21:03 2020 -0700
    
        [ONNX] Remove inverse op (#37005)
    
        Summary:
        ONNX inverse op is being removed.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37005
    
        Reviewed By: hl475
    
        Differential Revision: D21230728
    
        Pulled By: houseroad
    
        fbshipit-source-id: 7e10414918c57938cda4ca03875c070319d429fb
    
    commit b18f57e5480ce4461c7583d66188357c635e2cbc
    Author: Sebastian Messmer <messmer@fb.com>
    Date:   Sat Apr 25 11:29:38 2020 -0700
    
        Boxing uses if_constexpr instead of SFINAE (#31092)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/31092
    
        -
        ghstack-source-id: 102878439
    
        Test Plan: unit tests
    
        Reviewed By: ezyang
    
        Differential Revision: D18927221
    
        fbshipit-source-id: 70d99025b45edfaef11a0d587cf8bf8e749df6b8
    
    commit f5e6f1f333b98a596daef9f277cb7f915de91c75
    Author: Sebastian Messmer <messmer@fb.com>
    Date:   Sat Apr 25 11:29:38 2020 -0700
    
        if_constexpr for C++14 (#31091)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/31091
    
        This implements a C++17 "if constexpr" like feature for C++14.
        This can be used, for example, to replace SFINAE or to force the compiler to remove some parts of a function in the assembly based on a condition.
        PRs stacked on top will use this to simplify some of our template metaprogramming.
        ghstack-source-id: 102867141
    
        Test Plan: unit tests
    
        Differential Revision: D18927220
    
        fbshipit-source-id: 19a135e00af6ebb0139ce3730353762d4512158f
    
    commit 04b36fc264c63d31e481166c675935b1d99afc5e
    Author: Bram Wasti <bwasti@fb.com>
    Date:   Sat Apr 25 09:59:06 2020 -0700
    
        [TensorExpr] rfactor implementation (#36237)
    
        Summary:
        A similar interface to Halide's rfactor: https://halide-lang.org/tutorials/tutorial_lesson_18_parallel_associative_reductions.html
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36237
    
        Reviewed By: zheng-xq
    
        Differential Revision: D21233309
    
        Pulled By: bwasti
    
        fbshipit-source-id: d2706a9e90b707ee195e339f834ff4a54b63a256
    
    commit c52deb694ed9a5e18520a81be07a249fd9a70567
    Author: Shen Li <cs.shenli@gmail.com>
    Date:   Sat Apr 25 09:33:11 2020 -0700
    
        Consolidate usage on torch::jit::toPyObject in RPC request_callback (#37249)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37249
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21234990
    
        Pulled By: mrshenli
    
        fbshipit-source-id: d07210151342bd2ad12d1364d9f22817ee59b0c2
    
    commit 3d934c3d36f8967d79016b36e3cc7b9c2ffa6821
    Author: Shen Li <cs.shenli@gmail.com>
    Date:   Sat Apr 25 09:33:11 2020 -0700
    
        Add using torch::utils::Future to simplify code in RRefContext (#36811)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36811
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21093846
    
        Pulled By: mrshenli
    
        fbshipit-source-id: 61a6b1483ef1533803a18bec216ebe82aa187458
    
    commit 269ec9a139d381605fa898539670163a92d0d107
    Author: Shen Li <cs.shenli@gmail.com>
    Date:   Sat Apr 25 09:33:11 2020 -0700
    
        Prevent RRef.to_here() to block an RPC thread on the callee using Future callbacks (#36805)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36805
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21093847
    
        Pulled By: mrshenli
    
        fbshipit-source-id: 81b0934874af36e03329fe6176628e3aca12811f
    
    commit 6e1e55c1344400f1a38b3e2a2a40f96816cf81d3
    Author: Shen Li <shenli@devfair017.maas>
    Date:   Sat Apr 25 09:33:11 2020 -0700
    
        Prevent RRef unpickle to block waiting for OwnerRRef creation (#36785)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36785
    
        Currently, RRef unpickle (both Python and TorchScript) will block
        until the OwnerRRef has been created by the original `rpc.remote`
        call, if it is an OwnerRRef. This is not ideal as the correctness
        would then depends on the number of threads configuration. This
        commit changed that behavior. Both `rpc.remote` and the unpickle
        can create OwnerRRefs. More specifically, whichever one arrives
        first will create the OwnerRRef and the subsequent ones will
        retrieve the same OwnerRRef, so that no one is blocking.
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21083089
    
        Pulled By: mrshenli
    
        fbshipit-source-id: 34ef063d50549b01c968b47815c4fe9fac179d3d
    
    commit 8872e00e11926833c1d7d5a0578524f808ebd631
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Sat Apr 25 09:08:22 2020 -0700
    
        fix type meta
    
    commit d7f7c290e3d76a1e3019166644baf78de0d95a31
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Sat Apr 25 07:40:50 2020 -0700
    
        addmv migration [resubmit] (#37236)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37236
    
        Differential Revision: D21232988
    
        Pulled By: anjali411
    
        fbshipit-source-id: ac6c0ee018aef3c841b039d76e6e1fbb3cd0292d
    
    commit b9a2c35fdf203680a98d30f9201dac2b50548157
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Sat Apr 25 01:31:32 2020 -0700
    
        remove debug print
    
    commit cfd70207b1753648ee5c474bee9905b0543d4db4
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Sat Apr 25 01:30:24 2020 -0700
    
        fix copy kernel
    
    commit b6eb2a5f73640518b40708d6356ccb06b243d8ba
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Sat Apr 25 01:18:44 2020 -0700
    
        fix type in dispatch
    
    commit 856e8cf0288fe3c1701d11fae61b214c08635b9d
    Author: Ilia Cherniavskii <iliacher@fb.com>
    Date:   Sat Apr 25 00:57:06 2020 -0700
    
        Revert D21213786: Enable global observers API
    
        Test Plan: revert-hammer
    
        Differential Revision:
        D21213786
    
        Original commit changeset: e618254da74a
    
        fbshipit-source-id: 425ea5d44fa55655ec0dd586c5075996b926177b
    
    commit e6231c9e24c05e435eeb9dfcd66247e4520c559a
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Sat Apr 25 00:09:09 2020 -0700
    
        Do not run valgrind on the Aten unit tests compiled with clang (#37152)
    
        Summary:
        Valgrind detects some unitialized variables if torch_cpu is compiled with clang, which are not reproducible if the same code is compiled with gcc nor using address sanitizer tool
        See https://github.com/pytorch/pytorch/issues/37117
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37152
    
        Differential Revision: D21241577
    
        Pulled By: malfet
    
        fbshipit-source-id: 4a5dddf2a4fc4238dc9117cb92ee4e34af9e6064
    
    commit c9b3d94a4dc571b9c711e7e8e6e378dbd78a2e0a
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 23:59:12 2020 -0700
    
        fix to
    
    commit cd4688138bb52f36978ed6c9daab19ee864429b9
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 23:55:57 2020 -0700
    
        fix to
    
    commit 6e659e928ba48afa8a6f5d734c37ab187734927b
    Author: Ilia Cherniavskii <iliacher@fb.com>
    Date:   Fri Apr 24 23:47:33 2020 -0700
    
        Enable global observers API (#37195)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37195
    
        After adding c10::DispatchKey::Profiler the behavior of RecordFunction
        observers is also controlled by the dispatch key,
        this PR moves the logic outside of the profiler into the record function
    
        Reviewed By: ngimel
    
        Differential Revision: D21213786
    
        fbshipit-source-id: e618254da74a4f1ce16c51a3869bbd75a4f561ad
    
    commit a0f1c2c97249b8c3e5fc9d537ebaee169cdab88a
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 23:25:53 2020 -0700
    
        fix item
    
    commit 4e976b9334acbcaa015a27d56540cd2115c2639b
    Author: Sebastian Messmer <messmer@fb.com>
    Date:   Fri Apr 24 23:08:18 2020 -0700
    
        Remove callBoxedWorkaround (#36850)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/36850
    
        Since now all unboxing happens after dispatch, which means that all c10 ops support unboxing, we can now use op.callBoxed() for all ops and don't need callBoxedWorkaround (which was going through the JIT registry) anymore.
        ghstack-source-id: 102879558
    
        Test Plan: waitforsandcastle
    
        Differential Revision: D21102375
    
        fbshipit-source-id: d1e041116563a9650d5a86b07eb96d217d8756f3
    
    commit 6efca91edcab0f258293467bc962c3fd1332f79a
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 22:48:46 2020 -0700
    
        enable data_ptr for std::complex
    
    commit 6ea2aedab9da83a2dcf421880436c471ab40f0ec
    Author: Hong Xu <hong@topbug.net>
    Date:   Fri Apr 24 22:35:24 2020 -0700
    
        Cast shape_.size() to int64_t before comparing with squash_dim (#37109)
    
        Summary:
        This is generating a considerable amount of warning messages since TensorIterator.h is included from a lot of files:
    
            /home/hong/xusrc/pytorch/aten/src/ATen/native/TensorIterator.h:372:47:
            warning: comparison of integers of different signs: 'const int64_t' (aka 'const long') and 'c10::SmallVectorTemplateCommon::size_type' (aka 'unsigned long') [-Wsign-compare]
                TORCH_CHECK(squash_dim >= 0 && squash_dim < shape_.size(),
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37109
    
        Differential Revision: D21242163
    
        Pulled By: ngimel
    
        fbshipit-source-id: aec2978ee76750676a449eb6671142a782658de3
    
    commit 35decf020b31c7ca25e5c65492046077ceb9f2cc
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 22:23:12 2020 -0700
    
        type meta
    
    commit 30eb0bdf3257a62df303ab59991ad6eb784dd177
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Fri Apr 24 21:30:25 2020 -0700
    
        Do not define list "0" in torch/CMakeLists.txt (#37275)
    
        Summary:
        Per https://cmake.org/cmake/help/latest/command/list.html list insert arguments order is
        `list(INSERT <list> <index> [<element>...])`
    
        That is first argument is list name not the index it gets inserted into
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37275
    
        Differential Revision: D21243539
    
        Pulled By: malfet
    
        fbshipit-source-id: b947ad64f1a3549df68083383537899b19abd9ca
    
    commit 904949382e36c282c547db545d98bde23553695f
    Author: Raghuraman Krishnamoorthi <raghuraman@fb.com>
    Date:   Fri Apr 24 20:55:32 2020 -0700
    
        Ensure that histogram observers have zero-point of zero for post ReLU activations (#37107)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37107
    
        Currently histogram observers relax both the min and max values of the activations for performance speedup reasons. This causes an issue for glow where there is a slow down if the zero-point is not zero for post ReLU activations.
        ghstack-source-id: 102768017
    
        Test Plan: buck test caffe2/test:quantization -- 'test_histogram_observer_one_sided \(quantization\.test_quantization\.RecordHistogramObserverTest\)' --print-passing-details
    
        Differential Revision: D21187636
    
        fbshipit-source-id: 8d616b9e9caf2979a26a215e99434f71025e3d8b
    
    commit ef9ec03e770d36b7138189ac5a96515487902a2f
    Author: Xiaodong Wang <xdwang@fb.com>
    Date:   Fri Apr 24 20:27:05 2020 -0700
    
        [CUDA11] Pytorch change (#37187)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37187
    
        Adding CUDACC guard for gcc9+
    
        Reviewed By: ngimel
    
        Differential Revision: D21209798
    
        fbshipit-source-id: 5cc4efc7108577d74bee4c12c942ed1e5bf9bbac
    
    commit 984f1ef2d6d967f3aca9b23f7b763f484260212b
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 20:25:48 2020 -0700
    
        ident
    
    commit ea25b50901a5c56e8845ee364a95d82fc41f95e6
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 20:20:28 2020 -0700
    
        save
    
    commit a80a438e3752d0f4b1820492e9d0051760b926bb
    Author: Nikolay Korovaiko <korovaikon@gmail.com>
    Date:   Fri Apr 24 20:10:21 2020 -0700
    
        correctly set and restore states in te tests (#37210)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37210
    
        Differential Revision: D21238634
    
        Pulled By: Krovatkin
    
        fbshipit-source-id: 6462239753399c10c871baa5d5fdff5465cf2544
    
    commit 686b521784a869cd48a75a16fce38bc25560a2ef
    Author: Xiao Wang <24860335+xwang233@users.noreply.github.com>
    Date:   Fri Apr 24 20:10:10 2020 -0700
    
        Update cusparse deprecated Xcsrmm2 call (#37202)
    
        Summary:
        Reland of https://github.com/pytorch/pytorch/issues/36845 due to Windows CI failure.
    
        binary_windows_wheel_3_7_cu102_build is passed, so the windows guard should be fine this time.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37202
    
        Differential Revision: D21233358
    
        Pulled By: xw285cornell
    
        fbshipit-source-id: 707de0ff21d178686354ffaea7625f1d68b3e8d3
    
    commit 22e79aaaa4cd395eee9409d13dc64f7ce1f85b1e
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 20:01:02 2020 -0700
    
        save
    
    commit 4a72ddedcd2c645bb8fd507b375a0a42483ad1e1
    Author: Gao, Xiang <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 19:47:36 2020 -0700
    
        Show cpu info for macos jobs (#37220)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37220
    
        Differential Revision: D21243205
    
        Pulled By: ezyang
    
        fbshipit-source-id: 77a4d904e80c59b6d4d39b1a1a0fb441d8a35f0c
    
    commit 1d0334dd62ae18c7fd0c9fa5d048bf4a796e0c16
    Author: Yang Gu <yangu@microsoft.com>
    Date:   Fri Apr 24 19:46:53 2020 -0700
    
        Add cpu build and test to Windows CI (#37135)
    
        Summary:
        Add windows build and test for cpu
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37135
    
        Differential Revision: D21243189
    
        Pulled By: ezyang
    
        fbshipit-source-id: dd804ac258940e608facaf375d80ff5a0c59a7ae
    
    commit 4a05558bd9de93fd90b85099b9606a03f53cd163
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 19:37:14 2020 -0700
    
        fix distribution
    
    commit 5b7d9817c35fa4fb6adf31929b41e019cbdf958e
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 19:23:37 2020 -0700
    
        fix
    
    commit f71593ee31f2366dafd660b4bbcfb3086dea0e81
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 19:11:47 2020 -0700
    
        fix
    
    commit ff19d415d769d8e12dbd06ba5aae5e9ea951179d
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 19:01:42 2020 -0700
    
        fix comment
    
    commit 3ada82d2364ca43e77eb7f77ff8e43fb858f296e
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 18:56:29 2020 -0700
    
        Automatically include c10/util/dont_wrap_complex.h
    
    commit 398608de9a4bb2ab17dbfa826f435091fc75c8d3
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 18:53:12 2020 -0700
    
        fix
    
    commit 093564d6918242aff70cd281554ab0142e01751e
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 18:38:16 2020 -0700
    
        fix
    
    commit f71f97e17a4f3a4abf75ccd9d2f32a6312d8ebc5
    Merge: 626473f5fe 1d8012a624
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 18:22:23 2020 -0700
    
        Merge branch 'master' of github.com:pytorch/pytorch into hacking-dispatch
    
    commit 626473f5fe717d106e4888b9afdb49cd38782d81
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 18:20:26 2020 -0700
    
        Make c10::complex the C++ type for complex tensors
    
    commit 1d8012a624e4dbc9f66c7942e82e168707796855
    Author: Sebastian Messmer <messmer@fb.com>
    Date:   Fri Apr 24 18:05:47 2020 -0700
    
        Delete dead code (#37254)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37254
    
        This code is leftover from the KernelFactory deletion.
        ghstack-source-id: 102866045
    
        Test Plan: waitforsandcastle
    
        Differential Revision: D21235480
    
        fbshipit-source-id: 739ba677d2139ba9934d103f75a609638f1a3856
    
    commit 1f08ff12ecd27cf18fe21cf1fcf90a1c824b3ff7
    Author: Michael Suo <suo@fb.com>
    Date:   Fri Apr 24 17:40:48 2020 -0700
    
        [jit] fix named tuples as attributes (#37251)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37251
    
        This was broken by recent changes to how we serialize with type tags. We
        save a name (like `Dict[str, MyNamedTuple]`) and then relied on the
        mobile type parser to resolve that name back into a set of types.
    
        This doesn't work for any NamedTypes as the mobile type parser doesn't
        know how to resolve those. The unpickler allows the caller to inject a
        type resolver in for this purpose, use that so that when importing in a
        non-mobile environment you get the right results.
    
        A second problem also had to be fixed: the SourceImporter type loader
        would only load named types directly (e.g. `MyNamedTuple`) and choked if
        it was a general type that contained a named tupe (e.g.
        `List[MyNamedTuple]`). Fixed that and renamed `loadNamedType` to
        `loadType` for clarity.
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21235213
    
        Pulled By: suo
    
        fbshipit-source-id: 16db0f4c5e91a890d67a8687cc8ababa6b94b0f4
    
    commit 47c4dca1ab3fedfde7b1ce383e779454e7903e86
    Author: Nikita Shulga <nshulga@fb.com>
    Date:   Fri Apr 24 17:39:53 2020 -0700
    
        Remove python-2 or python<3.5 checks from unit tests (#37252)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37252
    
        Test Plan: CI
    
        Differential Revision: D21241083
    
        Pulled By: malfet
    
        fbshipit-source-id: 44164b822f7905288abb2beda0175d2162d86143
    
    commit 521910e0e97f6014c976cdab7dff024a038a0a76
    Author: Michael Suo <suo@fb.com>
    Date:   Fri Apr 24 17:17:39 2020 -0700
    
        Update clang_format_ci.sh (#37268)
    
        Summary:
        shellcheck led me astray!
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37268
    
        Differential Revision: D21241361
    
        Pulled By: suo
    
        fbshipit-source-id: 68244bb889e784ccd36d714209c2c15e2d6f04f8
    
    commit b60c3dfdd963cd5b0879d9fae5130fac3ed79bbf
    Author: James Reed <jamesreed@fb.com>
    Date:   Fri Apr 24 16:22:25 2020 -0700
    
        Add fallback wrapper for profiler (#37194)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37194
    
        Test Plan: Imported from OSS
    
        Reviewed By: ilia-cher, ngimel
    
        Differential Revision: D21217886
    
        Pulled By: jamesr66a
    
        fbshipit-source-id: b06195e9ac110979d128391e067d5c9f416c1873
    
    commit 047488a7ffb42a4dad5c12992663738bd6c96004
    Author: Basil Hosmer <bhosmer@fb.com>
    Date:   Fri Apr 24 16:06:08 2020 -0700
    
        Mask all high dispatch keys in BackendSelect kernels (#37257)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37257
    
        Previously, we were relying on fragile invariants to avoid collecting
        and feeding high precedence, non-backend dispatch keys to backend
        initialization machinery, which would assert on them. (These same
        keys are then used for redispatch, so a second latent problem lurks
        behind the first.) Here we mask off the BackendDispatch key and all
        keys to its left.
    
        Followup: move backend init code to backend-specific wrappers
        (`CPUType` etc.). This will let us remove the backend init code from
        both BackendSelect and STATIC_DISPATCH wrappers. (Though BackendSelect
        will still need to compute a dispatch key, so the logic introduced
        here will still be necessary.)
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21235856
    
        Pulled By: bhosmer
    
        fbshipit-source-id: 1b8bd7897ed4b41a95718f3cfceddf4ee094744a
    
    commit b6bb644e41b3928b5a515330ad35c8b447fcb876
    Author: Zachary DeVito <zdevito@fb.com>
    Date:   Fri Apr 24 15:12:12 2020 -0700
    
        Fix long line splitting issue in python_print (#37088)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37088
    
        For an inlined expression tree like `(e_0, (e_1, e_long))` the previous
        algoritm only scanned the same statement as `e_long`, splitting the
        inlined expressions across lines. Because it did not scan `e_0`, `e_0`
        would still get emitted inline, causing it to reverse order with `e_1` and
        `e_long`. The new algorithm scans starting at `e_long` and going all
        the way back up the expression until it reaches the end of the inlined
        statement. Caching of what has already been scanned has been added so that
        if there was a second long long `e_long2` after `e_long`, it would not
        rescan and re-inline the statements that were already split.
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21180394
    
        Pulled By: zdevito
    
        fbshipit-source-id: 4d142c83a04c89a47d04282f67a513f82cf153c0
    
    commit d6ce6570f96e8edbf450728a5bfa080f181bcba0
    Author: Hong Xu <hong@topbug.net>
    Date:   Fri Apr 24 15:08:39 2020 -0700
    
        Remove unused imports in aten/src/ATen/function_wrapper.py (#37245)
    
        Summary:
        typing is available since Python 3.5, no need to try-import.
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37245
    
        Differential Revision: D21236650
    
        Pulled By: albanD
    
        fbshipit-source-id: daf150103835d0c6cd3c39300044e548bb6d311d
    
    commit 4f3946a89b639f3b87c37b4190e2bc3dc22ee608
    Author: anjali411 <chourdiaanjali123@gmail.com>
    Date:   Fri Apr 24 15:03:38 2020 -0700
    
        Added complex dtypes to get_all_math_dtypes, complex acc type for cpu, fixed rdiv and pow for complex (#37193)
    
        Summary:
        Resolves https://github.com/pytorch/pytorch/issues/36730 https://github.com/pytorch/pytorch/issues/36057
        Partially resolves: https://github.com/pytorch/pytorch/issues/36671
        ```
        >>> 2j / torch.tensor([4], dtype = torch.complex64)
        tensor([(0.0000+0.5000j)], dtype=torch.complex64)
        >>> 1 / torch.tensor(3+4j)
        tensor((0.1200-0.1600j), dtype=torch.complex64)
        ```
        rdiv is more generally broken for all dtypes because it doesn't promote the types properly
        eg.
        ```
        >>> 1 / torch.tensor(2)
        tensor(0)
        >>> 2j / torch.tensor(4)
        tensor(0)
        ```
        so that issue should be fixed in a separate PR
    
        Adding CPU acc types for complex
        Added cumsum, cumprod for complex dtypes
    
        Added complex dtypes to get_all_math_dtypes to expand testing for complex dtypes
    
        Old PR - https://github.com/pytorch/pytorch/pull/36747
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37193
    
        Differential Revision: D21229373
    
        Pulled By: anjali411
    
        fbshipit-source-id: 8a086136d8c10dabe62358d276331e3f22bb2342
    
    commit c38dcd45d70b2850047d9956e45ff3312966a078
    Author: Wanchao Liang <wanchaol@users.noreply.github.com>
    Date:   Fri Apr 24 14:45:11 2020 -0700
    
        [jit] fix return different types bug in tracing module calls (#37190)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37190
    
        if module call return different types, we need to record them correctly
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21214871
    
        Pulled By: wanchaol
    
        fbshipit-source-id: 46ba98f08ed4ade22f9740cb3fca84b29557e125
    
    commit 5362a0b948450e2d2ba5f8ce2157a65b2f06b392
    Author: Wanchao Liang <wanchaol@users.noreply.github.com>
    Date:   Fri Apr 24 14:45:11 2020 -0700
    
        [jit] fix lifting bug in tracing module calls (#37189)
    
        Summary:
        Pull Request resolved: https://github.com/pytorch/pytorch/pull/37189
    
        This fix bug in tracing module calls to correct lift values with its
        correponding value type, rather than the default tensor type.
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21214872
    
        Pulled By: wanchaol
    
        fbshipit-source-id: f635154851365e2d7b88186d6e47634123eac42f
    
    commit a13b5b0ae85ea6b9ba6038f99658a88039e23782
    Author: Xiang Gao <qasdfgtyuiop@gmail.com>
    Date:   Fri Apr 24 14:16:54 2020 -0700
    
        Split reduction compile units (#37205)
    
        Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37205
    
        Test Plan: Imported from OSS
    
        Differential Revision: D21233254
    
        Pulled By: ngimel
    
        fbshipit-source-id: 68b37ebbdd715a30c616e425a39b6b21c01b37e2

commit ea741f829e825e8ff87ed67cd80a71d65fbb9c73
Author: Nikita Shulga <nshulga@fb.com>
Date:   Sat Apr 25 13:51:10 2020 -0700

    Add `--repeat` option to python unit-test (#37281)
    
    Summary:
    This would run same testsuite (or individual test) multiple time
    Useful for detecting flaky tests
    
    Example usage: `python test_autograd.py TestAutograd.test_profiler -v --repeat=100`
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/37281
    
    Differential Revision: D21244442
    
    Pulled By: malfet
    
    fbshipit-source-id: 3ecafec7ae87bc1e418aa28151bbc472ef37a713

commit f0a533c5dd81a92350bf43131322ec9cc99678ef
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Tue Apr 21 16:34:39 2020 -0700

    Fix flaky test_backward_node_failure_python_udf (#36969)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/36969
    
    `test_backward_node_failure_python_udf` was flaky since it used the
    RPC framework to indicate rank 0 was done with processing. Since we kill nodes
    in this unit test, it is very likely that listenLoop() has exited on some nodes
    and hence using an RPC to inform all nodes about rank 0's completion
    might not work, since the RPC might not be processed on certain nodes.
    
    To fix this, we use the c10d store instead for this notification.
    ghstack-source-id: 102549873
    
    Test Plan: waitforbuildbot
    
    Differential Revision: D21147099
    
    fbshipit-source-id: 745273a6cae0debbae131bb4cc7debe9c201bf98

commit 32307efd6885f74b242b02f65125fbe98784c805
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Mon Apr 20 18:52:41 2020 -0700

    Fix flaky test_barrier_timeout* tests for test_distributed. (#36963)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/36963
    
    A couple of reasons why these tests were flaky:
    1) Sometimes the error message for timeout would include lowercase 'timeout'
    which was not included in the regex.
    2) The timeout was 0.2 seconds, which was probably too small for ASAN/TSAN.
    ghstack-source-id: 102541231
    
    Test Plan: waitforbuildbot
    
    Differential Revision: D21144954
    
    fbshipit-source-id: 57945f53e1627028835cbfd2adb72f21d87f593f

commit d7bdffabed7cce0f12470e4618d0476a46407727
Author: Shen Li <shenli@devfair017.maas>
Date:   Mon Apr 20 07:55:29 2020 -0700

    [v1.5 Patch] Disable flaky test_backward_node_failure_python_udf test in dist_autograd_test.py
    
    This test is flaky on 1.5 release branch. Below is a failed CI run:
    https://app.circleci.com/pipelines/github/pytorch/pytorch/157331/workflows/b3e0bd6b-6c55-4d14-bde8-96b8345cf9e2/jobs/5190025

commit 4a49ad0da7e14cf2b61e853efd374ae38ae762e5
Author: Omkar Salpekar <osalpekar@fb.com>
Date:   Wed Apr 15 10:48:45 2020 -0700

    Fixed error Regex Parsing for Node Failure Tests (#36620)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/36620
    
    Sending to a node that has been shutdown in ProcessGroupAgent could throw several possible exceptions. This PR updates the tests to check for the right exceptions while waiting for other nodes in the gang to fail in `test_backward_node_failure` and `test_backward_node_failure_python_udf`.
    ghstack-source-id: 102153944
    
    Test Plan: Stress-tested `test_backward_node_failure` and `test_backward_node_failure_python_udf`. They were previously completely broken, but this change makes  `test_backward_node_failure`  functional and `test_backward_node_failure_python_udf` is flaky but fails infrequently. A change to make the last test work reliably is planned.
    
    Differential Revision: D21027280
    
    fbshipit-source-id: e85c2d219ee408483442bd9925fff7206c8efe4b

commit 2ef1ace877edb593b031edc3251e83cf470aa115
Author: Rohan Varma <rvarm1@fb.com>
Date:   Mon Apr 6 15:15:36 2020 -0700

    [rpc] call threadPool.waitWorkComplete after listenerThread.join() to fix (#35394)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35394
    
    As above
    ghstack-source-id: 101592571
    
    Test Plan: Existing CI, no longer flaky
    
    Differential Revision: D20632405
    
    fbshipit-source-id: fbfd81470b3361371109af341f0db3ef8b3a415b

commit 8ad59f03a83f05b7737800a7a99efa6893538a6a
Author: Gregory Chanan <gchanan@fb.com>
Date:   Thu Apr 2 08:26:20 2020 -0700

    Skip ROCm test in test/test_cpp_extensions_aot.py (#35838)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35838
    
    It may be flaky.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D20807409
    
    Pulled By: gchanan
    
    fbshipit-source-id: f085d05bcb6a04d304f3cd048c38d2e8453125d6

commit 1bd68eafb53b5264df8db4e84589509d5f10163b
Author: Gregory Chanan <gchanan@fb.com>
Date:   Thu Apr 2 08:26:20 2020 -0700

    Skip ROCm test in test/test_cpp_extensions_aot.py (#35838)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35838
    
    It may be flaky.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D20807409
    
    Pulled By: gchanan
    
    fbshipit-source-id: f085d05bcb6a04d304f3cd048c38d2e8453125d6

commit 299bd6d701263cd035d24c82253c433505adb250
Author: peter <peterghost86@gmail.com>
Date:   Thu Mar 26 07:31:37 2020 -0700

    Update randomtemp on Windows (#35375)
    
    Summary:
    Introduce max retry times to the flaky CUDA build command.
    Changes: https://github.com/peterjc123/randomtemp/compare/v0.2...v0.3
    Targets https://github.com/pytorch/pytorch/issues/25393#issuecomment-603776413.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35375
    
    Differential Revision: D20653082
    
    Pulled By: ezyang
    
    fbshipit-source-id: a609379af680ac15ad24c9e2e5b330ffba3d1149

commit 082e48e3462832fa308225dd7014ee6ec53e743e
Author: Michael Suo <suo@fb.com>
Date:   Sun Mar 22 18:46:45 2020 -0700

    skip ctc_loss test on Windows (#35069)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/35069
    
    It is flaky on Windows only, so disable for now:
    https://github.com/pytorch/pytorch/issues/34870
    
    Test Plan: Imported from OSS
    
    Differential Revision: D20544736
    
    Pulled By: suo
    
    fbshipit-source-id: 49e35a4b4f0d1d20157769a4dff22cb4fe86770c

commit d2d26bf6434b39f01b7c60e99e72351b91711066
Author: Rohan Varma <rvarm1@fb.com>
Date:   Thu Mar 19 16:10:09 2020 -0700

    [rpc] fix test_debug_info for python 3.5 (#34828)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/34828
    
    python 3.5 does not ensure ordering of dictionary keys, this was added
    in python 3.6+. Fixing this so the test is no longer flaky in 3.5. Tested by
    500 stresstests with python 3.5
    ghstack-source-id: 100426555
    
    Test Plan: 500 stress tests in python 3.5
    
    Differential Revision: D20474996
    
    fbshipit-source-id: 89b614a32363d1e7f3f7a4f27bec4fd7d507721d

commit ff3d205ee5a253fb020494ab561502c542deb114
Author: Rohan Varma <rvarm1@fb.com>
Date:   Tue Mar 17 18:57:39 2020 -0700

    [rpc] handle exceptions in ProcessGroupAgent::enqueueRecv (#34413)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/34413
    
    In this diff we have made various improvements to ProcessGroupAgent in order to accomodate edge and error cases such as a "non-clean" shutdown (shutdowns in which we abort RPC as quickly as possible, and don't wait for all pending work across all RPC agents to be completed):
    
    1. Catch and log exceptions in `enqueueRecv`. This prevents us from calling `std::terminate()` in a different thread and logs an error message indicating the issue. With this we no longer have crashes caused by exceptions in this thread during non-graceful shutdown.
    
    2. Provide cleaner error messages everywhere (and use `c10::str` where possible). One example is in `agent::send()`.
    
    3. Add the ability to abort pending sends that cause blocking waits in `handleSend`. The reason we need to abort this is since during a non-graceful shutdown, we could become blocked waiting for these since there is no guarantee the remote end is still active and this would result in a long wait and eventual timeout. We abort these by adding them to a map, and go through this map during `shutdown()`.
    
    4. Fix flaky tests: `test_handle_send_exceptions` and `test_backward_node_failure` and `test_backward_node_failure_python_udf`. These tests were flaky since they dealt with non-graceful shutdown of workers which has chances for a bunch of edge cases explained above.
    
    We have also refactored `createExceptionResponse`, `enqueueRecv`, and some test functions for the above reasons in this diff.
    
    For testing:
    Ensured that the tests are no longer flaky with 500 tests runs. Previously, these tests were flaky and disabled. Also added a unit test in the internal `ProcessGroupAgentTest.cpp`.
    ghstack-source-id: 100311598
    
    Test Plan: Ensured that the tests are no longer flaky with 500 tests runs. Previously, these tests were flaky and disabled. Also added a unit test in the internal `ProcessGroupAgentTest.cpp`.
    
    Reviewed By: mrshenli
    
    Differential Revision: D20269074
    
    fbshipit-source-id: de9cad7f7185f9864ffbb6b14cd8ca9f6ff8f465

commit 31eaeba38a0f2970d6c76103a349f35f57ee35b5
Author: Xiang Gao <qasdfgtyuiop@gmail.com>
Date:   Mon Mar 16 15:03:08 2020 -0700

    Increase the prec of test_baddbmm (#34764)
    
    Summary:
    This test is flaky on my computer, the error is:
    ```
    AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05
    ```
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/34764
    
    Differential Revision: D20476006
    
    Pulled By: ezyang
    
    fbshipit-source-id: dad7e702275346070552c8a98765c37e6ca2c197

commit 3f1ba3c465769fddf56eacfed8d905a47ec2f1ef
Author: Nathan Goldbaum <nathan12343@gmail.com>
Date:   Thu Mar 12 10:19:59 2020 -0700

    Redo of "Add API for listing functions overridable by __torch_function__" (#34240)
    
    Summary:
    This is a redo of https://github.com/pytorch/pytorch/pull/33791, which was reverted because it introduced a flaky test. The test was flaky and only flaky on Python3.5 because of dict order randomization.
    
    I've fixed the issue with tests clobbering each other in b539fec and removed the override tests for `torch.nn.functional.tanh` and `torch.nn.functional.sigmoid`, which are deprecated and shouldn't be overridable in e0d7402. I also verified that no more test clobbering is happening.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/34240
    
    Differential Revision: D20252442
    
    Pulled By: cpuhrsch
    
    fbshipit-source-id: 069568e342a41c90e1dc76cbf85ba4aed47f24be

commit 15caf3b5163797e764d3f8af9e92bf1862263162
Author: Yanli Zhao <yanlizhao@fb.com>
Date:   Sun Mar 1 14:14:28 2020 -0800

    move test helper functions out of test funciton (#33960)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33960
    
    test helper functions should be out of test function. it is possible process 2 launches test functions slower than process 1, and process 1 sends request to run a helper function on process 2. process 2 may have not compile the helper function yet when process 2 starts to serve processs 1's request, and thus may return error like "attempted to get undefined function"
    ghstack-source-id: 99205620
    
    Test Plan: test_remote_script_module was flaky for thrift backend in my local stress test runs, due to error "attempted to get undefined function". With fix in this diff, stress runs passed
    
    Differential Revision: D20167969
    
    fbshipit-source-id: 8a2b9cd7bd62462e24bdbcb69ad32dca745d6956

commit e10aa6b72fa63d327d2a8db3a9beb39d38908bd6
Author: Dmytro Dzhulgakov <dzhulgakov@fb.com>
Date:   Fri Feb 21 16:01:48 2020 -0800

    Fix flaky DagNetTest unittest
    
    Summary: The first run of the net is noisy sometimes - just run it twice.
    
    Reviewed By: cheshen1
    
    Differential Revision: D20039274
    
    fbshipit-source-id: 639e65646bf52f3efe1ecd4bbcd0e413d9389b29

commit 11f15f874457dd8cedd3b3f79bf8268d70c8865d
Author: Jianyu Huang <jianyuhuang@fb.com>
Date:   Thu Feb 20 22:49:59 2020 -0800

    Update on "[pt][quant] Enable the dynamic quant LSTM unit test by removing hypothesis"
    
    
    Check more discussions in https://github.com/pytorch/pytorch/issues/32644.
    
    Previously, after landing https://github.com/pytorch/pytorch/pull/32757, we found the flaky tests on `test_quantized_rnn` in some specific CI environment, where we cannot reproduce in our devserver.
    
    The error is reported as it failed the `scale > 0` check in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/quant_utils.h#L69 .
    
    - Step 1: we disabled the unit tests in https://github.com/pytorch/pytorch/pull/32742
    
    - Step 2: we investigated the FBGEMM `ChooseQuantizationParams` routine and added assert for min, max, qmin, qmax, but we didn't find the abnormal numbers :
    https://github.com/pytorch/pytorch/pull/32739
    
    scale is calculated as the following:
    float scale = (static_cast<double>(max) - min) / (qmax - qmin);
    
    If max is larger than min, and qmax is larger than qmin, then scale should be always positive.
    
    - Step 3: we tried to print the intermediate result of x_min and x_max in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp#L59-L63. Check https://github.com/pytorch/pytorch/pull/33167 for the places we add the prints.
    
    One interesting test case caused the failure:
    ```
    Jan 29 23:46:57 x_min: -nan, x_max: -nan
    Jan 29 23:46:57 input: -nan 0.195275 -0.380797 -0.510193 -0.662963 0.761594 0.923773 -nan -0.123308 -0.380797 -0.761594 -0.603171 0.906278 0.000028 -nan 0.462941 -0.380797 -0.651336 0.717926 0.761594 0.460751 -nan 0.945544 -0.380797 -0.334840 0.831535 0.761594 0.793159 -nan -0.752703 -0.380797 -0.761594 0.809177 0.761594 0.955718 -nan -0.717292 -0.380797 -0.650404 0.790869 0.761594 0.812008 -nan 0.917088 -0.380797 -0.761594 0.829536 0.761594 0.000000 -nan 0.855218 -0.380796 -0.036113 0.815578 0.761594 0.942870 -nan -0.675926 -0.380797 -0.755426 -0.658459 0.000000 0.000000 -nan 0.490974 -0.380797 -0.725917 -0.539679 0.761594 0.836110 -nan -0.714314 -0.380797 -0.761594 -0.738534 0.761594 0.791485 -nan 0.345541 -0.380797 -0.670953 -0.609272 0.962786 0.834005 -nan -0.583866 -0.380797 -0.761594 -0.620485 0.750284 0.854029 -nan -0.661655 -0.380797 -0.282387 0.863691 0.761593 0.956279 -nan -0.667277 -0.380797 -0.730052 -0.732839 0.761594 0.002185 -nan 0.380570 -0.380797 -0.453875 -0.523069 0.834210 0.792132
    ```
    
    More detailed test results are here:
    https://circleci.com/gh/pytorch/pytorch/4374802?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
    
    Basically, x_min and x_max are both -nan, which failed the check for x_min <= x_max. Actually if any operands for a comparison is nan (e.g., nan < 3, nan < nan, nan <= nan), the result for the comparison will always be False. The culprit should be this NaN value.
    
    The issue is where we generated such NaN value?
    
    - Step 4:
    We analyzed that NaN can possibly come from two sources:
    
        - Unit test: when when we initialize the `torch.Tensor` or `torch.rand` somewhere, it generates NaN number;
    
        - Somewhere in `aten/src/ATen/native/RNN.cpp` or `torch/nn/quantized/dynamic/modules/rnn.py`, we used some illegal instructions.
    
    As there is no such issue before https://github.com/pytorch/pytorch/pull/32757, we suspect some changes in that PR triggered this issue.
    
    In https://github.com/pytorch/pytorch/pull/33167, we avoided using hypothesis by removing `@given(qengine=st.sampled_from(("qnnpack", "fbgemm"))):` and all tests pass.
    
    It looks like that the hypothesis added in https://github.com/pytorch/pytorch/pull/32757 triggered the generation of NaN in some input tensors.
    
    
    - Step 5:
    
    We still need to find the root cause for NaN, either it is the problem of hypothesis itself, or hypothesis generated more random cases which helped triggered some NaN numbers in `RNN.cpp` or `rnn.py`.
    
    Differential Revision: [D20016038](https://our.internmc.facebook.com/intern/diff/D20016038/)
    
    [ghstack-poisoned]

commit bc0cd1b02094252ff321f996f7eeafb429122fa2
Author: Jianyu Huang <jianyuhuang@fb.com>
Date:   Thu Feb 20 22:49:59 2020 -0800

    [pt][quant] Enable the dynamic quant LSTM unit test by removing hypothesis
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33594
    
    
    
    Check more discussions in https://github.com/pytorch/pytorch/issues/32644.
    
    Previously, after landing https://github.com/pytorch/pytorch/pull/32757, we found the flaky tests on `test_quantized_rnn` in some specific CI environment, where we cannot reproduce in our devserver.
    
    The error is reported as it failed the `scale > 0` check in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/quant_utils.h#L69 .
    
    - Step 1: we disabled the unit tests in https://github.com/pytorch/pytorch/pull/32742
    
    - Step 2: we investigated the FBGEMM `ChooseQuantizationParams` routine and added assert for min, max, qmin, qmax, but we didn't find the abnormal numbers :
    https://github.com/pytorch/pytorch/pull/32739
    
    scale is calculated as the following:
    float scale = (static_cast<double>(max) - min) / (qmax - qmin);
    
    If max is larger than min, and qmax is larger than qmin, then scale should be always positive.
    
    - Step 3: we tried to print the intermediate result of x_min and x_max in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp#L59-L63. Check https://github.com/pytorch/pytorch/pull/33167 for the places we add the prints.
    
    One interesting test case caused the failure:
    ```
    Jan 29 23:46:57 x_min: -nan, x_max: -nan
    Jan 29 23:46:57 input: -nan 0.195275 -0.380797 -0.510193 -0.662963 0.761594 0.923773 -nan -0.123308 -0.380797 -0.761594 -0.603171 0.906278 0.000028 -nan 0.462941 -0.380797 -0.651336 0.717926 0.761594 0.460751 -nan 0.945544 -0.380797 -0.334840 0.831535 0.761594 0.793159 -nan -0.752703 -0.380797 -0.761594 0.809177 0.761594 0.955718 -nan -0.717292 -0.380797 -0.650404 0.790869 0.761594 0.812008 -nan 0.917088 -0.380797 -0.761594 0.829536 0.761594 0.000000 -nan 0.855218 -0.380796 -0.036113 0.815578 0.761594 0.942870 -nan -0.675926 -0.380797 -0.755426 -0.658459 0.000000 0.000000 -nan 0.490974 -0.380797 -0.725917 -0.539679 0.761594 0.836110 -nan -0.714314 -0.380797 -0.761594 -0.738534 0.761594 0.791485 -nan 0.345541 -0.380797 -0.670953 -0.609272 0.962786 0.834005 -nan -0.583866 -0.380797 -0.761594 -0.620485 0.750284 0.854029 -nan -0.661655 -0.380797 -0.282387 0.863691 0.761593 0.956279 -nan -0.667277 -0.380797 -0.730052 -0.732839 0.761594 0.002185 -nan 0.380570 -0.380797 -0.453875 -0.523069 0.834210 0.792132
    ```
    
    More detailed test results are here:
    https://circleci.com/gh/pytorch/pytorch/4374802?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
    
    Basically, x_min and x_max are both -nan, which failed the check for x_min <= x_max. Actually if any operands for a comparison is nan (e.g., nan < 3, nan < nan, nan <= nan), the result for the comparison will always be False. The culprit should be this NaN value.
    
    The issue is where we generated such NaN value?
    
    - Step 4:
    We analyzed that NaN can possibly come from two sources:
    
        - Unit test: when when we initialize the `torch.Tensor` or `torch.rand` somewhere, it generates NaN number;
    
        - Somewhere in `aten/src/ATen/native/RNN.cpp` or `torch/nn/quantized/dynamic/modules/rnn.py`, we used some illegal instructions.
    
    As there is no such issue before https://github.com/pytorch/pytorch/pull/32757, we suspect some changes in that PR triggered this issue.
    
    In https://github.com/pytorch/pytorch/pull/33167, we avoided using hypothesis by removing `@given(qengine=st.sampled_from(("qnnpack", "fbgemm"))):` and all tests pass.
    
    It looks like that the hypothesis added in https://github.com/pytorch/pytorch/pull/32757 triggered the generation of NaN in some input tensors.
    
    
    - Step 5:
    
    We still need to find the root cause for NaN, either it is the problem of hypothesis itself, or hypothesis generated more random cases which helped triggered some NaN numbers in `RNN.cpp` or `rnn.py`.
    ghstack-source-id: 98710588
    
    Differential Revision: [D20016038](https://our.internmc.facebook.com/intern/diff/D20016038/)

commit 8f885c72fc05e70bed23a5e5ea02d0bb97cf3bb4
Author: Jianyu Huang <jianyuhuang@fb.com>
Date:   Thu Feb 20 22:08:56 2020 -0800

    Update on "[pt][quant] Enable the dynamic quant LSTM unit test by removing hypothesis"
    
    
    Check more discussions in https://github.com/pytorch/pytorch/issues/32644.
    
    Previously, after landing https://github.com/pytorch/pytorch/pull/32757, we found the flaky tests on `test_quantized_rnn` in some specific CI environment, where we cannot reproduce in our devserver.
    
    The error is reported as it failed the `scale > 0` check in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/quant_utils.h#L69 .
    
    - Step 1: we disabled the unit tests in https://github.com/pytorch/pytorch/pull/32742
    
    - Step 2: we investigated the FBGEMM `ChooseQuantizationParams` routine and added assert for min, max, qmin, qmax, but we didn't find the abnormal numbers :
    https://github.com/pytorch/pytorch/pull/32739
    
    scale is calculated as the following:
    float scale = (static_cast<double>(max) - min) / (qmax - qmin);
    
    If max is larger than min, and qmax is larger than qmin, then scale should be always positive.
    
    - Step 3: we tried to print the intermediate result of x_min and x_max in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp#L59-L63. Check https://github.com/pytorch/pytorch/pull/33167 for the places we add the prints.
    
    One interesting test case caused the failure:
    ```
    Jan 29 23:46:57 x_min: -nan, x_max: -nan
    Jan 29 23:46:57 input: -nan 0.195275 -0.380797 -0.510193 -0.662963 0.761594 0.923773 -nan -0.123308 -0.380797 -0.761594 -0.603171 0.906278 0.000028 -nan 0.462941 -0.380797 -0.651336 0.717926 0.761594 0.460751 -nan 0.945544 -0.380797 -0.334840 0.831535 0.761594 0.793159 -nan -0.752703 -0.380797 -0.761594 0.809177 0.761594 0.955718 -nan -0.717292 -0.380797 -0.650404 0.790869 0.761594 0.812008 -nan 0.917088 -0.380797 -0.761594 0.829536 0.761594 0.000000 -nan 0.855218 -0.380796 -0.036113 0.815578 0.761594 0.942870 -nan -0.675926 -0.380797 -0.755426 -0.658459 0.000000 0.000000 -nan 0.490974 -0.380797 -0.725917 -0.539679 0.761594 0.836110 -nan -0.714314 -0.380797 -0.761594 -0.738534 0.761594 0.791485 -nan 0.345541 -0.380797 -0.670953 -0.609272 0.962786 0.834005 -nan -0.583866 -0.380797 -0.761594 -0.620485 0.750284 0.854029 -nan -0.661655 -0.380797 -0.282387 0.863691 0.761593 0.956279 -nan -0.667277 -0.380797 -0.730052 -0.732839 0.761594 0.002185 -nan 0.380570 -0.380797 -0.453875 -0.523069 0.834210 0.792132
    ```
    
    More detailed test results are here:
    https://circleci.com/gh/pytorch/pytorch/4374802?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
    
    Basically, x_min and x_max are both -nan, which failed the check for x_min <= x_max. Actually if any operands for a comparison is nan (e.g., nan < 3, nan < nan, nan <= nan), the result for the comparison will always be False. The culprit should be this NaN value.
    
    The issue is where we generated such NaN value?
    
    - Step 4:
    We analyzed that NaN can possibly come from two sources:
    
        - Unit test: when when we initialize the `torch.Tensor` or `torch.rand` somewhere, it generates NaN number;
    
        - Somewhere in `aten/src/ATen/native/RNN.cpp` or `torch/nn/quantized/dynamic/modules/rnn.py`, we used some illegal instructions.
    
    As there is no such issue before https://github.com/pytorch/pytorch/pull/32757, we suspect some changes in that PR triggered this issue.
    
    In https://github.com/pytorch/pytorch/pull/33167, we avoided using hypothesis by removing `@given(qengine=st.sampled_from(("qnnpack", "fbgemm"))):` and all tests pass.
    
    It looks like that the hypothesis added in https://github.com/pytorch/pytorch/pull/32757 triggered the generation of NaN in some input tensors.
    
    
    - Step 5:
    
    We still need to find the root cause for NaN, either it is the problem of hypothesis itself, or hypothesis generated more random cases which helped triggered some NaN numbers in `RNN.cpp` or `rnn.py`.
    
    Differential Revision: [D20016038](https://our.internmc.facebook.com/intern/diff/D20016038/)
    
    [ghstack-poisoned]

commit ccb9d2f349a64e50a62e57a7ea01afecb4addef1
Author: Jianyu Huang <jianyuhuang@fb.com>
Date:   Thu Feb 20 17:01:19 2020 -0800

    [pt][quant] Enable the dynamic quant LSTM unit test by removing hypothesis
    
    Check more discussions in https://github.com/pytorch/pytorch/issues/32644.
    
    Previously, after landing https://github.com/pytorch/pytorch/pull/32757, we found the flaky tests on `test_quantized_rnn` in some specific CI environment, where we cannot reproduce in our devserver.
    
    The error is reported as it failed the `scale > 0` check in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/quant_utils.h#L69 .
    
    - Step 1: we disabled the unit tests in https://github.com/pytorch/pytorch/pull/32742
    
    - Step 2: we investigated the FBGEMM `ChooseQuantizationParams` routine and added assert for min, max, qmin, qmax, but we didn't find the abnormal numbers :
    https://github.com/pytorch/pytorch/pull/32739
    
    scale is calculated as the following:
    float scale = (static_cast<double>(max) - min) / (qmax - qmin);
    
    If max is larger than min, and qmax is larger than qmin, then scale should be always positive.
    
    - Step 3: we tried to print the intermediate result of x_min and x_max in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp#L59-L63. Check https://github.com/pytorch/pytorch/pull/33167 for the places we add the prints.
    
    One interesting test case caused the failure:
    ```
    Jan 29 23:46:57 x_min: -nan, x_max: -nan
    Jan 29 23:46:57 input: -nan 0.195275 -0.380797 -0.510193 -0.662963 0.761594 0.923773 -nan -0.123308 -0.380797 -0.761594 -0.603171 0.906278 0.000028 -nan 0.462941 -0.380797 -0.651336 0.717926 0.761594 0.460751 -nan 0.945544 -0.380797 -0.334840 0.831535 0.761594 0.793159 -nan -0.752703 -0.380797 -0.761594 0.809177 0.761594 0.955718 -nan -0.717292 -0.380797 -0.650404 0.790869 0.761594 0.812008 -nan 0.917088 -0.380797 -0.761594 0.829536 0.761594 0.000000 -nan 0.855218 -0.380796 -0.036113 0.815578 0.761594 0.942870 -nan -0.675926 -0.380797 -0.755426 -0.658459 0.000000 0.000000 -nan 0.490974 -0.380797 -0.725917 -0.539679 0.761594 0.836110 -nan -0.714314 -0.380797 -0.761594 -0.738534 0.761594 0.791485 -nan 0.345541 -0.380797 -0.670953 -0.609272 0.962786 0.834005 -nan -0.583866 -0.380797 -0.761594 -0.620485 0.750284 0.854029 -nan -0.661655 -0.380797 -0.282387 0.863691 0.761593 0.956279 -nan -0.667277 -0.380797 -0.730052 -0.732839 0.761594 0.002185 -nan 0.380570 -0.380797 -0.453875 -0.523069 0.834210 0.792132
    ```
    
    More detailed test results are here:
    https://circleci.com/gh/pytorch/pytorch/4374802?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
    
    Basically, x_min and x_max are both -nan, which failed the check for x_min <= x_max. Actually if any operands for a comparison is nan (e.g., nan < 3, nan < nan, nan <= nan), the result for the comparison will always be False. The culprit should be this NaN value.
    
    The issue is where we generated such NaN value?
    
    - Step 4:
    We analyzed that NaN can possibly come from two sources:
    
        - Unit test: when when we initialize the `torch.Tensor` or `torch.rand` somewhere, it generates NaN number;
    
        - Somewhere in `aten/src/ATen/native/RNN.cpp` or `torch/nn/quantized/dynamic/modules/rnn.py`, we used some illegal instructions.
    
    As there is no such issue before https://github.com/pytorch/pytorch/pull/32757, we suspect some changes in that PR triggered this issue.
    
    In https://github.com/pytorch/pytorch/pull/33167, we avoided using hypothesis by removing `@given(qengine=st.sampled_from(("qnnpack", "fbgemm"))):` and all tests pass.
    
    It looks like that the hypothesis added in https://github.com/pytorch/pytorch/pull/32757 triggered the generation of NaN in some input tensors.
    
    
    - Step 5:
    
    We still need to find the root cause for NaN, either it is the problem of hypothesis itself, or hypothesis generated more random cases which helped triggered some NaN numbers in `RNN.cpp` or `rnn.py`.
    
    Differential Revision: [D20016038](https://our.internmc.facebook.com/intern/diff/D20016038/)
    
    [ghstack-poisoned]

commit ffe327f7d9c0bf3f6d2fc64fd4dfb8b2c2013be8
Author: peter <peterghost86@gmail.com>
Date:   Thu Feb 20 09:06:26 2020 -0800

    Revert "Disable flaky test TestCppExtensionAOT.test_cuda_extension in… (#33404)
    
    Summary:
    … Windows CI (https://github.com/pytorch/pytorch/issues/33282)"
    
    This reverts commit 5b922918d023126ad1f468c68577c9b599ad202d.
    
    Fixes https://github.com/pytorch/pytorch/issues/33270.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33404
    
    Differential Revision: D19972594
    
    Pulled By: ezyang
    
    fbshipit-source-id: c8f67536fd6e4b7135171d621ad671b1b2a21fd4

commit 6cb9e6b0158d3c9c0843c6c02335547a12217dfe
Author: Rohan Varma <rvarm1@fb.com>
Date:   Wed Feb 19 17:15:32 2020 -0800

    Back out "Revert D19871946: [distributed] pass in timeout to TCP store when initializing" (#33434)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33434
    
    Reland of https://github.com/pytorch/pytorch/pull/33325, since the
    unit test was flaky and failed on land.
    To ensure that the test is not flaky, I bumped the timeout so the rendezvous
    does not timeout (timing out the rendezvous in 1s led to the flakiness). I also
    generalized our mechanism for retrying on errors to include retrying on errors
    due to timeout in rendezvous.
    ghstack-source-id: 98558377
    
    Test Plan: Added UT test_tcp_store_timeout_set
    
    Differential Revision: D19935390
    
    fbshipit-source-id: 56ccf8c333dd2f954a33614d35cd1642d4e9473a

commit 5b922918d023126ad1f468c68577c9b599ad202d
Author: Will Feng <willfeng@fb.com>
Date:   Thu Feb 13 13:08:16 2020 -0800

    Disable flaky test TestCppExtensionAOT.test_cuda_extension in Windows CI (#33282)
    
    Summary:
    See https://github.com/pytorch/pytorch/issues/33270 for details.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33282
    
    Differential Revision: D19886975
    
    Pulled By: yf225
    
    fbshipit-source-id: 7e6756095b1bb8c55fc5acb8fc2cb02c1e89b032

commit 47e589eb6e322fe6d4a752a1e8ef0adab78b7dc1
Author: Jithun Nair <jithun.nair@amd.com>
Date:   Tue Feb 11 22:47:11 2020 -0800

    Disable flaky tests test_DistributedDataParallel and test_backend_group for ROCm (#33211)
    
    Summary:
    Getting intermittent error in CI runs:
    
    **TestDistBackend.test_DistributedDataParallel**
    ```
    02:36:32   File "/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/serialization.py", line 442, in _legacy_save
    02:36:32     pickler.dump(obj)
    02:36:32 AttributeError: Can't pickle local object 'Module._replicate_for_data_parallel.<locals>.zero_grad'
    ```
    Some CI runs where it failed:
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16163/console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16165/console
    
    **TestDistBackend.test_backend_group**
    ```
    test_backend_group (__main__.TestDistBackend) ... Memory access fault by GPU node-5 (Agent handle: 0x265c670) on address 0x7fded754a000. Reason: Page not present or supervisor privilege.
    ```
    Some CI runs where it failed:
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/16288/console
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33211
    
    Differential Revision: D19849089
    
    Pulled By: bddppq
    
    fbshipit-source-id: 5e997653cc344f4c6819d46bedc6d3bd75b5d854

commit ad90c97c0ac455546b2b6353a22caa25dcf0574d
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Tue Feb 11 12:19:06 2020 -0800

    Removes flaky check (#33146)
    
    Summary:
    Addresses https://github.com/pytorch/pytorch/issues/32949.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/33146
    
    Differential Revision: D19836001
    
    Pulled By: mruberry
    
    fbshipit-source-id: 773069ae0c181e1a050b65b888c87590c1dddb32

commit 3c4cec56aaa3c5094249fdb1a9a139407ceb305c
Author: Jithun Nair <37884920+jithunnair-amd@users.noreply.github.com>
Date:   Mon Feb 10 12:36:56 2020 -0800

    Enable test_distributed for ROCm but only with nccl backend [REDUX] (#32551)
    
    Summary:
    This is a redux of the original PR https://github.com/pytorch/pytorch/issues/28814 which was reverted in PR https://github.com/pytorch/pytorch/issues/29736 due to test_DistributedDataParallel being suspected as being flaky. Further investigation revealed it wasn't flakiness, but a bug in the PyTorch source code which has been now fixed in PR https://github.com/pytorch/pytorch/issues/32356. This PR is another attempt at enabling the test_distributed unit test suite only for the nccl backend.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32551
    
    Differential Revision: D19729966
    
    Pulled By: bddppq
    
    fbshipit-source-id: 12a0d850991a903cc7723d63693b6157071d7115

commit 5c019fede378db791d13da28c96b8c968e0e11ed
Author: neginraoof <neginmr@utexas.edu>
Date:   Mon Feb 3 14:21:48 2020 -0800

    [ONNX] Fix for constant folding flaky tests (#32546)
    
    Summary:
    Fix for constant folding flaky tests
    Looks like the constant folding test modules are sometimes exported with ONNX_ATEN op export type, which is causing the CI failures.
    I'm unable to repro this issue locally, but my guess is that the op export param is being overwritten on CI build at some point.
    This PR sets the op export type and hopefully fixes the issue.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32546
    
    Reviewed By: hl475
    
    Differential Revision: D19606919
    
    Pulled By: houseroad
    
    fbshipit-source-id: 31793d6857bbbf99b43b4a7c22a045a56ae19e44

commit ad78c0f4fc880c0da5064ccb85290c38dbe252ab
Author: Omkar Salpekar <osalpekar@fb.com>
Date:   Fri Jan 31 16:51:37 2020 -0800

    Fixed the flaky test_rref_context_debug_info (#32749)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32749
    
    The test was flaky since the message from owner RRef confirming fork would arrive after the test checked whether the pending User RRefs map was empty - leading to an assertion error. This diff creates a utility function that should be used by any test to wait for this message to complete processing before doing any assertions related to the pending User RRefs map.
    
    GitHub Issue: https://github.com/pytorch/pytorch/issues/30988
    
    Test Plan: Stress tested `test_rref_context_debug_info` 200 times.
    
    Differential Revision: D19612289
    
    fbshipit-source-id: 57a7c19b1cf792b94c263d3efbbbb6da60c07d07

commit 9de3208449940374f1928eb6840a245a55aa90a5
Author: Rohan Varma <rvarm1@fb.com>
Date:   Tue Jan 28 12:38:29 2020 -0800

    [rpc][flaky-tests] fix for test_handle_send_exceptions and (#32656)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32656
    
    Fixes these flaky tests.
    
    Test Plan: Run the test 500 times and verify that it succeeds every time.
    
    Differential Revision: D19584453
    
    fbshipit-source-id: 07cbc4914211f274182ac0fa74bb5ef6d43392d1

commit b3848c568e3434896278bec59ac126ed02e894e1
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Mon Jan 27 21:08:07 2020 -0800

    Fix flaky test_nccl_timeout. (#32653)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32653
    
    This test was flaky since the watchdog thread could abort the
    communicator instead of the thread calling `wait()`. As a result, we could
    actually see `NCCL error` instead of `Operation timed out` on the user end.
    ghstack-source-id: 97250714
    
    Test Plan: waitforbuildbot
    
    Differential Revision: D19583003
    
    fbshipit-source-id: 5c07326d1a16f214dcdbabed97ca613e0a5b42b9

commit 904ab092c286a750a270b4fd66ffd3a8baf39302
Author: Rohan Varma <rvarm1@fb.com>
Date:   Fri Jan 17 03:55:28 2020 -0800

    fix testSend and testRecv in ProcessGroupGlooTest (#32134)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/32134
    
    These tests weren't written in the most correct way and were often
    flaky. It was tricky to identify these tests as flaky until we moved this file
    to use gtest.
    
    The gist of the issue is that the test previously would not coordinate sends
    and recvs properly. For example, we created a single thread to test an
    abortRecv and a successful recv. A separate sender thread was used to send 2
    messages. What could go wrong here is that the first send could successfully
    complete, resulting in the receiving end processing the message before it gets
    the abort signal. In this case we would have an error in the test.
    ghstack-source-id: 96806879
    
    Differential Revision: D19379395
    
    fbshipit-source-id: 24782ccaf6e6ec6b445378b29d5f10f901e0dee6

commit 0f3f4ec64c3a58fd318a1271306f96a025c1784b
Author: James Reed <jamesreed@fb.com>
Date:   Fri Dec 6 13:33:20 2019 -0800

    Kill hypothesis deadline testing (#30890)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30890
    
    We've received way too many complaints about this functionality making tests flaky, and it's not providing value to us anyway. Let's cut the shit and kill deadline testing
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18857597
    
    Pulled By: jamesr66a
    
    fbshipit-source-id: 67e3412795ef2fb7b7ee896169651084e434d2f6

commit c829c6f3d2a6b2d521ebe0965fd2a27fd128d5ce
Author: Shen Li <cs.shenli@gmail.com>
Date:   Fri Jan 3 16:58:26 2020 -0800

    Disable flaky test_debug_info
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31847
    
    Test Plan: Imported from OSS
    
    Differential Revision: D19278009
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 652fa6741a48f35d9f8f54534e84d64fdd96b439

commit 39297bfe08af11a63a06037020a01cfa18676f70
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Mon Dec 30 18:05:17 2019 -0800

    Fix flaky test_debug_info. (#31675)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/31675
    
    This test could be flaky since there could be inflight RPC requests as
    part of startup which might not have finished. As a result, if they finish
    between the different calls to retrieve debug_info, there could be a problem
    since we would report separate information. As a result, we wait to ensure
    the metrics stabilize to avoid flakiness.
    ghstack-source-id: 96188488
    
    Test Plan: waitforbuildbot
    
    Differential Revision: D19242588
    
    fbshipit-source-id: 8f3db7e7365acbd3742e6ec0c2ddcca68f27db9e

commit 319bd5d43151b8c4246198e3feb88c474eb589e6
Author: gchanan <gchanan@fb.com>
Date:   Thu Dec 26 16:20:37 2019 -0500

    Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369) (#31637)
    
    Summary:
    Related to https://github.com/pytorch/pytorch/issues/31368
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/31369
    
    Co-authored-by: Vitaly Fedyunin <vitalyf@fb.com>

commit fe76af96eddbf62df0d2722220b907e56e3ee8e6
Author: Rohan Varma <rvarm1@fb.com>
Date:   Sun Dec 22 17:59:50 2019 -0800

    fix test_process_group_debug_info flaky test (#31533)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/31533
    
    Fixes this test that was flaky and has been disabled (see
    https://github.com/pytorch/pytorch/issues/31112)
    ghstack-source-id: 96038999
    
    Test Plan: Run the test 1000 times and ensure that it passes.
    
    Differential Revision: D19203366
    
    fbshipit-source-id: 7978cbb8ca0989a0a370a36349cdd4db3bb8345b

commit c5d2758c35ef7b7d1de33e2c7b07a4558ba0382b
Author: Vitaly Fedyunin <vitalyf@fb.com>
Date:   Tue Dec 17 19:15:12 2019 -0800

    Disable flaky TestMomentumSGD.test_fp16momentum_sgd (#31369)
    
    Summary:
    Related to https://github.com/pytorch/pytorch/issues/31368
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/31369
    
    Differential Revision: D19147072
    
    Pulled By: VitalyFedyunin
    
    fbshipit-source-id: 6fad13be7b35f992d84a20f23877cad05ff18616

commit dab5f725431bc1f51cf694c75254f2609570a4b5
Author: Zachary DeVito <zdevito@fb.com>
Date:   Tue Dec 17 11:55:50 2019 -0800

    we should have a config-based way to skip flaky tests (#30978)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30978
    
    This particular approach queries our issue tracker for test titles that
    match the following format:
    
    ```
    DISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)
    ```
    
    And then skips the python test for them. There is 1 second timeout so
    if the internet flakes we still run the test suite, without disabling any
    tests.
    
    This is intended as a quick fix, similar to ninja unland, to get to a green
    master. Long term test disables should go into the code.
    
    Test Plan: Imported from OSS
    
    Pulled By: zdevito
    
    Differential Revision: D18890532
    
    fbshipit-source-id: fe9447e59a6d5c9ad345f7c3ff15d63b6d2a09e2

commit a53b39f09d0c06f55ea6990ac72a3998b70423b5
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Dec 11 11:34:20 2019 -0800

    Disable flaky test_process_group_debug_info
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31113
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18932365
    
    Pulled By: mrshenli
    
    fbshipit-source-id: a2996b6a8d3881be4ffc174b85509aeee8c51c96

commit fa6661422fdaecd1cc25967c5f555d42ed66598c
Author: Shen Li <cs.shenli@gmail.com>
Date:   Mon Dec 9 15:53:44 2019 -0800

    Disable flaky test_rref_context_debug_info
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30990
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18893023
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 80b36927f243fa53c4d64f7e7c51097290ffdeee

commit 4fd20c0816d00e445d96085b9dc2fa7a443159ff
Author: James Reed <jamesreed@fb.com>
Date:   Fri Dec 6 13:33:20 2019 -0800

    Kill hypothesis deadline testing (#30890)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30890
    
    We've received way too many complaints about this functionality making tests flaky, and it's not providing value to us anyway. Let's cut the shit and kill deadline testing
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18857597
    
    Pulled By: jamesr66a
    
    fbshipit-source-id: 67e3412795ef2fb7b7ee896169651084e434d2f6

commit ef95a72690ff8d26e212a04b896b1214ee06d612
Author: Rohan Varma <rvarm1@fb.com>
Date:   Thu Dec 5 21:44:26 2019 -0800

    modify test_local_shutdown_with_rpc to not be flaky (#30837)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30837
    
    This test would get very occasional flakes, with an error saying the
    RPC timed out. This happened because one worker would still be waiting for the
    return value of an RPC, but another worker had already performed its local
    shutdown, so it would not have sent the response. This didn't show up in
    initial testing since the flakiness is very rare (< 1/100 test runs). This diff
    fixes the issue by not erroring if these RPCs timeout. The reason this is okay
    is because with a local shutdown, we should not expect for all outstanding RPCs
    to be completed, since workers are free to shut down without completing/waiting
    on outstanding work.
    ghstack-source-id: 95021672
    ghstack-source-id: 95021672
    
    Test Plan: Ran the test 1000 times to ensure that it is not flaky.
    
    Differential Revision: D18775731
    
    fbshipit-source-id: 21074e8b4b4bbab2be7b0a59e80cb31bb471ea46

commit c1159494a67e9e18de51cb3b89a8c3248cbc22dc
Author: Zachary DeVito <zdevito@fb.com>
Date:   Thu Dec 5 17:06:13 2019 -0800

    Revert D18621773: we should have a config-based way to skip flaky tests
    
    Test Plan: revert-hammer
    
    Differential Revision:
    D18621773
    
    Original commit changeset: 5532f1d5fa3f
    
    fbshipit-source-id: 22239b88a6f9551938e6e2178bf9162e3385b011

commit e5bd7a794270a35c162b4bf5b0b28334610eb30a
Author: Zachary DeVito <zdevito@fb.com>
Date:   Thu Dec 5 14:25:38 2019 -0800

    we should have a config-based way to skip flaky tests (#29944)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29944
    
    This particular approach queries our issue tracker for test titles that
    match the following format:
    
    ```
    DISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)
    ```
    
    And then skips the python test for them. There is 1 second timeout so
    if the internet flakes we still run the test suite, without disabling any
    tests.
    
    This is intended as a quick fix, similar to ninja unland, to get to a green
    master. Long term test disables should go into the code.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18621773
    
    Pulled By: zdevito
    
    fbshipit-source-id: 5532f1d5fa3f83f77fc3597126cbb7dba09a3c33

commit 2d0a4e42e9e55207aa8e72d64bc4d01d0ac7af77
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Mon Dec 2 18:10:29 2019 -0800

    Add barriers to fix flaky test_graph_for_py_nested_call and (#30624)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30624
    
    These tests were flaky since we would end up calling the 'verify'
    methods before some of the RPCs were done. The `check_rpc_done` function might
    not guarantee this since set_rpc_done sets an appropriate flag in python which
    causes `check_rpc_done` to pass. Although, there are a few steps after that
    like attaching the send functions for the response of the RPC that might not
    have executed by then.
    ghstack-source-id: 94781954
    
    Test Plan: Run the tests 100 times.
    
    Reviewed By: zhaojuanmao
    
    Differential Revision: D18768786
    
    fbshipit-source-id: a14c3f4b27de14fe5ecc6e90854dc52652f769b8

commit 188d0a9add7e10578dc1d6989bb3b22f59850eb9
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Thu Nov 21 13:42:35 2019 -0800

    Skips flaky UtilsNMSTest.GPUEqualsCPURotatedCorrectnessTest (#30053)
    
    Summary:
    See https://github.com/pytorch/pytorch/issues/26811.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/30053
    
    Differential Revision: D18597070
    
    Pulled By: mruberry
    
    fbshipit-source-id: a3ab8abda8e019fb9978ad8d41ef44451129868c

commit 551e387ffffbedaf8d4b70d1c2db7072a082daa6
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Nov 20 07:39:57 2019 -0800

    Disable flaky test test_graph_for_py_nested_remote_call
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/30132
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18609560
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 00fbfc8753e002808f49cf9f09ce0c0966a74485

commit 63c957cd94e4870fcef3a735dec752ddcfd4c4d8
Author: Pritam Damania <pritam.damania@fb.com>
Date:   Tue Nov 19 15:49:20 2019 -0800

    Use std::shared_ptr for DistAutogradContext. (#29770)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29770
    
    We were passing around const and non-const references for
    DistAutogradContext from DistAutogradContainer. This wasn't safe since the
    context could be deleted from the container and a thread might still be using
    the reference. This usually would happen when a backward pass fails on the node
    driving the backward pass (resulting in delete context messages being sent to
    all nodes) but other nodes are still executing code related to that autograd
    context.
    
    This was also the reason why `test_backward_autograd_engine_error` was flaky.
    
    Using a std::shared_ptr everywhere ensures we're safe and never crash.
    
    Closes #28928
    Closes #26922
    ghstack-source-id: 94201446
    
    Differential Revision: D18494814
    
    fbshipit-source-id: 0c925fdbd5755f6d876dad56885e2cbaf41fc5f0

commit 57acc2ff3a45ad3a920f335afba01b6157b85897
Author: Tao Xu <taox@fb.com>
Date:   Tue Nov 19 11:01:42 2019 -0800

    add an unit test target to TestApp (#29962)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29962
    
    ### Summary
    
    Recently we've found that the master branch was constantly broken due to some unwanted change being landed on mobile. The problem is that our CI was not able to detect the runtime errors. Starting from this PR, we'll add some unit tests to the iOS Simulator build. As follows:
    
    1. Add an unit test target to XCode (this PR)
    2. Use Fastlane to run the tests on CI
    3. Modify the CI scripts to trigger tests
    
    ### Test Plan
    
    - Don't break the existing CI jobs unless they are flaky.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18582908
    
    Pulled By: xta0
    
    fbshipit-source-id: f960c47d3bbda79e754a0513e8711867fd3588d2

commit 861ef050150f4dc8565b90fea253a70a76ba4dc4
Author: Yanli Zhao <yanlizhao@fb.com>
Date:   Tue Nov 19 07:00:11 2019 -0800

    Remove rpc fork and dist autograd fork tests from PyTorch repo (#29827)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29827
    
    There are known issues for "fork tests + OMP" in Pytorch, rpc and dist autograd tests use OMP thread pools, this caused rpc fork and dist autograd fork tests to be flaky. So remove these fork tests from PyTorch repo. rpc spawn and dist autograd spawn tests are still running.
    
    Test Plan: unit tests
    
    Differential Revision: D18507384
    
    fbshipit-source-id: 9e239f13850832b4b84724828537f73512f3fca9

commit 4da509090e6619d96f084e0b2123be0e670f5512
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Fri Nov 15 21:57:15 2019 -0800

    Disables TestNN.test_CTCLoss_1d_target (#29841)
    
    Summary:
    A variant of this test is flaky in CI. See https://github.com/pytorch/pytorch/issues/29380.
    
    This disables the entire test until a fix is determined.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29841
    
    Differential Revision: D18531542
    
    Pulled By: mruberry
    
    fbshipit-source-id: 3b033e3a7d55418cf459e7664d856d6dd4c98aa5

commit 9fd7db616a0e220a74f4459958ea2d3186d5e55a
Author: Junjie Bai <baidingding7@gmail.com>
Date:   Wed Nov 13 23:54:54 2019 -0800

    Disable Caffe2 RCCL tests (#29792)
    
    Summary:
    They are flaky on master
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29792
    
    Differential Revision: D18500737
    
    Pulled By: bddppq
    
    fbshipit-source-id: 18a39b2d6117a7c3b48e1d6a635f24acb35fc497

commit 2b05ae0704cc25bf8d801e9a2242b4de09518505
Author: Junjie Bai <baidingding7@gmail.com>
Date:   Wed Nov 13 13:50:01 2019 -0800

    Revert "Enable test_distributed for ROCm but only with nccl backend" (#29736)
    
    Summary:
    This reverts commit 7073ee209000a7781c0c863c4ef39bb3bfdb4932.
    
    They are flaky on master:
    
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/6830//console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/6824//console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py3.6-clang7-rocmdeb-ubuntu16.04-test2/6802//console
    
    cc jithunnair-amd
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29736
    
    Differential Revision: D18480543
    
    Pulled By: bddppq
    
    fbshipit-source-id: 9a1dd9aa5f5959dc6fbbfdab0df997514221217a

commit 3e5af22650c3dc05dd324fdd87c4595c4f08b605
Author: Shen Li <cs.shenli@gmail.com>
Date:   Sun Nov 10 21:31:52 2019 -0800

    Disable flaky RPC tests (#29485)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29485
    
    The flakiness is likely due to the problem with OMP and fork. We
    should disable fork tests for good, but that would have negative
    impact on internal test coverage. This commit disables the most
    buggy nested tests for now, until we find a way to turn fork test
    off.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18407529
    
    Pulled By: mrshenli
    
    fbshipit-source-id: dcbe49a9d104fcf1eaf83107d58904d49dc18aff

commit f5074ccafe24d656d2b3a3f29442c03fcadee21e
Author: Lingyi Liu <lingyiliu@fb.com>
Date:   Sat Nov 9 09:29:06 2019 -0800

    set the no_deadline for the adaptive_avg_pool_nhwc test (#29502)
    
    Summary:
    It is reported this test is flaky due to the time expiration. This pr flags it as no_deadline test.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29502
    
    Differential Revision: D18416632
    
    Pulled By: lly-zero-one
    
    fbshipit-source-id: 27cd7b28139f3f16ee0cf5802a0709385719d487

commit 991c2ac383801ecd3b0bf40c0c8595dd216bb988
Author: Mike Ruberry <38511765+mruberry@users.noreply.github.com>
Date:   Fri Nov 8 13:46:14 2019 -0800

    Disables flaky test_rand_quantization (#29463)
    
    Summary:
    See https://github.com/pytorch/pytorch/issues/28550.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29463
    
    Differential Revision: D18405669
    
    Pulled By: mruberry
    
    fbshipit-source-id: 2984c3896a9260a06fbf052afb06e0cb8d28b53d

commit 1dd3c8e53909d6cf35ade5cf85cd7430e5c655f9
Author: James Reed <jamesreed@fb.com>
Date:   Thu Nov 7 12:49:09 2019 -0800

    Skip flaky test
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/29403
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18377162
    
    Pulled By: jamesr66a
    
    fbshipit-source-id: 69052a7466d03468146e99da45f1ee2c9e85dfa8

commit 4515edfe1596db609b1a4c4601026ce2befc089b
Author: Supriya Rao <supriyar@fb.com>
Date:   Wed Nov 6 21:28:40 2019 -0800

    Disable QNNPACK tests on MacOS (#29328)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29328
    
    Tests are flaky as seen in issue #29326.
    Disable until we fix the kernels.
    
    Test Plan:
    python test/test_quantized.py TestQNNPackOps
    
    Imported from OSS
    
    Differential Revision: D18358200
    
    fbshipit-source-id: 58f1981799fe8253234fcc7b0540e1c0b6babc15

commit 74b2d9ed2ed9b1ea3b33d153854ec0349152ab0a
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Wed Nov 6 13:27:55 2019 -0800

    Skips test_equiv_recurrent (#29255)
    
    Summary:
    This test is flaky, per issue https://github.com/pytorch/pytorch/issues/10322.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29255
    
    Differential Revision: D18350782
    
    Pulled By: mruberry
    
    fbshipit-source-id: 53a7d33e17428c2484211618cb71e870ce2d6a03

commit 2f2a0d1607113003eeb1bbdcba343a745aa1e6d9
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Tue Nov 5 16:51:30 2019 -0800

    Disables test_atomic_ops and testInputOrder (#29145)
    
    Summary:
    These tests have been flaky for some time, see:
    
    - https://github.com/pytorch/pytorch/issues/28179
    - https://github.com/pytorch/pytorch/issues/9064
    
    This PR disables them. The actual tests were added/updated 2+ years ago. It's unclear who, if anyone, would own them now.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29145
    
    Differential Revision: D18327937
    
    Pulled By: mruberry
    
    fbshipit-source-id: d02731d662aff3545b581272e5ae8db4e3097d87

commit 003cb8595b092ed18c9a120fc7f087cd540b811e
Author: Rohan Varma <rvarm1@fb.com>
Date:   Tue Nov 5 15:45:17 2019 -0800

    skip more flaky rpc tests (#29157)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29157
    
    As reported, these tests are flaky and time out. Skip them
    while we investigate further.
    ghstack-source-id: 93287663
    
    Test Plan: CI
    
    Differential Revision: D18309204
    
    fbshipit-source-id: 95f0ea5e0c1162b78da412a34db446a01dfc33bf

commit 3a4e622165c753d2421990e20a02da0fc8337965
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Mon Nov 4 11:57:45 2019 -0800

    disables flaky tests

commit cddda17394d4d763d47059678d2ce2d8c9c677b0
Author: Kevin Wilfong <kevinwilfong@fb.com>
Date:   Fri Nov 1 13:57:25 2019 -0700

    ParallelWorkersTest.testParallelWorkersInitFun is flaky (#29045)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/29045
    
    Addressing an issue seen in GitHub https://github.com/pytorch/pytorch/issues/28958
    
    It seems sometimes the workers in this test don't stop cleanly.  The purpose of this test is to check that the init_fun in init_workers works as expected, which is captured by the assertEqual in the for loop in the test.  The behavior of stop() is not really important here.
    
    The fact it's returning false is probably indicative that a worker is getting blocked but that doesn't affect the correctness of the test.
    
    Test Plan: Ran the test 100 times, it consistently succeeds.
    
    Reviewed By: akyrola
    
    Differential Revision: D18273064
    
    fbshipit-source-id: 5fdff8cf80ec7ba04acf4666a3116e081d96ffec

commit 05e88dc4fecd77ea87685895480aafdd317d4d0e
Author: Rohan Varma <rvarm1@fb.com>
Date:   Thu Oct 31 10:11:18 2019 -0700

    skip additional flaky rpc tests (#28934)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/28934
    
    These tests are flaky, skip them as we investigate for a root cause
    ghstack-source-id: 92945898
    
    Test Plan: tests pass
    
    Differential Revision: D18235766
    
    fbshipit-source-id: 9bff65653954b767e32bcc1d25c65b0cea2c4331

commit 64c7ac233e4045aae76bf06bf445392d0d02957c
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Oct 30 18:41:23 2019 -0700

    Disable flaky remote tests in dist_autograd_test
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28920
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18233625
    
    Pulled By: mrshenli
    
    fbshipit-source-id: d4b04ea3629d0828756ebb118f5763677d62729b

commit 06f03cdf5a0cd5bb97999195d3468d70cea372ad
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Oct 30 14:27:44 2019 -0700

    Disable flaky remote tests in dist_autograd_test
    
    [ghstack-poisoned]

commit 807fbf88168b229951a2c4a1c4c09a331df739f1
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Oct 30 11:32:52 2019 -0700

    Disable flaky tests in dist_autograd_test
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28876
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18224445
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 4de2c24ac6e9ffb004457e2dc43730dc7e478e5a

commit e96ea288a8706b9e6dfad89aa47d73198960631c
Author: Tao Xu <taox@fb.com>
Date:   Fri Oct 25 19:49:00 2019 -0700

    Automation scripts for perf testing (#28622)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/28622
    
    ### Summary
    
    As discussed in #28405 , this is the third PR.  The`bootstrap.sh` script is mainly for those who want to do perf on iOS but don't want to touch XCode or any iOS code.  But it does require you have valid iOS dev credentials installed on your machine. (You can easily acquire those stuff from any experienced iOS developers. Takes only 5 mins to setup )
    
     All you need to do is run
    
    ```shell
    ./bootstrap -t ${TEAM_ID} -p ${PROFILE}
    ```
    
    The testing app will be automatically installed on your device. The log of the benchmark function will be displayed on the screen.
    
    ### Test plan
    
    Don't break any CI jobs unless they're flaky.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18156178
    
    Pulled By: xta0
    
    fbshipit-source-id: cd7ba8d87bf26db885262888b9d6a5fd072309d1

commit 896b5d9113523d8b5a8f5beac8c62f8f2fa7a326
Author: Tao Xu <taox@fb.com>
Date:   Wed Oct 23 16:15:07 2019 -0700

    Scripts for setting up benchmark projects (#28469)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/28469
    
    ### Summary
    
    As described [here](https://github.com/pytorch/pytorch/pull/28405), This PR is the second one that contains scripts for setting up the benchmark projects.
    
    ### Test Plan
    
    Don't break CI jobs unless they are flaky.
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18097248
    
    Pulled By: xta0
    
    fbshipit-source-id: 6f9d1275a07aecae21afd81d5e90a89a75d0270f

commit 636fbcdd0a287a590b401fb511da74aa69354b23
Author: Tao Xu <taox@fb.com>
Date:   Tue Oct 22 12:48:44 2019 -0700

    add benchmark code to iOS TestApp (#28405)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/28405
    
    ### Summary
    
    As discussed with AshkanAliabadi  and ljk53, the iOS TestApp will share the same benchmark code with Android's speed_benchmark_torch.cpp. This PR is the first part which contains the Objective-C++ code.
    
    The second PR will include the scripts to setup and run the benchmark project. The third PR will include scripts that can automate the whole "build - test - install" process.
    
    There are many ways to run the benchmark project. The easiest way is to use cocoapods. Simply run `pod install`. However, that will pull the 1.3 binary which is not what we want, but we can still use this approach to test the benchmark code. The second PR will contain scripts to run custom builds that we can tweak.
    
    ### Test Plan
    - Don't break any existing CI jobs  (except for those flaky ones)
    
    Test Plan: Imported from OSS
    
    Differential Revision: D18064187
    
    Pulled By: xta0
    
    fbshipit-source-id: 4cfbb83c045803d8b24bf6d2c110a55871d22962

commit 415b17e81cb32d9d8dddcd8ae495bf63f919cb1e
Author: Kutta Srinivasan <kutta@fb.com>
Date:   Thu Oct 10 13:52:24 2019 -0700

    Fix for flaky caffe2 dataio test (test_time_limit_reader_with_short_limit) (#27592)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/27592
    
    The caffe2 data reader test `test_time_limit_reader_with_short_limit` is flaky as-written because it places an upper bound on how much can be read, but under stress it is possible for fewer records to be read. The fix is to make the assertion check a fuzzy/range check rather than exact equality, since there's not a straightforward way to precisely test a timer-based feature.
    ghstack-source-id: 91543898
    
    Test Plan:
    `buck test mode/dev-tsan //caffe2/caffe2/python:dataio_test-2.7 -- --stress-runs 20` -> P117156924 (with fix, 100% pass)
    
    P117158750 - without fix, lots of failures in this test
    
    Reviewed By: boryiingsu
    
    Differential Revision: D17816775
    
    fbshipit-source-id: 2ab0d3304fbd9c9806d37a4fe2912c840616db61

commit e09c97b113b0d1e2826f990dffc8e058b1294fc9
Author: Shen Li <cs.shenli@gmail.com>
Date:   Tue Oct 1 13:15:25 2019 -0700

    Disable flaky distributed autograd test under spawn mode
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/27158
    
    Test Plan: Imported from OSS
    
    Differential Revision: D17693184
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 920ff1b45d1010cd8f2045269aa48af5cb8d5a6a

commit 70a5fb732bc803a90d92fa8569aea278dd45672f
Author: yanlizhao <yanlizhao@fb.com>
Date:   Tue Sep 24 22:26:52 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit 05b93592007880c7865065c260d189013486173c
Merge: 76bdae0 d4dc844
Author: yanlizhao <yanlizhao@fb.com>
Date:   Tue Sep 24 22:10:38 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit 76bdae016dc8b7e641e472fbd6eccf7eba365aa8
Merge: bb2f1c5 3f72bcf
Author: yanlizhao <yanlizhao@fb.com>
Date:   Tue Sep 24 15:39:16 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit bb2f1c54a870f4071db9e5756e009ed6059dc498
Author: yanlizhao <yanlizhao@fb.com>
Date:   Tue Sep 24 11:57:52 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit e5e0084aeed7363515453db128c7dd5708eaf460
Author: yanlizhao <yanlizhao@fb.com>
Date:   Tue Sep 24 09:20:36 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit ef2f2abcd56c4c66e03c663b2940062dd8f3fc86
Author: yanlizhao <yanlizhao@fb.com>
Date:   Mon Sep 23 17:09:40 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit cbdbdd3c8c428fb3e2f4edd48ebb1e21b52dc1e0
Author: Jianyu Huang <jianyuhuang@fb.com>
Date:   Mon Sep 23 14:18:14 2019 -0700

    Fix the flaky test_qlinear test caused by hypothesis deadline (#26663)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/26663
    
    As Title says.
    
    Example error:
    https://circleci.com/gh/pytorch/pytorch/2894108?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link%2Fconsole
    
    ```
    Sep 23 19:08:00 Unreliable test timings! On an initial run, this test took 453.13ms, which exceeded the deadline of 200.00ms, but on a subsequent run it took 23.01 ms, which did not. If you expect this sort of variability in your test timings, consider turning deadlines off for this test by setting deadline=None.
    ```
    ghstack-source-id: 90613535
    
    Test Plan: CI
    
    Differential Revision: D17534476
    
    fbshipit-source-id: d3ab91c8b290a0433eab4af3fc73ecbf728ec5bf

commit 107bf209891e0690123ff8e8e7e5aefc0a7615c1
Author: yanlizhao <yanlizhao@fb.com>
Date:   Mon Sep 23 12:22:02 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit cef2ef90aec6bbb10d5c1a8f3c286e075696eac0
Author: yanlizhao <yanlizhao@fb.com>
Date:   Mon Sep 23 12:00:16 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit c8008e7e785a83ac5838430b47054ce97409988d
Merge: aafc548 15b5060
Author: yanlizhao <yanlizhao@fb.com>
Date:   Mon Sep 23 10:14:57 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit aafc54859ef234f38153576e4539648f3598bc48
Author: yanlizhao <yanlizhao@fb.com>
Date:   Fri Sep 20 13:57:06 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit 75eb1796bcbd5dc189abf91b86084213743d6003
Merge: 351d687 74710f9
Author: yanlizhao <yanlizhao@fb.com>
Date:   Fri Sep 20 12:35:56 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but spawn caught it.  so add spawn tests in common_distributed.py. The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is still using fork right now as there are a few flaky tests need to be addressed separately, will make c10d test to use spawn in a separated diff
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit 351d687630a856d79171d0f0854570f7dc6628e0
Merge: 8b25d80 4c1a2c2
Author: yanlizhao <yanlizhao@fb.com>
Date:   Thu Sep 19 17:00:06 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but torch multiprocessing fork and spawn caught it.  so add torch spawn in a new file multiprocessing_test_case.py imported by both test_rpc and test_dist_autograd.  The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is not touched right now as there are a few flaky tests need to be addressed separately, will make it use the new multiprocessing_test_case.py in a separated diff
    
    3. fix the real segment exit issue caused by python rpc handler temporarily by removing module_ global variable, separate diff D17097999 is to change python rpc handler to be singletion class for deterministic destruction order of variables
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)
    
    [ghstack-poisoned]

commit 68895eb9f4788656f03cddc0b631f9ff854cd368
Author: Alexander Melnikov <sanekmelnikov@fb.com>
Date:   Thu Sep 19 11:11:09 2019 -0700

    fix flaky test (#26395)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/26395
    
    This diff makes each SummaryWriter write into its own unique directory.
    
    Reviewed By: orionr
    
    Differential Revision: D17441500
    
    fbshipit-source-id: d284fcf0e7e7a7214e644349e345f1de0e1a1aba

commit 4160b8cd77c699d80b201a876d7a5de66b672b07
Author: Mike Ruberry <mruberry@devfair044.maas>
Date:   Sat Sep 14 00:33:13 2019 -0700

    adds sync to flaky test_events_multi_gpu_query (#26231)
    
    Summary:
    This test can sometimes fail in CI.
    
    I suspect this flakiness is because the test asks a CUDA stream to record an event, fails to synchronize the CPU with that stream, then checks if the event is recorded on the CPU. There is no guarantee this will have happened.
    
    This one-line change preserves the intent of the test while ensuring the GPU has recorded the event before the CPU queries it.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/26231
    
    Differential Revision: D17382110
    
    Pulled By: mruberry
    
    fbshipit-source-id: 35b701f87f41c24b208aafde48bf10e1a54de059

commit 827d71d769f9b8249d368616993dbe10b7ebd6f3
Author: Junjie Bai <bai@in.tum.de>
Date:   Thu Sep 12 10:04:42 2019 -0700

    Disable test_cuda.test_stream_event_nogil on ROCm (#26087)
    
    Summary:
    Was recently enabled in https://github.com/pytorch/pytorch/pull/26055, it's flaky on master:
    
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/37575
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/37577
    ```
    05:39:35 test_stream_event_nogil (__main__.TestCuda) ... Exception in thread Thread-3:
    05:39:40 Traceback (most recent call last):
    05:39:40   File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
    05:39:40     self.run()
    05:39:40   File "/usr/lib/python2.7/threading.py", line 754, in run
    05:39:40     self.__target(*self.__args, **self.__kwargs)
    05:39:40   File "test_cuda.py", line 1894, in _test_stream_event_nogil
    05:39:40     c2p.put(sync_func(self, TestCuda.FIFTY_MIL_CYCLES))
    05:39:40   File "test_cuda.py", line 1882, in _event_wait
    05:39:40     self.assertTrue(s1.query())
    05:39:40   File "/usr/lib/python2.7/unittest/case.py", line 422, in assertTrue
    05:39:40     raise self.failureException(msg)
    05:39:40 AssertionError: False is not true
    ```
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/26087
    
    Differential Revision: D17340891
    
    Pulled By: bddppq
    
    fbshipit-source-id: b2b70beb1b068db53197a5f9f6a80cb046e66ebd

commit 2655b2710c8f0b3253fe2cfe0d2674f5d3592d22
Author: Shen Li <cs.shenli@gmail.com>
Date:   Tue Sep 10 16:56:34 2019 -0700

    Disable flaky test_invalid_names in test_rpc.py (#25916)
    
    Summary:
    pietern discovered that `test_invalid_names` is flaky on master. https://github.com/pytorch/pytorch/issues/25656 is potentially the fix. Disable this test for now and will try to add it again when https://github.com/pytorch/pytorch/issues/25656 is in.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/25916
    
    Differential Revision: D17287496
    
    Pulled By: mrshenli
    
    fbshipit-source-id: 9313958d3480c2bab20cd2341837c7821e3bb1b5

commit 8b25d801728b55e9a5d359f3d812a953c2f377e6
Author: yanlizhao <yanlizhao@fb.com>
Date:   Fri Sep 6 16:27:10 2019 -0700

    Update on "make rpc multiprocess test to use spawn instead of fork"
    
    
    1. current native fork unit tests did not catch rpc exit crash issue, but torch multiprocessing fork and spawn caught it.  so add torch spawn in a new file multiprocessing_test_case.py imported by both test_rpc and test_dist_autograd.  The run time is 28s vs 8s for test_rpc, we sacrifice performance for correctness here
    
    2. c10d test suite is not touched right now as there are a few flaky tests need to be addressed separately, will make it use the new multiprocessing_test_case.py in a separated diff
    
    3. fix the real segment exit issue caused by python rpc handler temporarily by removing module_ global variable, separate diff D17097999 is to change python rpc handler to be singletion class for deterministic destruction order of variables
    
    Differential Revision: [D17086007](https://our.internmc.facebook.com/intern/diff/D17086007/)

commit 3c6009e6f1e546a981d77644c5eec8e8de119f0c
Author: Jerry Zhang <jerryzh@fb.com>
Date:   Fri Sep 6 10:50:11 2019 -0700

    derandomize hypothesis tests (#25513)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/25513
    
    Randomized tests are flaky, this PR derandomized some of them
    
    Test Plan:
    python test/test_fake_quant.py
    python test/test_quantized_nn_mods.py
    
    Imported from OSS
    
    Differential Revision: D17221273
    
    fbshipit-source-id: f6978704ba0139071c26f443e923955a2f849832

commit 04764d575147fa25577afc6030c41413198d2ee7
Author: Shen Li <cs.shenli@gmail.com>
Date:   Thu Aug 29 14:44:16 2019 -0700

    Fix allreduce_coalesced tests in c10d (#25419)
    
    Summary:
    1. `test_allreduce_coalesced_stress` is flaky.
    2. `test_allreduce_coalesced_checks` uses GPU but didn't claim so.
    
    cc jfc4050
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/25419
    
    Differential Revision: D17119311
    
    Pulled By: mrshenli
    
    fbshipit-source-id: f560b126d6bc01363a14bdf6d697ecd55c4db468

commit 6c9410ffd13e68cfd00942c9e8a4f888408cb3de
Author: SsnL <tongzhou.wang.1994@gmail.com>
Date:   Wed Aug 28 07:47:40 2019 -0700

    Fix infer np scalar dtype mem leak (#24267)
    
    Summary:
    Fixes https://github.com/pytorch/pytorch/issues/24200 . I'm a bit worried that the test might be flaky...
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/24267
    
    Differential Revision: D17079762
    
    Pulled By: gchanan
    
    fbshipit-source-id: a120688b9583ca4b74bdfb295914298f22540ffd

commit 44a7879b6efbea28e5a9d4ba841c5763cd4674cb
Author: Edward Yang <ezyang@fb.com>
Date:   Tue Aug 27 09:07:59 2019 -0700

    Disable flaky test_adaptive_avg_pool2d test. (#25249)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/25249
    
    See #25097
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    Test Plan: Imported from OSS
    
    Differential Revision: D17071632
    
    Pulled By: ezyang
    
    fbshipit-source-id: 1c5ad7204f1d30f5c67d682fbb083608e067cb2a

commit 67179d71f7be0a001485bdd774bb864e6b545c2c
Author: Pieter Noordhuis <pietern@fb.com>
Date:   Wed Jul 24 11:17:31 2019 -0700

    Reduce number of processes spawned for gloo_test.TestCase.test_forked_cw (#23221)
    
    Summary:
    It used to be run with comm_size=8, which causes flaky results in a
    stress run. The flakiness was caused by too many listening sockets
    being created by Gloo context initialization (8 processes times 7
    sockets times 20-way concurrency, plus TIME_WAIT).
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/23221
    ghstack-source-id: 86995596
    
    Reviewed By: d4l3k
    
    Differential Revision: D16437834
    
    fbshipit-source-id: 998d0e2b087c0ab15eca64e308059c35e1b51e7b

commit 25eae3ed08d82c1fdca7f85559cc35d8aa8b77ef
Author: Tongzhou Wang <tongzhou.wang.1994@gmail.com>
Date:   Wed Jun 26 09:36:52 2019 -0700

    Disable test_proper_exit flaky worker_kill (#22208)
    
    Summary:
    I learned from https://github.com/pytorch/pytorch/pull/22058 that `worker_kill` is just flaky, regardless of `hold_iter_reference`. So let's disable it altogether for now.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/22208
    
    Differential Revision: D15990307
    
    Pulled By: soumith
    
    fbshipit-source-id: d7d3f4fe7eaac4987f240cb8fd032c73a84157d7

commit 710821875af460d0f10c1f8c0c03d964b35ec5da
Author: Stefan Krah <skrah@bytereef.org>
Date:   Fri Jun 14 11:32:33 2019 -0700

    Fix flaky nuclear_norm() test (#21638)
    
    Summary:
    Try to fix a sporadic failure on some CIs.
    
    I've run this test hundreds of times on my machine (GeForce 1060, MAGMA) but I cannot reproduce this.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/21638
    
    Differential Revision: D15827779
    
    Pulled By: ezyang
    
    fbshipit-source-id: 3586075e48907b3b84a101c560a34cc733514a02

commit bc6281028ce02d7a92093e5331ac11047347a162
Author: Tongzhou Wang <ssnl@users.noreply.github.com>
Date:   Fri Jun 14 09:06:49 2019 -0700

    rebuild_storage_fd retry on EINTR (#21723)
    
    Summary:
    Some data loader tests are flaky on py 2 with the following error
    ```
    Jun 12 22:17:31 Traceback (most recent call last):
    Jun 12 22:17:31   File "test_dataloader.py", line 798, in test_iterable_dataset
    Jun 12 22:17:31     fetched = sorted([d.item() for d in dataloader_iter])
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 697, in __next__
    Jun 12 22:17:31     idx, data = self._get_data()
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 664, in _get_data
    Jun 12 22:17:31     success, data = self._try_get_data()
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 617, in _try_get_data
    Jun 12 22:17:31     data = self.data_queue.get(timeout=timeout)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/queues.py", line 135, in get
    Jun 12 22:17:31     res = self._recv()
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/queue.py", line 22, in recv
    Jun 12 22:17:31     return pickle.loads(buf)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 1382, in loads
    Jun 12 22:17:31     return Unpickler(file).load()
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 858, in load
    Jun 12 22:17:31     dispatch[key](self)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 1133, in load_reduce
    Jun 12 22:17:31     value = func(*args)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/reductions.py", line 274, in rebuild_storage_fd
    Jun 12 22:17:31     fd = multiprocessing.reduction.rebuild_handle(df)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py", line 157, in rebuild_handle
    Jun 12 22:17:31     new_handle = recv_handle(conn)
    Jun 12 22:17:31   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py", line 83, in recv_handle
    Jun 12 22:17:31     return _multiprocessing.recvfd(conn.fileno())
    Jun 12 22:17:31 OSError: [Errno 4] Interrupted system call
    ```
    
    Apparently, Python 2.7's `recvfd` calls `recvmsg` without EINTR retry: https://github.com/python/cpython/blob/2.7/Modules/_multiprocessing/multiprocessing.c#L174
    So we should call it with an outer try-catch loop.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/21723
    
    Differential Revision: D15806247
    
    Pulled By: ezyang
    
    fbshipit-source-id: 16cb661cc0fb418fd37353a1fef7ceeb634f02b7

commit e268fc97c3c931cfc303d044969699d50d8383e0
Author: Brennan Vincent <btv@fb.com>
Date:   Tue Jun 4 17:35:10 2019 -0700

    Re-add Tensor.T (#21175)
    
    Summary:
    Something flaky is going on with `test_inplace_view_saved_output` on Windows.
    
    With my PR #20598 applied, the test fails, even though there is no obvious reason it should be related, so the PR was reverted.
    
    Based on commenting out various parts of my change and re-building, I think the problem is with the name -- renaming everything from `T` to `asdf` seems to make the test stop failing. I can't be sure that this is actually the case though, since I could just be seeing patterns in non-deterministic build output...
    
    I spoke with colesbury offline and we agreed that it is okay to just disable this test on Windows for now and not block landing the main change. He will look into why it is failing.
    
    **Test Plan:** I will wait to make sure the Windows CI suite passes before landing this.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/21175
    
    Differential Revision: D15566970
    
    Pulled By: umanwizard
    
    fbshipit-source-id: edf223375d41faaab0a3a14dca50841f08030da3

commit d23d04f17fc210424d4ecb30c82f9a08f1d5ca48
Author: Thomas Viehmann <tv.code@beamnet.de>
Date:   Tue May 28 21:17:40 2019 -0700

    Allow nondet_tol for nondeterminism in gradcheck and gradgradcheck (#20980)
    
    Summary:
    gradcheck currently includes a determinism check (although only trying twice and seeing if results match).
    This can lead to flaky tests, e.g. in #20971, but also #13818.
    This adds nondet_tol for both gradcheck and gradgradcheck. It does not change / reenable any tests yet.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/20980
    
    Differential Revision: D15530129
    
    Pulled By: soumith
    
    fbshipit-source-id: 04d7f85b5b59cd62867820c74b064ba14f4fa7f8

commit 430d1a2761ab85ce8e8a3b6f81f7106481a4973a
Author: Sam Gross <colesbury@gmail.com>
Date:   Fri May 24 15:46:33 2019 -0700

    Attempt to fix flaky test_structseq_repr (#20931)
    
    Summary:
    Previously, this used `crepr` afer the decref of `repr`. This is not
    allowed because `repr` owns the cached copy of `crepr`.
    
    Let's see if this fixes the contbuild.
    
    See #20926
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/20931
    
    Differential Revision: D15501929
    
    Pulled By: colesbury
    
    fbshipit-source-id: 24141ba62df8758d2a3998cf7c2054be09088b6a

commit 9005a2c0fc2d981c65b388ccd2ec4be3e1106a66
Author: Brian Vaughan <bvaughan@fb.com>
Date:   Mon May 6 08:30:19 2019 -0700

    disable flaky test_proper_exit again, still occasionally failing (#20063)
    
    Summary:
    test was disabled for being flaky, re-enabled in https://github.com/pytorch/pytorch/pull/19421 but still occasionally failing:
    
    https://circleci.com/gh/pytorch/pytorch/1520165?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link
    
    ```
    Apr 29 19:51:58 ======================================================================
    Apr 29 19:51:58 FAIL: test_proper_exit (__main__.TestDataLoader)
    Apr 29 19:51:58 There might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore
    Apr 29 19:51:58 ----------------------------------------------------------------------
    Apr 29 19:51:58 Traceback (most recent call last):
    Apr 29 19:51:58   File "/var/lib/jenkins/workspace/test/common_utils.py", line 129, in wrapper
    Apr 29 19:51:58     fn(*args, **kwargs)
    Apr 29 19:51:58   File "test_dataloader.py", line 847, in test_proper_exit
    Apr 29 19:51:58     self.fail(fail_msg + ', and had exception {}'.format(loader_p.exception))
    Apr 29 19:51:58 AssertionError: test_proper_exit with use_workers=True, pin_memory=False, hold_iter_reference=False, exit_method=worker_kill: loader process did not terminate, and had exception Traceback (most recent call last):
    Apr 29 19:51:58   File "test_dataloader.py", line 227, in run
    Apr 29 19:51:58     super(ErrorTrackingProcess, self).run()
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py", line 114, in run
    Apr 29 19:51:58     self._target(*self._args, **self._kwargs)
    Apr 29 19:51:58   File "test_dataloader.py", line 424, in _test_proper_exit
    Apr 29 19:51:58     for i, _ in enumerate(it):
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 545, in __next__
    Apr 29 19:51:58     idx, batch = self._get_batch()
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 522, in _get_batch
    Apr 29 19:51:58     success, data = self._try_get_batch()
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py", line 480, in _try_get_batch
    Apr 29 19:51:58     data = self.data_queue.get(timeout=timeout)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/queues.py", line 135, in get
    Apr 29 19:51:58     res = self._recv()
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/queue.py", line 22, in recv
    Apr 29 19:51:58     return pickle.loads(buf)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 1382, in loads
    Apr 29 19:51:58     return Unpickler(file).load()
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 858, in load
    Apr 29 19:51:58     dispatch[key](self)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/pickle.py", line 1133, in load_reduce
    Apr 29 19:51:58     value = func(*args)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/reductions.py", line 274, in rebuild_storage_fd
    Apr 29 19:51:58     fd = multiprocessing.reduction.rebuild_handle(df)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py", line 155, in rebuild_handle
    Apr 29 19:51:58     conn = Client(address, authkey=current_process().authkey)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/connection.py", line 169, in Client
    Apr 29 19:51:58     c = SocketClient(address)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/multiprocessing/connection.py", line 304, in SocketClient
    Apr 29 19:51:58     s.connect(address)
    Apr 29 19:51:58   File "/opt/python/2.7.9/lib/python2.7/socket.py", line 224, in meth
    Apr 29 19:51:58     return getattr(self._sock,name)(*args)
    Apr 29 19:51:58 error: [Errno 111] Connection refused
    
    ```
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/20063
    
    Differential Revision: D15218223
    
    Pulled By: nairbv
    
    fbshipit-source-id: 32018c4220f7cb9372ef138631fc3a79759265e1

commit 6b0ca8eae5d663ad3db560b428abcef465f09dbb
Author: Shen Li <cs.shenli@gmail.com>
Date:   Wed Apr 10 20:30:46 2019 -0700

    Fix flaky store timeout test (#19114)
    
    Summary:
    ~Sometimes, `init_process_group()`, `store.get()`, and `destory_process_group()` can take more than a few seconds. Hence, removing thread join timeout.~
    
    The error was due to `Address already in use` when starting TPC backend. The solution is to catch the error and report it to the `retry_on_address_already_in_use_error` decorator.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/19114
    
    Reviewed By: ezyang
    
    Differential Revision: D14872680
    
    Pulled By: mrshenli
    
    fbshipit-source-id: fc504d02853ca73f76288c0ade564ab20bc01f7e

commit 8793e8db429ffbc18369ae62f970ee528c10829c
Author: Edward Yang <ezyang@fb.com>
Date:   Fri Apr 5 09:37:11 2019 -0700

    Disable flaky test_proper_exit test. (#18950)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/18950
    ghimport-source-id: 27bd575fd3c73a51ace1360aa020fa63a792a5d2
    
    Differential Revision: D14802009
    
    Pulled By: ezyang
    
    fbshipit-source-id: 051e1d038892c2c6e8337357fa80771b8dc42680

commit c3e3c5cc39165470ddab5afb6373399fdbd6598e
Author: Min Ni <minn@fb.com>
Date:   Wed Mar 27 11:14:32 2019 -0700

    Skip tests if C2/ONNX models cannot be read (#18494)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/18494
    
    Today we have some C2 end2end test run requiring reading model data from external filesystem (for example, Gluster and AWS). This could be a source for flaky test when the external filesystems are not reachable during the tests.
    
    In this diff, we add try/catch logic around where we download models and open model files from external system. In case such attempts fails, we will catch the excption and let the unittest skip the current test instead of failure.
    
    I also refactor the code a little bit by removing some duplicated logic on downloading and build the c2 model data. It has been duplicated in two classes and a few functions...
    
    Reviewed By: yinghai
    
    Differential Revision: D14442241
    
    fbshipit-source-id: da8bf56c8d096efa34ca2070de5cd10a18aad70c

commit 6a1a019c0ad63182ea10afa2e9fc52d38ec5eebc
Author: Duc Ngo <duc@fb.com>
Date:   Mon Mar 25 16:55:30 2019 -0700

    caffe2 - support flaky operator tests for caffe2 build (#18155)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/18155
    
    - Make a python decorator caffe2_flaky for caffe2 operator unit tests.
    - The environment variable CAFFE2_RUN_FLAKY_TESTS are now used to mark flaky test mode
    
    During test run,
    - If flaky tests mode are on, only flaky tests are run
    - If flaky tests mode are off, only non-flaky tests are run
    
    Mark ctc_beam_search_decoder_op_test as flaky
    
    Reviewed By: ezyang, salexspb
    
    Differential Revision: D14468816
    
    fbshipit-source-id: dceb4a48daeb5437ad9cc714bef3343e9761f3a4

commit 1ba1ca0acb5e91c22ef6cc14d4d47a8453f1d3d7
Author: J M Dieterich <dieterich@ogolem.org>
Date:   Thu Mar 14 18:44:24 2019 -0700

    Update to ROCm2.2 (#18007)
    
    Summary:
    ROCm 2.2 was released today, if we respin the CI docker images with the attached, PyTorch/Caffe2 will support ROCm 2.2
    
    Changes necessary:
    * for the Ubuntu target, HIP PR 934 needs to be applied to fix the forceinline definition. ROCm 2.3 will contain this.
    * two unit tests proof flaky on different platforms, disable them defensively.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/18007
    
    Differential Revision: D14473903
    
    Pulled By: bddppq
    
    fbshipit-source-id: b1939f11d1c765a3bf71bb244b15f6ceb0e816d3

commit ac87488bd3dad62fb4d9d556a8e1ccc277f71aeb
Author: Jerry Zhang <jerryzh@fb.com>
Date:   Thu Mar 7 18:31:33 2019 -0800

    Change ConvPoolOp<Context>::SetOutputSize to ConvPoolOp<Context>::GetOutputSize (#17764)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/17764
    
    Original commit changeset: f1923fdca4a1
    
    reverted int8 ops fixes the original runtime regression.
    We'll ignore the memory regression since it is flaky, see D14228484
    
    Reviewed By: dzhulgakov
    
    Differential Revision: D13885233
    
    fbshipit-source-id: ccbe4b94acb44b7b4cb3ae4d73e3f6091e1e1195

commit 9b7f3da74bd782f1cc9933da5e4a03825af8e046
Author: Junjie Bai <bai@in.tum.de>
Date:   Wed Feb 13 17:12:01 2019 -0800

    Skip test_cudnn_multiple_threads_same_device on ROCm (flaky) (#17061)
    
    Summary:
    cc iotamudelta
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/10722//console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/10710//console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/10753//console
    https://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-devtoolset7-rocmrpm-centos7.5-test/1756//console
    ```
    19:07:18 ======================================================================
    19:07:18 FAIL: test_cudnn_multiple_threads_same_device (test_nn.TestNN)
    19:07:18 ----------------------------------------------------------------------
    19:07:18 Traceback (most recent call last):
    19:07:18   File "/var/lib/jenkins/workspace/test/test_nn.py", line 3905, in test_cudnn_multiple_threads_same_device
    19:07:18     (2048 - test_iters) * (2048 - test_iters))
    19:07:18   File "/var/lib/jenkins/workspace/test/common_utils.py", line 453, in assertEqual
    19:07:18     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)
    19:07:18 AssertionError: 3794704.0 not less than or equal to 1e-05 :
    ```
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/17061
    
    Differential Revision: D14069324
    
    Pulled By: bddppq
    
    fbshipit-source-id: e33b09abca217a62a8b577f9c332ea22985ef4ff

commit 10cd9d5a03f47f16ef496ddc7697b68a6701fddc
Author: peter.yeh@amd.com <peter.yeh@amd.com>
Date:   Fri Feb 1 00:50:24 2019 -0800

    Skip dag_net_forking test on Rocm (#16639)
    
    Summary:
    -Skip the test due to flaky behavior on AMD/Rocm
    -The fix is expected in Rocm 2.2 ( HSA runtime)
    bddppq
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/16639
    
    Differential Revision: D13915231
    
    Pulled By: bddppq
    
    fbshipit-source-id: 66e1d275836337170b15ceb9d60cfdd3242d4df8

commit 879bf65811ea3af43f72a734ca9dad67ce889cfb
Author: Edward Yang <ezyang@fb.com>
Date:   Wed Jan 23 11:53:55 2019 -0800

    Disable flaky test
    
    Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/16274
    
    Reviewed By: pietern
    
    Differential Revision: D13788036
    
    fbshipit-source-id: a9b7353fb0655908e6d47387cc77af33e9471aed

commit 52942e1f096de1c8503b86e433ec2805e48f842e
Author: Johannes M Dieterich <johannes.dieterich@amd.com>
Date:   Fri Dec 7 18:55:21 2018 -0800

    Enable unit tests known to work on ROCm (#14011)
    
    Summary:
    * Enable unit tests known to work on ROCm.
    * Disable a few that are known to be flaky for the time being.
    * Use std::abs for Half
    * No more special casing for ROCm in TensorMathReduce
    * Document an important detail for a hardcoded block size w.r.t. ROCm in TensorMathReduce
    
    ezyang bddppq for awareness
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/14011
    
    Differential Revision: D13387679
    
    Pulled By: bddppq
    
    fbshipit-source-id: 4177f2a57b09d866ccbb82a24318f273e3292f71

commit bb404e7a32f72374af23d8adcf3c604517ee8d71
Author: Haixin Liu <haixin@fb.com>
Date:   Fri Nov 16 19:08:49 2018 -0800

    Update atol scale in dnnlowp test (#14135)
    
    Summary:
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/14135
    
    Update atol scale of dnnlowp test. Can't reproduce the flaky test error in the task locally even after setting the same seed value, but found according to comments in check_quantized_results_close(), atol_scale should be 1/1.9=0.526315789473684, which is larger than current value 0.51. So increase the atol_scale to 0.53.
    
    Reviewed By: jspark1105
    
    Differential Revision: D13108415
    
    fbshipit-source-id: 1e8840659fdf0092f51b439cf499858795f9706a

commit 1256cbaa6936ecae4db290b3f7af1923da5ba89a
Author: Thomas Viehmann <tv.code@beamnet.de>
Date:   Fri Nov 16 11:36:06 2018 -0800

    Relax limits for gradients in test_jit's checkGraph (#14094)
    
    Summary:
    - This should help TestJit.test_lstm_fusion_concat_cuda
      to be less flaky. (Checked on manual_seed 0..99)
      Fixes: #14026
    - Revert the renaming of test_fused_abs that was introduced
      to game the order of tests to avoid the flakiness above.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/14094
    
    Differential Revision: D13100174
    
    Pulled By: soumith
    
    fbshipit-source-id: 91bb63b07a960a81dddfc0bf25c67696c0f6c46d

commit 4c06f1f2bbbc655fcbffcd16af1da38f8906c1cc
Author: Will Feng <willfeng@fb.com>
Date:   Wed Oct 31 09:29:21 2018 -0700

    CircleCI: enable all flaky tests (#13356)
    
    Summary:
    A few Caffe2 tests are currently disabled in `py2-gcc4.8-ubuntu14.04` test job because they are known to be flaky. https://github.com/pytorch/pytorch/pull/13055 likely had fixed the flakiness, and this PR tests it.
    
    Fixes https://github.com/pytorch/pytorch/issues/12395.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/13356
    
    Differential Revision: D12858206
    
    Pulled By: yf225
    
    fbshipit-source-id: 491c9c4a5c48ac1b791fdc9d78acf66091e80457

commit 70c527dacdd6ffefe4d0c6bc9179b29fa12a849d
Author: bddppq <bai@in.tum.de>
Date:   Tue Oct 16 22:48:24 2018 -0700

    Re-disable softmax ops tests in ROCM (#12749)
    
    Summary:
    They are flaky in master.
    
    ashishfarmer petrex
    
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/12749
    
    Differential Revision: D10420265
    
    Pulled By: bddppq
    
    fbshipit-source-id: cac58efb711941786b10b07ada58e0d59ab1db1d

commit cdead5ace16c105ebb230ffd97228084946b914d
Author: Will Feng <willfeng@fb.com>
Date:   Mon Oct 8 17:07:57 2018 -0700

    Enable CircleCI for Linux jobs (#12389)
    
    Summary:
    Changes in this PR:
    1. Intermediate Docker image is shared from build stage to test stage through ECR, in order to fix the Caffe2 flaky CUDA tests.
    2. There are ~7 Caffe2 operator tests that are only flaky in `caffe2_py2_gcc4_8_ubuntu14_04_test` on CPU. Disabling those tests on that config only, which is okay to do because we are still running those tests in other test jobs.
    
    After this PR is merged, CircleCI will be running on master automatically, and will be running on PRs if the author rebased their PR onto the newest master (which we will ask all the authors to do when we switch off Jenkins for Linux).
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/12389
    
    Differential Revision: D10224267
    
    Pulled By: yf225
    
    fbshipit-source-id: dd1a90a425c3d13b870d3d328cb301eee2e6e2cd

commit 7122f8b3bb7f7287cf0410a5ced2c5b120d15d30
Author: Will Feng <willfeng@fb.com>
Date:   Tue Sep 25 10:19:39 2018 -0700

    Disable more flaky tests on CircleCI (#11399)
    
    Summary:
    Fixes https://github.com/pytorch/pytorch/issues/11362.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11399
    
    Differential Revision: D9736673
    
    Pulled By: yf225
    
    fbshipit-source-id: cad8c0e86a70a01b047e648975ca5b9926e4acb3

commit daa379ffd7e684c4db02914918dcb298849ed600
Author: Edward Yang <ezyang@fb.com>
Date:   Wed Sep 12 17:30:48 2018 -0700

    Disable flaky test ObserverTest.TestMultipleNetBase (#11596)
    
    Summary:
    Tracked in https://github.com/pytorch/pytorch/issues/9137
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11596
    
    Differential Revision: D9803256
    
    Pulled By: ezyang
    
    fbshipit-source-id: 973393203ed8343a3a0feef36d34e561d9f653c4

commit 02c4cd3c8a3a4234ffbddde8d1214f0cfb209086
Author: Tongzhou Wang <tongzhou.wang.1994@gmail.com>
Date:   Wed Sep 12 14:45:52 2018 -0700

    Skip flaky distributed tests (#11594)
    
    Summary:
    context: https://github.com/pytorch/pytorch/issues/11582
    
    cc pietern The controller you requested could not be found.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11594
    
    Differential Revision: D9798871
    
    Pulled By: SsnL
    
    fbshipit-source-id: 9f9e1871c7fd9505ca898865eb8068fab4d3416d

commit 576807ce1a2291895cd1301ec6a982df21beb77f
Author: Teng Li <tengli@fb.com>
Date:   Fri Sep 7 14:02:27 2018 -0700

    flaky test fix trial (#11391)
    
    Summary:
    Add a barrier() to wait for all PG created before destroy
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11391
    
    Differential Revision: D9727383
    
    Pulled By: teng-li
    
    fbshipit-source-id: 689d62c978e642b68f4949dcf29982e34869ada4

commit 2946b021e38dd4cd4d6465b5aa4a8efbb160c2e5
Author: Edward Yang <ezyang@fb.com>
Date:   Thu Sep 6 20:39:21 2018 -0700

    Disable flaky test, see #11360 (#11361)
    
    Summary:
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11361
    
    Reviewed By: yf225
    
    Differential Revision: D9696524
    
    Pulled By: ezyang
    
    fbshipit-source-id: f6801d6f4f34090d467b16810db9cf576d5d519b

commit c9e66351a7080d11a746a0794fa833437015c1a8
Author: Will Feng <willfeng@fb.com>
Date:   Wed Sep 5 16:22:54 2018 -0700

    Port all PyTorch and Caffe2 jobs to CircleCI (#11264)
    
    Summary:
    This PR adds all PyTorch and Caffe2 job configs to CircleCI.
    
    Steps for the CircleCI mini-trial:
    - [ ] Make sure this PR passes Jenkins CI and fbcode internal tests
    - [x] Approve this PR
    - [ ] Ask CircleCI to turn up the number of build machines
    - [ ] Land this PR so that the new `.circleci/config.yml` will take effect
    
    Several Caffe2 tests are flaky on CircleCI machines and hence skipped when running on CircleCI. A proper fix for them will be worked on after a successful mini-trial.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/11264
    
    Differential Revision: D9656793
    
    Pulled By: yf225
    
    fbshipit-source-id: 7832e90018f3dff7651489c04a179d6742168fe1

commit 44b47fd7f340a61410492267242c75fd5f392da9
Author: Teng Li <tengli@fb.com>
Date:   Wed Aug 22 18:14:33 2018 -0700

    Working pybind version of MPI process group and abort() pybind (#10606)
    
    Summary:
    This will make pybind version of MPI PG work. The issue is the scope of the tensor list won't be available for the MPI worker thread. So we pass the vector by value instead.
    
    Also added recv_anysource pybind to make it work. The front-end API will wrap one level up with an int for this function. So taking a tensor should be the easiest way for now.
    
    Also added abort pybind and fixed the flaky test.
    ```
    tengli@devfair033:~/new_pytorch/pytorch/torch/lib/build/c10d/test$ mpirun -np 8 ProcessGroupMPITest
    Test successful
    Test successful
    Test successful
    Test successful
    Test successful
    Test successful
    Test successful
    Test successful
    ```
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/10606
    
    Differential Revision: D9474393
    
    Pulled By: teng-li
    
    fbshipit-source-id: cca236c333656431e87d0d3573eeae9232c598b0

commit d3ba9a173e9b97e3b8a7a5007ef231a7aea95ed8
Author: Gregory Chanan <gchanan@fb.com>
Date:   Fri Jul 27 08:59:19 2018 -0700

    Handle case where THC btrifact doesn't zero info. (#9907)
    
    Summary:
    This was showing up in the n-dimensional empty tests as flaky because it's reading uninitialized cuda memory.
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/9907
    
    Differential Revision: D9021413
    
    Pulled By: gchanan
    
    fbshipit-source-id: 31542b7597919df9afd6e528bb108a4a3e8eaf60

commit 997f46d1e12aec84e38a5d7e21b1997850a071cb
Author: Junjie Bai <bai@in.tum.de>
Date:   Wed Jul 25 21:33:00 2018 -0700

    Disable "filter too much" health check for fc operator tests (#9865)
    
    Summary:
    makes the CI flaky
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/9865
    
    Differential Revision: D9011882
    
    Pulled By: bddppq
    
    fbshipit-source-id: 5124ab97d258eed7585734d64fb01e5df98abd0d

commit 9413fabb0b0dbd572e2cc417a142458e847c5b45
Author: Edward Yang <ezyang@fb.com>
Date:   Mon Jul 16 12:56:41 2018 -0700

    Nuke TestCollectEnv (#9459)
    
    Summary:
    The tests were too flaky, and the procedure for legitimately
    updating versions of software too onerous, to warrant continually
    testing these.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    Pull Request resolved: https://github.com/pytorch/pytorch/pull/9459
    
    Reviewed By: zou3519
    
    Differential Revision: D8852357
    
    Pulled By: ezyang
    
    fbshipit-source-id: 24e99cd00b4252cdeec2a1d9af92456b4a54912a

commit f6cfd83a80c11a8cc887da96cc6f7f4ea7317cb5
Author: Pieter Noordhuis <pietern@fb.com>
Date:   Fri Jun 29 12:14:51 2018 -0700

    Find unused port for test dynamically (#9037)
    
    Summary:
    Closes https://github.com/pytorch/pytorch/pull/9037
    
    Fixes flaky test failures due to port in use.
    
    Reviewed By: soumith
    
    Differential Revision: D8696779
    
    fbshipit-source-id: a05412d1eb1dcb9a4b35023dead371aa33d62c39

commit edb88b5f3af03718b443d015f195faa1832ce95b
Author: Orion Reblitz-Richardson <orionr@gmail.com>
Date:   Tue Jun 26 14:55:48 2018 -0700

    Update from Facebook (#8887)
    
    * add opencl + fpga context
    
    adds an opencl context inside caffe2/fb which can be used for fpga access
    
    * [Caffe2] Force tensor inference checks to be triggered during testing
    
    We've started to rely on TensorInference functions more for different analysis.  This diff ensures that the TensorInference function's result matches what is expected from the definition of the operator.
    
    * Enable building //caffe2:torch with @mode/opt
    
    In @mode/opt, python runs out of a PAR, which breaks a lot of
    assumptions in the code about where templates/ folders live relative
    to __file__. Rather than introduce hacks with parutil, I simply turn
    template_path into a parameter for all the relevant functions and
    thread it through from the top level.
    
    * [Caffe2] Fix cost models for DotProduct and Div.  Update Tensor Inference for dot product
    
    As title.  DotProduct states that output is a 1-D tensor (https://caffe2.ai/docs/operators-catalogue.html#dotproduct) though code suggests it is either 0- or 1-D depending on inputs.  TensorInference defined to support implementation.
    
    * [SG-MoE] Add an option to make the experts NOT as components
    
    * [nomnigraph] Rename and fixup convertToNeuralNetOperator API
    
    This will make things a bit cleaner
    
    * no longer symlink THNN.h and THCUNN.h
    
    * forced decoder network (onnx export)
    
    Closes https://github.com/pytorch/translate/pull/95
    
    Add networks in ensemble_export.py to create a forced decoding network from PyTorch NMT checkpoints. This network takes an arbitrary numberized (source, target) pair and returns the model score for the translation, including penalties.
    
    Vocabulary reduction networks are also supported, but note that target indices which are not in the possible_translation_tokens generated for the source input will be trea
    
    * Revert schema change to fix production models
    
    Revert schema change to fix production models
    
    * MockLogDeviceReader - rebase on FIX
    
    # Goal
    
    1), Build a make_mock_log_device_reader using make_mock_reader
    
    2), Replace the real log_device_reader here: https://fburl.com/raihwf1p
    
    # Log by D8151734
    
    Real log_device_reader:
    ```
    I0529 20:29:05.373108 954994 tensor.h:839] Tensor print_net/log of type std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >. Dims: (): read_net/ParseOpenTrainingRow:0
    I0529 20:29:05.373244 954994 tensor.h:839] Tensor read_net/ParseOpenTrainin
    
    * [C2/D2][1/n]: Nonnegative-Constrained Optimization -- log barrier
    
    implement log barrier as a regularization method
    
    * Add teacher weight screening.
    
    Add teacher weight sceening according to teacher labels. If teacher label is zero, we do not use the distill loss in the objective function.
    
    * Add NormalizerContext
    
    See task for more detail. This implementation is a copy of what exists for RegularizerContext except for how the parameters are defined in the model_definition thrift file.
    
    I'll try an alternative implementation which overrides the default arguments of functions instead like for argscopes in tensorflow.
    
    https://github.com/pytorch/pytorch/compare/master...MaximeBoucher:update-from-facebook-0939578c068c?expand=1
    
    * Adding cosine similarity option in dot processor
    
    Add pairwise cosine similarity option in dot product.
    Add an option to concate dot product and cosine similarity.
    Add test cases.
    
    * [nomnigraph][redo] Concat elim for sparseNN
    
    Same as D7962948, which was reverted because Operator Schema was not
    defined
    
    * [pytorch] Revert pytorch/pytorch#7918 'Release GIL when copying to shared memory', breaks ASAN
    
    Revert this pytorch diff that breaks ASAN when running Filament in dev mode; in opt mode it gives "bad file descriptor" errors. Looks like a race when copying tensors to shared memory in multiple mp.Queue's (which spawn separate threads).
    
    https://github.com/pytorch/pytorch/pull/7918/files
    
    * [nomnigraph][mobile] Enable nomnigraph by default, use -Oz on nomnigraph related code to reduce code size
    
    enables nomnigraph and reduces codesize
    
    * [Warmup] Allow both offline incremental training and online training
    
    Change plan name on saving side and reading side to support both training type
    
    This diff depends on D8128530 and D8168651.
    
    * Revert D7802642: [Warmup] Allow both offline incremental training and online training
    
    This reverts commit afc213cf9b36cecf75333a788391c4d09f4afccc
    
    @bypass-lint
    
    An infra SEV is better than not reverting this diff.
    If you copy this password, see you in SEV Review!
    @cause_a_sev_many_files
    
    * Add legacy grad logic to fix div op on old graphs.
    
    Add legacy grad logic to fix div op on old graphs.
    
    * Correctly propagate operator failures
    
    Propagate errors from operators that throw exceptions and return false
    
    * Revert D8374829: [caffe2][nomnigraph][redo] Concat elim for sparseNN
    
    This reverts commit 6dda028c463e54bb5c32188bbbe9202107e188a5
    
    @bypass-lint
    
    An infra SEV is better than not reverting this diff.
    If you copy this password, see you in SEV Review!
    @cause_a_sev_many_files
    
    * [Caffe2] Added extra_info to core.DeviceOption(), enforced extra_info to be inherited in scope.DeviceScope
    
    extra_info is a newly defined field in DeviceOption proto. This diff added extra_info to the core.DeviceOption().  And, In scope.DeviceScope(), this diff enforce the new scope to inherit the extra_info from old scope.
    
    * [opt] hgdirsync wasn't enabled, merge diverged code
    
    Here's the damage, P59732616 basically xplat was left behind but had
    the change from assert to CAFFE_ENFORCE
    
    * OMP parallelism over RoIs for RoIAlign op
    
    Simpler to parallelize over RoIs. Shouldn't affect other uses as it relies on
    the number of OMP threads set during startup.
    
    PR: https://github.com/pytorch/pytorch/pull/8562
    
    * Use int64_t for shape in FillOps
    
    to avoid overflow of int32
    
    * Implement Rotated RoIAlign op
    
    Based on Rotated RPNs as explained in https://arxiv.org/abs/1703.01086.
    The idea is simple - orientation/angle is added as an RPN
    anchor parameter and then the angle is further regressed similar to bbox
    coords. There are some additional changes related to NMS and IoU, but besides
    that it's a direct extension to Faster-RCNN. Further details in https://fb.quip.com/sZHlA1iMfWPZ.
    
    RoIs are represented in [center_x, center_y, width, height, angle] format.
    `angle` repre
    
    * Rotated RoIAlign op CUDA forward implementation
    
    CUDA forward impl for D8415490
    
    * RoIAlignRotated op CUDA backward pass implementation
    
    TSIA
    
    * All remaining fixes to eliminate process_github.sh
    
    Most of this diff has already been reviewed separately, except for the parts relating to _thnn/utils.py and _utils._internal.py
    
    remove skipIf(True, 'Fbcode') line from process_github.sh
    
    replace sed of cpp file with #ifdef to control cudnnDestroy use
    
    undo sync-time deletion of .gitattributes, remove process_github.sh
    
    switch to using _utils._internal rather than try-import-except
    
    This diff also fixes the open-source bug where rebuilds have
    
    * Back out "Revert D7802642: [Warmup] Allow both offline incremental training and online training"
    
    Original commit changeset: 7707d2efe60e The original diff is backout becuase the online trainer package is backed out. This code would only work with new online trainer package
    
    * [easy] improve error log in adagrad op
    
    as title
    
    * re-allow use of thnn_h_path
    
    This fixes cffi usage in OSS
    
    * [4/4] [tum] paralyzing layerNorm for GPU full sync
    
    as title
    
    * add compile=False to pytorch tests, remove hack with pyc
    
    * Add shape and type inference for RowWiseArgMax operator
    
    See title
    
    * Revert D8515341: Back out "Revert D7802642: [Warmup] Allow both offline incremental training and online training"
    
    This reverts commit 78167eeef0af16b60f72c82f9dcdda9b41b4dcbd
    
    @bypass-lint
    
    An infra SEV is better than not reverting this diff.
    If you copy this password, see you in SEV Review!
    @cause_a_sev_many_files
    
    * [fix-flaky-test] mock_hive_reader_test flaky, because GlobalCounter collects local counts intervally
    
    # Problem
    
    `MockHiveReader` uses `GlobalCounter` to limit `max_examples`.
    
    GlobalCounter on server node collect local counts from worker nodes every 1 sec.
    
    This 1 sec delay makes it impossible to limit exactly to the `max_examples`, it will definitely exceed `max_examples`.
    
    # Plan
    
    Given,
    ```
    Expected num_examples = max_examples + num_examples/sec (Read Speed) x 1 sec (GlobalCounter Sync Int
    
    * [Caffe2] Fix FCGradient cost inference.  Prevent overflow in cost inference
    
    FCGradient missed a factor 2 in the `num_outputs == 3` case.  Overflow was occurring with flop calculation for FC.  Changed types to `uint64_t` to prevent future problems.
    
    * Fix binary ops with empty inputs
    
    Fix binary ops with empty inputs
    
    * Support the filling of input blob with provided data
    
    as title for Biz Integrity case
    
    * Back out "Revert D8515341: Back out "Revert D7802642: [Warmup] Allow both offline incremental training and online training""
    
    Original commit changeset: 30c55dd38816 Original diff is reverted due to introducing bad integration test. Fixed the integration test.
    
    * [c2][easy] improve pack ops error loggings
    
    as desc.
    
    * Add ShapeTypeInference for LpNorm operator
    
    As desc
    
    * Shard test_nn to reduce runtime for each test target
    
    Closes https://github.com/pytorch/pytorch/pull/8793
    
    The current test_nn would time out and be disabled in GreenWarden, and we need to have an option to split it up in order to pass the stress test. Right now GreenWarden roughly allows running 100 test cases in test_nn before timing out, and here we have an option to divide test_nn into 30 shards (with ~40 tests in each shard) to allow for some test suite growth in the future.
    
    * Change default caffe2_streams_per_gpu to 1
    
    * Remove IN_SANDCASTLE from common.py and test_nn.py
    
    We prefer to disable the failing tests through Sandcastle UI instead.
    
    * Add a new class for an updated prof_dag.proto
    
    This diff contains:
    - An updated prof_dag.proto that contains blob profiles.
    - A class to deserialize this information (serialization is in a follow up diff)
    - Update to separate profiling information from NeuralNet (and use it as part of the class above).
    - Unit tests
    
    * Lambdarank for SparseNN
    
    This diff adds a lambda_rank_layer for SparseNN.
     changes include
    1) Adds support for multi sessions in c2 op
    2) Adds support for two different loss functions in c2 op
    3) Unit tests for op
    
    * Revert D8586950: Back out "Revert D8515341: Back out "Revert D7802642: [Warmup] Allow both offline incremental training and online training""
    
    This reverts commit 012220ed63eccc35659a57b31d16a3625da6317b
    
    @bypass-lint
    
    An infra SEV is better than not reverting this diff.
    If you copy this password, see you in SEV Review!
    @cause_a_sev_many_files
    
    * [easy] A few fixups to multithread predictor benchmark
    
    (1) support perf on T6 server
    (2) remove dead code
    
    * fix a bug about the map size
    
    as title
    
    * Fix reduce sum on in-place case.
    
    Fix reduce sum on in-place case.
    
    * [Warmup] Reland reverted diff Allow both offline incremental training and online training
    
    Closes https://github.com/pytorch/pytorch/pull/8827
    
    fix net transform integration test. Allow offline and online trainer to coexist D7802642.
    
    * Add StoreHandlerNotAvailableException
    
    Add an exception for a store that is not available or has been
    deleted.
    
    * Use exception handling for fault tolerance, missing KV store
    
    Remove status blobs to communication ops so that exceptions propagate on
    failure.
    
    * [C2/D2][2/n]: Nonnegative-Constrained Optimization -- bounded grad proj
    
    for simple bounded constrained optimization, incl non-negative box constraints.
    
    * [GanH]: Adaptive Weighting with More Estimations
    
    With implemented postivity optimization, we now learn adaptive weights with different
    parameterizations.
    
    This improves parameter estimation and training stability.
    
    * Revert some changes for landing
    
    * Remove AutoNoGIL in StorageSharing
    
    * Temporarily disable net_tests
    
    * Revert "[Caffe2] Force tensor inference checks to be triggered during testing"
    
    This reverts commit 67ef05c22b2f71b4a489695384932f968384a2a4.
    
    * Revert "Fix reduce sum on in-place case."
    
    This reverts commit 6cb8a8e1b3db7b6d20941b0053e3f3836068eb64.
    
    * Revert "Revert "Fix reduce sum on in-place case.""
    
    This reverts commit 130a257c0893dc09f4bd6e6a45d112261807fd2c.

commit 40262ca9d172decf5070c79c3edc6325b7160809
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Thu Jun 21 13:32:28 2018 -0400

    Disable flaky test_lstm_fusion_cpu test (#8747)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

commit 88db4c816ea9e8025b642b2ddc5bca4bc5aa56f7
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Mon Jun 18 11:24:37 2018 -0400

    Disable flaky Chaining tests (#8601)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

commit c9b8d8566d550e46f03c922a95429bd6d31ed271
Author: Wei Yang <38509346+weiyangfb@users.noreply.github.com>
Date:   Fri Jun 15 18:20:55 2018 -0700

    Added flip() fn in ATen (CPU + CUDA) (#7873)
    
    * Spelling fix in MultivariateNormal docstring (#7915)
    
    * [c10d] MPI Process Group Implementation (#7783)
    
    This provides a bare-minimum MPI Process Group implementation, the commit is on top of @pietern's Gloo Process Group PR.
    
    * [c10d] MPI Process Group Implementation
    
    ref: https://github.com/pytorch/pytorch/issues/7434
    
    * Better exception, atexit func, and addressed comments
    
    * Clang formatting changes
    
    * Static initialization and addressed comments
    
    * Added constness back
    
    * Test will now launch mpi processes if found
    
    * CMakeList Changed
    
    * Fix Windows doc for import error (#7704)
    
    * Fix Windows doc for import error
    
    * Fix doc again
    
    * Fix wrong format
    
    * Moved condition for dilated grouped convolutions to CUDNN convolution implementation (#7465)
    
    * Updates to caffe2 operator documentation (#7917)
    
    * Significant updates to the operator docs in prep for merge
    
    * [auto] Update onnx to 307995b - Update from upstream (onnx/onnx#1038)
    https://github.com/onnx/onnx/commit/307995b1439e478122780ffc9d4e3ee8910fb7ad
    
    * Test if ASAN is actually working as part of ASAN tests. (#6050)
    
    * Test if ASAN is actually working as part of ASAN tests.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Drop explicit use of libstdc++, we should not care.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Build with DEBUG=1
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Increase main thread stack size when using ASAN.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Split up detail.h (#7836)
    
    * Fix THCUNN SpatialDepthwiseConvolution assuming contiguity (#7952)
    
    * Fix fbcode compatibility (#7939)
    
    * add test for correctness of transpose fusion (#7950)
    
    * [JIT][script] Fix emitted gather and slice for dynamic indices (#7861)
    
    * [JIT][script] Fix emitted gather for dynamic indices
    
    * Also fix slice
    
    * Address comments
    
    * cache and use BLAS_SET_BY_USER so that it doesn't set itself to TRUE when run second time (#7942)
    
    * Add unsafe flag to skip checking in prepare (#7832)
    
    * Add unsafe flag to skip checking in prepare
    
    * pop
    
    * Rename cuda::type to cuda::into_type and provide cuda::from_type. (#7937)
    
    These are used to convert Half -> half and half -> Half respectively.
    from_type will be used for runtime type checking in THC.
    
    * Try to fix TORCH_CUDA_ARCH_LIST for PyTorch again (#7936)
    
    * try again
    
    * use DEFINED
    
    * use a loop
    
    * Minor fixes
    
    *  remove sort requirement from pad-sequence (#7928)
    
    * pad-sequence no longer requires sorting entries
    
    pad-sequence can get the max_len from the list of sequences. entries only need to be sorted if output will be used for pack_padded_sequence, which can throw the error itself.
    
    * remove sort requirement from pad-sequence
    
    Picks up from #5974.
    
    Removes the requirement that input sequences to pad_sequence have to be
    sorted. Addressed the comments in the PR:
    - Updated docstring for pad_sequence
    - Remove sort requirement in pad_sequence test
    - Test unsorted and sorted sequences in pad_sequence test
    
    * Fix checkBackend error message (#7926)
    
    * Fix checkBackend error message
    
    Fixes #7849
    
    * Switch order of printing args
    
    * Split CI tests in half and run them in parallel (#7867)
    
    * Split and run tests in parallel
    
    * Refactor tests
    
    * Handling of scalars in torch.Size (#5676)
    
    * Handling of scalars in torch.Size
    
    torch.Size() constructor uses python_arg_parser
    
    IntList in python_arg_parser can take iter/range
    
    Have IntList take python iterables and ranges.
    
    Address comments: don't use python_arg_parser and instead call __index__ in THPSize_pynew
    
    Address comments
    
    Address comments
    
    * Rebased
    
    * Address nit
    
    * [JIT] Fission and fusion passes for addmm (#7938)
    
    * Addmm decomposition pass
    
    * Addmm peephole pass
    
    * Fix handling of output shape in fusion pass
    
    * Add DCE to the peephole passes
    
    * add comments
    
    * maybe bugfix?
    
    * Fix GPU tests
    
    * fix py2/3 test issue
    
    * Set smaller grain size for some cases (#7941)
    
    * Fix returning scalar input in Python autograd function (#7934)
    
    * fix _wrap_outputs not working with scalar inputs
    
    * add a test
    
    * Prevent git autocrlf for bash scripts (#7949)
    
    * Delete unused file (#7919)
    
    * Fix typo in autodiff formula for addmm (#7932)
    
    * 1) use meshgrid for flip() CPU implementation, only need one copy of input tensor; 2) changed kernel of CUDA implementation, no need materialized indices tensor; 3) reusing error checking code
    
    * [caffe2] YellowFin parameter update GPU code fix. (#6993)
    
    * [Caffe2] Keep name of caffe2_pybind11_state and caffe2_pybind11_state_gpu in debug build (#7155)
    
    * Allowing MatMul to create a gradient even with 3 inputs. useful if you are differentiating a graph twice (#6536)
    
    * added const for local variables
    
    * Fix the cpp libtorch CUDA build (#7975)
    
    * Use mingfeima's mkldnn (#7977)
    
    * Fix the import part of the windows doc (#7979)
    
    * Change perf test folder after git checkout (#7980)
    
    * Move the broadcast check in MKL Add/Sum to runtime (#7978)
    
    * Use Glog's implementation of STL logging when possible. (#7206)
    
    Inject custom workaround into namespace std so that it can be found by ADL.
    
    * [Hotfix] Bring back warnings and -Werror to ATen (#7866)
    
    * Bring back warnings and -Werror to ATen
    
    * Unbreak...
    
    * Fix tbb errors
    
    * Enable ONNX backend Mean tests (#7985)
    
    * Add third wayt to determine IS_CONDA (#7971)
    
    * Fix EmbeddingBag max_norm option (#7959)
    
    * fix EmbeddingBag max_norm option
    
    * flake8
    
    * add warning to the embedding bag arg change
    
    * Raise error when torch.load a storage on a non-existing device (#7921)
    
    * Raise error when torch.load a storage on a non-existing device
    
    Before, doing torch.load(...) on a CUDA tensor on a CPU-only machine
    would raise an unreadable error:
    
    ```
    ~/pytorch/pytorch/torch/cuda/__init__.py in __enter__(self)
        223         if self.idx is -1:
        224             return
    --> 225         self.prev_idx = torch._C._cuda_getDevice()
        226         if self.prev_idx != self.idx:
        227             torch._C._cuda_setDevice(self.idx)
    
    AttributeError: module 'torch._C' has no attribute '_cuda_getDevice'
    ```
    
    This PR makes it so that torch.load raises a hard error if one tries to
    load a storage onto a non-existing device and suggests the user to use
    torch.load's map_location feature.
    
    * Address comments
    
    * missing dep
    
    * Make THStorage / THCStorage have void* data ptr. (#7964)
    
    * Make THStorage / THCStorage have void* data ptr.
    
    This is the initial step in unifying the ATen and TH tensor representations, next is to only generate a single THStorage / THCStorage type.
    
    The major changes here are:
    1) data has been renamed to data_ptr and made void* in THStorage/THCStorage.
    2) THStorage / THCStorage stores a at::ScalarType representing its data type (This will be useful when we generate a single THStorage/THCStorage).
    3) APIs for Accessing the data as a real*:
    a) storage->data<real>() -- this does runtime-type checking (checks that the at::ScalarType is correct).
    b) storage->unsafeData<real>() -- as above, but no runtime-type checking (used in inner loops / fast code paths).
    c) THStorage_(data)(storage) -- this already existed, just calls storage->data<real>().
    
    * Add include.
    
    * Attempt to fix clang build issues.
    
    * Clarify comment and remove extra character.
    
    * Rename unsafeData -> unsafe_data.
    
    * Remove unnecessary 'to' function to get compile time rather than link time errors.
    
    * Import/export observer symbols for DLL, which fixes the linking error in Visual Studio. (#6834)
    
    * Import/export observer symbols for DLL, which fixes the linking error in Visual Studio.
    
    * Add support of all default cmake build types for release to cuda.
    
    * Remove python bindings for `torch.slice` (#7924)
    
    * skip python bindings for slice
    
    * remove tests
    
    * convert slice test to indexing
    
    * Build ONNX for PyTorch version of libcaffe2 (#7967)
    
    * support loading gzip (#6490)
    
    * support loading gzip
    
    * address comments
    
    * address comments
    
    * fix lint
    
    * fix test for python2
    
    * Add memory leak check in CUDA tests (#7270)
    
    * Add memory leak check in CUDA tests
    
    * Tracking multi-GPU too
    
    * fix run_test.py not running __name__ == '__main__' content; add test for make_cuda_memory_checked_test
    
    * add a comment
    
    * skip if cuda
    
    * 1. Change the wrapper to a method in common.py:TestCase
    2. Refactor common constants/method that initialize CUDA context into common_cuda.py
    3. Update some test files to use TEST_CUDA and TEST_MULTIGPU
    
    * Fix MaxUnpool3d forward memory leak
    
    * Fix MultiLabelMarginCriterion forward memory leak
    
    * Fix MultiMarginLoss backward memory leak
    
    * default doCUDAMemoryCheck to False
    
    * make the wrapper skip-able
    
    * use TEST_MULTIGPU
    
    * add align_corners=True/False tests for Upsample; fix TEST_CUDNN
    
    * finalize interface
    
    * VolumetricMaxUnpooling_updateOutput
    
    * fix test_nccl
    
    * rename THC caching allocator methods to be clearer
    
    * make the wrapped function a method
    
    * address comments; revert changes to aten/src/THC/THCCachingAllocator.cpp
    
    * fix renamed var
    
    * Revert "Set smaller grain size for some cases" (#7988)
    
    * Entry for c10d in CODEOWNERS (#8001)
    
    * Fix a couple of typos (#7998)
    
    * Fix typo
    
    * Fix typo
    
    * Fix typo
    
    * Fix typo
    
    *  Add on-stack observer cache for Observable (#7931)
    
    observers_list_ stores all the observers for an observable. The list is allocated on heap, which
     can cause LLC miss. Add an on-stack observer cache for fast access. In production, we have seen 20%
     speed up for start and stop observer calls.
    
    * Reduce grain size for Unary operations (#8003)
    
    * [auto] Update onnx to 8ec0e5f - Add index check for Transpose's type inference function (onnx/onnx#1053)
    https://github.com/onnx/onnx/commit/8ec0e5fe9badecb1c4cc9f136f791499f20c1377
    
    * Make AT_FORALL_SCALAR_TYPES usable outside of at::namespace. (#7935)
    
    * Make AT_FORALL_SCALAR_TYPES usable outside of at::namespace.
    
    This requires renaming the _cast functions which used the unqualified names.
    
    * Separate onnx mapping of scalar type from cast name.
    
    * Fix flake8.
    
    * Properly cast onnx.
    
    * Remove WITH_ROCM cmake flag/variable (use USE_ROCM solely) (#8013)
    
    * Mention the pytorch-ci-hud on the README. (#8004)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Re-enable build env check (#7969)
    
    * Re-enable build env check
    
    * Fix linux test error
    
    * Try to fix macOS test error
    
    * Update nn.rst (#8029)
    
    * Example for Transformed Distribution (#8011)
    
    * [auto] Update onnx to 33e9cd4 - Remove the usage of default value to fix invalid proto3 files. (onnx/onnx#1052)
    https://github.com/onnx/onnx/commit/33e9cd4182fe468675241fba4ae8a16c2f0bd82f
    
    * [auto] Update onnx to 1504a33 - Convert schema assert for duplicate type names to exception (onnx/onnx#1057)
    https://github.com/onnx/onnx/commit/1504a33abb7b1bfa773e000e2442545ce403c740
    
    * Support CUDA tensors in ProcessGroupGloo  (#7694)
    
    This adds an unconditional dependency on CUDA, which is not desirable
    for the long term. Ideally we have split like ATen where we have
    different artifacts for different backends so you can decide at runtime
    what to use.
    
    * [auto] Update onnx to 3fb9656 - Fix for fbcode CI (onnx/onnx#1062)
    https://github.com/onnx/onnx/commit/3fb965666e7fc271d093ca27529a7a1b1e103c3b
    
    * propagate nan in some activations (#8033)
    
    * propagate nan in some activations
    
    * fix py2 not having math.nan
    
    * flake8
    
    * Fix profiler crash when no events register (#8034)
    
    * Fix profiler crash when no events register
    
    When trying to profile, attempting to print the event table throws a vague error because the event list is empty:
    
    ....
    max_name_length = max(len(evt.key) for evt in events)
    ValueError: max() arg is an empty sequence
    
    This change fixes the error by returning an empty string.
    
    * Update profiler.py
    
    * Allow CI testing with different AVX configs (#8020)
    
    * Allow CI testing with different AVX configs
    
    * Unset ATEN_DISABLE_AVX and ATEN_DISABLE_AVX2 in default config
    
    * Support for generating ATen during the fbcode build, rather than committing the generated files (#8002)
    
    Paint the internal bikeshed a slightly different color to appease Buck tooling.
    
    * Factor python dependency out of interpreter (#7970)
    
    * Factor python dependency out of interpreter
    
    * Remove NO_PYTHON for the autograd engine
    
    If there is no python bindings, then a default Engine is constructed
    the first time it is requested.
    
    If the python libraries are loaded, then they override the default
    accessor and the default engine becomes a python Engine.
    
    Note: it is possible for two engines to be generated if a non-python
    one gets created before the python bindings are loaded. This case
    is rare, and just results in additional threads being spawned.
    
    * Fixing AlexNet test which is skipped in CI
    
    * [auto] Update onnx to 760c928 - add missing hasNInputShapes check for bidirectionalBroadcastShapeInference (onnx/onnx#1060)
    https://github.com/onnx/onnx/commit/760c9283d0dfdc4b8705e4fa4bd95aca68dea459
    
    * Support modules that output scalar in Gather (and data parallel) (#7973)
    
    * Support modules that output scalar in Gather (and data parallel)
    
    * Improve warning msg
    
    * [auto] Update onnx to 9e7855d - Remove PyTorch generated Upsample tests cases (onnx/onnx#1064)
    https://github.com/onnx/onnx/commit/9e7855dcd43e855e26e13a797f4b12ac9d9f2188
    
    * [script] Add support for torch.zeros, torch.ones, etc. (#7799)
    
    * [script] Add support for torch.zeros, torch.ones, etc.
    
    * modifies gen_jit_dispatch to creating bindings for functions that do
      not take tensor arguments, but do have an initial type argument
    * adds tensor attributes to these functions for device, layout, and
      dtype specification
    * extends the list of valid compiler constants to include device, layout,
      and dtype.
    * allows functions with Generators, but only using the default generator
    
    Known limitations:
    * when using `torch.float`, we convert it to a scalar tensor and make
      no checks that it is actually used only in a dtype specification.
      This is similar to how we handle Python numbers, creating some situations
      where the script is more permissive. Fixing this requires much more
      significant changes to the IR, so is lower priority for now.
    * devices specified using string literals e.g. 'cuda:1' do not work,
      since we do not support string literals in general.
    
    * Add profiling annotations to NeuralNet[Operator|Data] (#8005)
    
    * Update from facebook 1ee4edd286a3 (#8040)
    
    * Adding instance weight to batch distill loss
    
    as title
    
    * add bfloat 16-31
    
    added bfloat 16-31 and their respective unit tests
    
    * [CUDA9] Upgrade - fbcode
    
    CUDA9 upgrade diff D5654023 has been out for a while thanks to Pieter. But with time growing it's becoming quite hard to rebase, because of the symlinks and auto-generated build/config files in tp2. Break D5654023 into two diffs, one touching tp2 config files, and another one touching fbcode TARGETS file (adding nvcc flag). These two should be a bit easier to rebase (for detailed procedure see "Test Plan").
    
    This diff can only be committed if:
    1. CUDA 9 rpm is rolled out fleet-wide (TBD)
    2. NVidia driver 390.40 is rolled out fleet-wide (done)
    3. Upgrade CUDA 9.1, cudnn 7.1, nccl 2.1 (done)
    4. Make sure all dependents are built (done)
    5. Test all C2 operators, PyTorch (see test plan)
    
    * Share intermediate int32 buffer across Conv ops
    
    Adding a known type
    
    * [C2 fix] infer function for ensure_cpu_output_op
    
    this is adding the missing device funtion for ensure_cpu_output_op
    
    * [int8] Add blob serializer/deserializer for Int8TensorCPU
    
    To export to logfiledb
    
    * [nomnigraph] Add try catch block to optimization passes in predictor
    
    This will catch failures that happen in the optimization pass.
    
    * Caffe2: avoid static initialization order fiasco for CAFFE_ENFORCE
    
    CAFFE_ENFORCE uses strack trace fetcher. Which is currently a
    global static variable. If at static initialization time CAFFE_ENFORCE
    is used, this is a SIOF. Recently CAFFE_ENFORCE was added into init
    functions registration, so we started to see this.
    
    Meyers singleton is going to provide safety here. If stacktrace
    fetcher was not registered yet, it will just use a dummy one.
    
    * NUMA support in SparseNN CPU benchmark
    
    Adding support for NUMA in SparseNN CPU benchmark
    
    * [mobile-roofline] Add logging needed for roofline model
    
    This should be all that's needed
    
    * Let the operators using the same input if the operators are not chained
    
    or else, we have to change the input data dims
    
    * fix null-pointer-use UBSAN errors in in reshape_op.h
    
    * revert previous fix on input blob name
    
    as title
    
    * Adding flag to let MineHardNegative automatically extract single value from dict
    
    Model exporter requires the output of the model to be a struct. This makes it convenient to use those models directly in MineHardNegative by allow automatic extraction of the single element of dict, which is a common use case.
    
    * Reverting change that broke internal tests back to OSS compatible state
    
    * Skip CUDA memory leak test on BN tests on windows (#8043)
    
    * workaround for Sequential when one cannot retrieve python source (#8048)
    
    * [auto] Update onnx to 0dbec2a - - Generate protoc type hints on Windows (onnx/onnx#1047)
    https://github.com/onnx/onnx/commit/0dbec2a0474abcc92806d54d4dab1948674fcf74
    
    * [auto] Update onnx to 4f8ef17 - Remove erroneous documentation around maps and sequences. (onnx/onnx#1069)
    https://github.com/onnx/onnx/commit/4f8ef17ad3965e834b93d3753e54dee296aabc11
    
    * [auto] Update onnx to e6a500e - Extract constant to initializer (onnx/onnx#1050)
    https://github.com/onnx/onnx/commit/e6a500e54c50e3d300141f62958dba5f163aea4f
    
    * [auto] Update onnx to 033f956 - make gcc happy (onnx/onnx#1061)
    https://github.com/onnx/onnx/commit/033f956f41c55fd409e1c4a0d09795ae5411447f
    
    * Remove NO_PYTHON macros from Exceptions.h/cpp (#8007)
    
    Removes cases where NO_PYTHON was unnecessary in Exception.h/cpp
    
    * [ready] Clean up torch.distributions (#8046)
    
    * Have a single THStorage and THCStorage type. (#8030)
    
    No longer generate data-type specific Storage types, since all Storage types are now identical anyway.
    For (some) backwards compatibility and documentation purposes, the Real names, e.g. THLongStorage are now #defined as aliases to the single THStorage type
    
    * Reduce usages of TensorUtils<T>::DataType in THC. (#8056)
    
    TensorUtils<T> is basically ATen-dispatch-lite in that it allows one to do multi-type THC function dispatch with a single call.
    However, it is templatized on the Tensor type, and since we are moving to a single Tensor type, this doesn't work.
    
    Most of the functions in TensorUtils (e.g. getDims) can be pulled up a level, to just call THCTensor_nDimension (or directly accessing the member),
    but the DataType specific functions are more problematic.
    
    So, this PR does two things:
    1) Replaces calls of 'TensorUtils<THCTensor>::DataType' with 'real' since these are identical
    2) Templatizes the THC_pointwiseApplyX functions to take scalar types.  To ensure this is done correctly, we static_assert that the scalar type template parameter matches the scalar type of
       the corresponding template parameter.  We will need to get rid of these static_asserts in the future, but this is useful for now.
    
    * Support to run ONNX Upsample operator (mode=nearest) in Caffe2 (#8037)
    
    * Added support to run ONNX Upsample operator (mode=nearest) in Caffe2
    
    * adding error checks to upsample
    
    * adding error checks to upsample
    
    * adding error checks to upsample
    
    * changing to np.isclose
    
    * Revert onnx submodule update
    
    * still fixing
    
    * [auto] Update onnx to eb12f72 - Add conv transpose test cases (onnx/onnx#886)
    https://github.com/onnx/onnx/commit/eb12f72a8619e2fbad0d86200677fd96201d4351
    
    * [auto] Update onnx to bd98abb - Add a hook for doing post-processing on protobuf generated header files (onnx/onnx#1068)
    https://github.com/onnx/onnx/commit/bd98abbba052c1fa2dadc266dbf0d36c1b941970
    
    * Skip ConvTraspose ONNX backend tests (#8074)
    
    * Post process onnx proto (#8064)
    
    * Post processing onnx generated protobuf files to hide global symbols
    
    * .
    
    * .
    
    * Add code for TensorBoard visualization of JIT GraphExecutors (#8050)
    
    * [auto] Update onnx to cc26486 - bump version to 7 for prelu. (onnx/onnx#1063)
    https://github.com/onnx/onnx/commit/cc2648654172f0b7044f9469e6c2204c19a3ae1e
    
    * [auto] Update onnx to 356208d - add input tensor dimension checks to shape inference (onnx/onnx#1070)
    https://github.com/onnx/onnx/commit/356208d7560a3e88cabf11fddfe6fbaa748da35c
    
    * Move backtrace to its own header (#8096)
    
    * Move backtrace to its own header
    
    * Move cxxabi.h into Backtrace.cpp
    
    * Fix and ignore some warnings (#8081)
    
    * Do an additional sanity check that nvcc and CUDA include dir agree. (#8094)
    
    If you set CUDA_HOME and CUDA_NVCC_EXECUTABLE together, you may
    end up in a situation where the CUDA_VERSION of your includes
    mismatches the CUDA version of your nvcc.  See #8092 for a concrete
    case where this can occur.  Explicitly detect this situation and
    give a good error message in this case!
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * use regex in kwarg parser (#8061)
    
    * Removing remaining NO_PYTHON ifdefs (#8067)
    
    * Remove NO_PYTHON in tracing
    
    * Remove NO_PYTHON in ir.h
    
    * Remove NO_PYTHON in test_jit.cpp
    
    * Replace std::size_t with size_t (#8093)
    
    * Remove out-of-date comment (#8114)
    
    * [Caffe2] Enabling AMD GPU Backend for Caffe2 (#7955)
    
    * Add hip support for caffe2 core
    
    * Add MIOPEN header/wrapper to caffe2 core
    
    * Add HIP device into caffe2 PB
    
    * top level makefile change for rocm/hip
    
    * makefile scaffolding for AMD/RocM/HIP
    
    * Makefile scafodding for AMD/RocM/HIP; add makefile/utility for HIP files
    
    * caffe2 PB update for AMD/ROCM HIP device
    
    * Add AMD/RocM/Thrust dependency
    
    * HIP threadpool update
    
    * Fix makefile macro
    
    * makefile fix: duplicate test/binary name
    
    * makefile clean-up
    
    * makefile clean-up
    
    * add HIP operator registry
    
    * add utilities for hip device
    
    * Add USE_HIP to config summary
    
    * makefile fix for BUILD_TEST
    
    * merge latest
    
    * Fix indentation
    
    * code clean-up
    
    * Guard builds without HIP and use the same cmake script as PyTorch to find HIP
    
    * Setup rocm environment variables in build.sh (ideally should be done in the docker images)
    
    * setup locale
    
    * set HIP_PLATFORM
    
    * Revert "set HIP_PLATFORM"
    
    This reverts commit 8ec58db2b390c9259220c49fa34cd403568300ad.
    
    * continue the build script environment variables mess
    
    * HCC_AMDGPU_TARGET
    
    * Cleanup the mess, has been fixed in the lastest docker images
    
    * Assign protobuf field hip_gpu_id a new field number for backward compatibility
    
    * change name to avoid conflict
    
    * Fix duplicated thread pool flag
    
    * Refactor cmake files to not add hip includes and libs globally
    
    * Fix the wrong usage of environment variables detection in cmake
    
    * Add MIOPEN CNN operators
    
    * Revert "Add MIOPEN CNN operators"
    
    This reverts commit 6e89ad4385b5b8967a7854c4adda52c012cee42a.
    
    * Resolve merge conflicts
    
    * .
    
    * Update GetAsyncNetHIPThreadPool
    
    * Enable BUILD_CAFFE2 in pytorch build
    
    * Unifiy USE_HIP and USE_ROCM
    
    * always check USE_ROCM
    
    * .
    
    * remove unrelated change
    
    * move all core hip files to separate subdirectory
    
    * .
    
    * .
    
    * recurse glob core directory
    
    * .
    
    * correct include
    
    * .
    
    * Detect CUDNN related environment variables in cmake (#8082)
    
    * Implement adaptive softmax (#5287)
    
    * Implement adaptive softmax
    
    * fix test for python 2
    
    * add return_logprob flag
    
    * add a test for cross-entropy path
    
    * address review comments
    
    * Fix docs
    
    * pytorch 0.4 fixes
    
    * address review comments
    
    * don't use no_grad when computing log-probs
    
    * add predict method
    
    * add test for predict
    
    * change methods order
    
    * get rid of hardcoded int values
    
    * Add an optional bias term to the head of AdaptiveSoftmax
    
    * Make libshm also test if rt requires pthread. (#8112)
    
    In some configurations (e.g., our internal build of GCC 5 + GLIBC 2.23),
    -lrt is not sufficient to use shm_open; you also need to declare
    a dependency on pthread.  This patch adds a surgical extra fix to
    detect this situation, in the case that I noticed it failing in the
    wild.
    
    Fixes #8110
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * [auto] Update onnx to 2d5ce4a - Remove empty model (onnx/onnx#1058)
    https://github.com/onnx/onnx/commit/2d5ce4aeb6c485490ad567cbe610bbe1a83ac72d
    
    * Add missing pragma once. (#8118)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * [auto] Update onnx to 2a87616 - Tests for LRN operator (onnx/onnx#903)
    https://github.com/onnx/onnx/commit/2a876162ac91438cea370d75a11c9a96942e89da
    
    * Split SparseTensorImpl off from TensorImpl. (#7990)
    
    * Split SparseTensorImpl off from TensorImpl.
    
    At the moment they have the same data layout, but with the upcoming refactor
    they will not, and we need a place to put all of the sparse tensor specific
    fields.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Update SparseTensorImpl.h
    
    * [Caffe2] Support non peer access in muji and fix bug when reduced_affix is empty (#6896)
    
    * [Caffe2] Support non peer access in muji
    
    * [Caffe2] Add test for 4 gpus and 2 groups
    
    * [Caffe2] Add comments
    
    * Fix bug when reduced_affix is empty
    
    * Fix typo and add comments about cpu and amd gpu
    
    * Skip OnnxBackendNodeModelTest::test_lrn_default_cuda that causes segfault (#8127)
    
    * Replace most remaining usages of TensorUtils<T>::DataType. (#8124)
    
    As in https://github.com/pytorch/pytorch/pull/8056, this doesn't work with a single TensorImpl type.
    This replaces the usages of with a templatized parameter and static_asserts that the new and old are equal.
    
    After this we can get rid of the old template parameter, but I want to ensure they are equivalent across all builds first.
    
    * Add utf-8 header to Python file with Unicode. (#8131)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Add back lrn test (#8134)
    
    * Revert "Skip OnnxBackendNodeModelTest::test_lrn_default_cuda that causes segfault (#8127)"
    
    This reverts commit 410191c4175eaae141306cdb3c3c1c1e8a495225.
    
    * Fix mismatched default values
    
    * Add non_blocking to Tensor/Module.to (#7312)
    
    * Add non_blocking to Tensor/Module.to
    
    * flake8
    
    * Add argparse tests
    
    * cpp parse
    
    * Use C++ parser
    
    * use a commong parse function with Tensor.to
    
    * fix test_jit
    
    * use THPObjectPtr
    
    * increase refcount for None, True, and False
    
    * address comments
    
    * address comments
    
    * Fix job name checking for AVX tests (#8135)
    
    * Fix a corner case for ReShapeOp (#8142)
    
    In my use case, in the backward propogate pass, the reshape need to
    change a [0] tensor into [0,0] shaped tensor. The original implementation would
    cause out of index issue. This diff fix this problem.
    
    * cpu/ideep context converter (#8139)
    
    * fix type mismatch while call torch._C._cuda_setDevice (#8065)
    
    * fix type mismatch while call torch._C._cuda_setDevice
    
    * fix type mismatch in scatter
    
    * fix type mismatch in scatter
    
    * fix type mismatch while call torch._C._cuda_setDevice
    
    * fix type mismatch while call torch._C._cuda_setDevice
    
    * fix type mismatch while call torch._C._cuda_setDevice
    
    * docs: Add warning to torch.repeat() (#8116)
    
    * docs: Add warning to torch.repeat()
    
    closes #7993
    
    * docs: Add links for numpy functions
    
    * docs: Break the too long line
    
    * Accelerate bernoulli number generation on CPU  (#7171)
    
    * opt bernoulli rng with vsl and openmp
    
    * detect cpu vendor for bernnoulli
    
    * retrigger test platform
    
    *  check the vendor more severely
    
    * use cpuinfo to check vendor
    
    * docs: add canonical_url and fix redirect link (#8155)
    
    * docs: enable redirect link to work for each specific page
    
    * docs: add canonical_url for search engines
    
    closes #7222
    
    * docs: update redirect link to canonical_url
    
    * docstring support for @script and @script_method (#7898)
    
    * docstring support for @script and @script_method
    
    * make it python2 compatible
    
    * improve according to review
    
    * improve build_stmts
    
    * use filter instead of list comprehension
    
    * improve the way wrap is handled for script_method
    
    * stash the original method instead
    
    * allow dynamic attr for ScriptMethod and GraphExecutor
    
    * a bit comment on build_Expr
    
    * remove _build_wrap
    
    * a bit improve on comments
    
    * rename to __original_methods
    
    * should be _original_methods
    
    * [auto] Update onnx to 968d28d - fix Node::isBefore (onnx/onnx#1075)
    https://github.com/onnx/onnx/commit/968d28d901d2efdaf6d5fcfd529106762524cdfa
    
    * remove some unnecessary cudaGetDevices (#8089)
    
    * remove unnecessary cudaGetDevices
    
    * make curDevice argument non-optional, add explicit checks to current_device
    
    * Fix cuda.framework error on OSX. (#8136)
    
    When compiling OSX with CUDA, Caffe2's build system uses
    find_package(cuda) to get its grubby hands on the CUDA driver
    library (for some strange reason, FindCUDA doesn't save this
    information as a variable).  Unfortunately, on OSX, sometimes
    this picks up the cuda.framework folder, and then our build
    system chokes to death because it doesn't try to link against
    this as a framework.  (Is the folder even a framework?  I have
    no idea).
    
    This commit attempts to fix this in a two pronged fashion:
    
    1. For some users, reducing the precedence of frameworks
    using CMAKE_FIND_FRAMEWORK seems to help.  So we set these
    variables.  However, this fix is not perfect; on my laptop
    it doesn't actually solve the problem.
    
    2. PyTorch doesn't actually need the CUDA driver API.  So we
    only add the dep when building Caffe2.
    
    Fixes #8022
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * [C++ API] Improve and use OrderedDict for parameters / modules (#7823)
    
    * Improve OrderedDict for C++ API
    
    * Give OrderedDict a subject and fix review comments
    
    * Fix OrderedDict use in torch/csrc/jit/script/init.cpp
    
    * Fix __rshift__ bug (#8161)
    
    * Fix __rshift__ bug
    
    * Add small tests for __lshift__ and __rshift__ in test_cuda
    
    * Add a more elaborate check for __lshift__ and __rshift__
    
    * refactor the test to address @zou3519 's comments
    
    * Move non-generic Storage code needed by TensorUtils to non-generic C++. (#8164)
    
    For non-generic function call implementations in Storage used by TensorUtils, we do the following:
    1) Move the declaration from generic/C to non-generic/C++; we don't need backwards compatibility on these functions and want to use e.g. at::ScalarType.
    2) Move the implementation from generic/C++ to non-generic/C++.
    3) Change the generic implementation to call the non-generic implementation.
    
    This will allow us to get rid of the corresponding TensorUtils calls (once we move over the Tensor functions in the same manner).
    
    * Pinning opencv to < 3.4 in conda builds (#7923)
    
    * Pinning opencv to 3.1.0 in conda builds
    
    * Also pinning numpy to 1.11
    
    * Trying only specifying <3.4
    
    * Adding -setup- path, and better code structure (#8122)
    
    * Abstract parallelization to faciliate using threadpools (#8163)
    
    * [Caffe2] Update elementwise ops to support numpy style boradcast (#8070)
    
    * Update elementwise ops to support numpy style boradcast
    
    Update elementwise ops to support numpy style boradcast
    
    * Fix sqrt_op
    
    * Fix compare ops
    
    * Fix gradient test
    
    * Fix optimizer legacy broadcast
    
    * Fix legacy broadcast for elementwise ops
    
    * Skip flaky test
    
    * Fix eigen simple binary op
    
    * Fix attention test
    
    * Fix rnn test
    
    * Fix LSTM test
    
    * Fix tan grad
    
    * Fix schema check
    
    * Export getCudnnHandle (#7726)
    
    * [JIT] Support a single TensorList argument anywhere in the argument list + index_put (#8173)
    
    * [JIT] Support a single TensorList argument anywhere in the argument list
    
    * [JIT] index_put
    
    * use the correct datatype format (#8144)
    
    * Add back onnx console scripts dropped during migration from onnx-caffe2 (#8143)
    
    * Get rid of SOVERSION (again). (#8132)
    
    We don't want SOVERSION because pip will lose the symlink and
    double your distribution size, and also because our setup.py
    accidentally links against both libcaffe2.dylib and libcaffe2.1.dylib
    on OS X.  This leads to a very puzzling error where you get
    the error "cannot initialize CUDA without ATen_cuda", because
    there are actually two copies of your registry in memory (because
    there are two copies of the dynamic library).  Dropping SOVERSION
    makes it impossible to make this mistake.
    
    In principle, if the shared library load is done with DYLD_GLOBAL,
    that should also prevent two copies of the registry from popping up.
    Worth checking at some later point, if you need to bring back
    SOVERSION (because, e.g., pip finally fixed their software.)
    
    Partially fixes #8022.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * Fix a corner case for ReShapeOp (#8178)
    
    In my use case, in the backward propogate pass, the reshape need to
    change a [0] tensor into [0,0] shaped tensor. The original implementation would
    cause out of index issue. This diff fix this problem.
    
    * Better conv error message basing on weight shape (#8051)
    
    * Add retry logic to sccache download for Windows build (#7697)
    
    * Add retry logic to sccache download for Windows build
    
    * fix script bug
    
    * clean up
    
    * fix caffe2 docker build (#7411)
    
    * [ONNX] Fix type_as symbolic (#8183)
    
    * [ONNX] Nuke type_as symbolic
    
    * make it better
    
    * Fix lookup + test
    
    * Yangqing as an ONNX codeowner (#8185)
    
    * Fix protobuf options (#8184)
    
    * protobuf
    
    * fix protobuf_MSVC_STATIC_RUNTIME
    
    * Add a loop unrolling pass to PyTorch JIT (#7672)
    
    * [auto] Update onnx to 4e65fd8 - fuse consecutive squeezes (onnx/onnx#1078)
    https://github.com/onnx/onnx/commit/4e65fd83baaeb94fbaa050ae9df1016378157116
    
    * [Caffe2] Merging setup.py with setup_caffe2.py (#8129)
    
    * Mergine setup.pys, torch works, caffe2 works up to other KP
    
    * Fix to super call for python 2
    
    * Works on python2 on mac
    
    * Consolidating Caffe2 flags
    
    * Fix scalar check for sparse tensors. (#8197)
    
    * Fix scalar check for sparse tensors.
    
    As discovered in #8152
    
    If `t` is a scalar sparse tensor, `t._indices` used to return a sparse
    empty tensor because the scalar check was incorrect. This PR modifies
    the scalar check to return a dense tensor instead of a sparse tensor.
    
    i.e.
    ```
    tensor = torch.sparse_coo_tensor([], [], torch.Size([]), device=device)
    out = tensor._indices()  # was a sparse tensor, now is dense.
    ```
    
    * Fix typos
    
    * fix lint
    
    * Add more annotations for arguments in ATen schema (#8192)
    
    * use THCThrustAllocator in BCECriterion (#8188)
    
    * Allow parallel_apply to take in list[Tensor] (#8047)
    
    * Docs for gradcheck and gradgradcheck; expose gradgradcheck (#8166)
    
    * Docs for gradcheck and gradgradcheck; expose gradgradcheck
    
    * address comments
    
    * Implement randperm for CUDA (#7606)
    
    * Implement randperm for CUDA
    
    * Use Thrust to implement randperm
    
    * clean up
    
    * Fix test
    
    * Offload small input scenario to CPU
    
    * Fixed test
    
    * Try to fix Windows error
    
    * Fix Windows error and clean up
    
    * Use fork_rng context manager
    
    * Move test_randperm_cuda to test_cuda
    
    * Add half tensor support
    
    * Fix cuda::type error
    
    * Fix CPU offloading
    
    * Fix issues
    
    * No need to check range for n == 0 case
    
    * Update c10d build to link against Caffe2 (#8201)
    
    This follows #7399.
    
    * add wipe_cache option (#8204)
    
    as title
    
    * Replace (non-data) TensorUtils calls with non-generic THCTensor calls. (#8176)
    
    * Replace (non-data) TensorUtils calls with non-generic THCTensor calls.
    
    TensorUtils is templatized on the THTensor type, so to support a single tensor type (like ATen), we need to remove these.
    
    This PR does the following:
    1) Allows THCTensorTypeUtils.cuh to include THCTensor.hpp.
       This involves moving includes of it outside of generic/, so we can use the new implementations.
    2) Defines a single _THTensor struct and changes THCRealTensor to be a derived type of _THCTensor.
       This allows us to implement a single non-generic function and avoid static_cast or void * tricks to call it from the generic functions.
    3) For functions inside of TensorUtils that don't use data pointers:
       a) Implement the functions in (non-generic) THTensor.cpp and declare them in (non-generic) THTensor.hpp.
       b) Have the generic versions call the non-generic versions.
       c) Replace the corresponding TensorUtils<THCTensor>::fn call with (non-generic) THTensor_fn.
    
    * Add comment about THCTensor struct.
    
    * Error if storage is null in setStorageNd or resizeNd.
    
    * Fix c10d compiler warnings (#8206)
    
    Copy compiler flags from the ones used in setup.py and fix warnings.
    This makes the root build that includes c10d headers warning free.
    
    * Bump gloo submodule (#8202)
    
    This includes facebookincubator/gloo#125.
    
    * rm -rf aten/contrib (#8165)
    
    * Remove aten/contrib
    
    * Remove from CMake
    
    * Fix tanh_op on ios build (#8207)
    
    * Fix tanh_op on ios build
    
    * Fix tanh
    
    * [auto] Update onnx to f28e2f1 - fix lrn spec (onnx/onnx#1090)
    https://github.com/onnx/onnx/commit/f28e2f1a601875593af35a52888f829ba82c0598
    
    * [cmake] deprecate caffe2_* specific cuda function in cmake. (#8200)
    
    * deprecate caffe2_* specific cuda function in cmake.
    
    * ENV{} -> $ENV{}
    
    * CUDA_ARCH_NAME -> TORCH_CUDA_ARCH_LIST
    
    * .
    
    * .
    
    * .
    
    * skip CUDA memory leak check on Windows altogether (#8213)
    
    * Record shape and type in autograd to validate gradients (#8168)
    
    The check that the gradient is defined is currently disabled because
    TestJit.test_ge_optimized will trigger the error.
    
    * [auto] Update onnx to 18d70ff - Graph should only have one (input) kParam node (onnx/onnx#1088)
    https://github.com/onnx/onnx/commit/18d70ff5294953ccdf791b44ce5ccd9065584945
    
    * Set up a c10 source folder (#7822)
    
    * Set up a c10 source folder
    
    * Change the benchmark log format and also log flops (#8215)
    
    as title
    
    * Move helper functions to unnamed namespace. (#8224)
    
    Currently, the helper functions in this file are in global
    namespace. I am guessing the purpose of excluding them from was to
    keep them local.
    
    * [auto] Update onnx to e96d823 - Update Google benchmark to 1.4.1 (onnx/onnx#1083)
    https://github.com/onnx/onnx/commit/e96d823e5cc69ab02dccaba4d7971897918173c4
    
    * Change new bernoulli implementation to be fully generic. (#8218)
    
    The current implementation depends on THTensor types being unique, which is not guaranteed going forward.
    
    * Structure THTensor like THCTensor is structured. (#8217)
    
    In particular, define a base type, _THTensor, that can be used for all THRealTensor structs.
    This is just to have less cognitive load when dealing with generic THTensor/THCTensor types (as in templates).
    
    * move THCP-related utils to cuda/utils.cpp. (#8221)
    
    These files don't follow the usual pattern: In general the files torch/csrc/X torch/csrc/cuda/X
    both include the generic file torch/csrc/generic/X, where torch/csrc/X includes the cpu implementations and torch/csrc/cuda/X includes the cuda implementations.
    (Aside: this is probably not the best structure, the torch/csrc/X fiels should probably be moved to torch/csrc/cpu/X).
    
    utils.cpp combines these so that torch/csrc/utils.cpp has cuda specific code.  This makes it impossible to declare a single THTensor and THCTensor template type (i.e. THPPointer<_THTensor>, THPointer<_THCTensor>).
    
    * [READY TO MERGE] Use ccache in macOS build (#8009)
    
    * Use ccache in macOS build
    
    * Moving to sccache
    
    * Don't use sccache in test job
    
    * [NEEDS REVIEW] Add nan and inf probability check to multinomial (#7647)
    
    * Add nan and inf probs check to multinomial
    
    * fix bug
    
    * Spawn CUDA test in subprocess
    
    * Make sure invalid input won't pass the test case
    
    * Try to fix error
    
    * Test failure cases in Python 3 only
    
    * Try to fix Windows error
    
    * Move CUDA test to test_cuda.py
    
    * fix issues
    
    * fix module name error
    
    * no need to check for CUDA existence in test_cuda
    
    * Use PY3
    
    * [READY TO MERGE] Enable tests that use DataLoader with multiple workers on Windows (#6745)
    
    * Don't import TEST_CUDA for test_dataloader on Windows
    
    * test_partial_workers is stuck on Windows
    
    * Don't copy unneeded grads when using a function for several derivatives (Fixes #7722) (#7759)
    
    Trying to copy all results fails when one of them is a tensor list which
    has not been populated. This blew up for CuDNN RNNs when the weights
    did not require grad.
    
    Thanks to Sylvain Gugger for reporting!
    
    * Fix win mkldnn (#7718)
    
    * Sync build_pytorch_libs.bat with build_pytorch_libs.sh
    
    * fix quoting
    
    * add warnings
    
    * fix warnings
    
    * Add /EHa
    
    * [Caffe2] Add ADD operator for IDEEP (#8220)
    
    * Add ADD operator for IDEEP
    
    * Add boradcast check
    
    * Comments
    
    * Allow optional build and installation of native test binaries (#8225)
    
    * test finetuning
    
    * install off by default
    
    * Turn BUILD_TEST=ON for jenkins.
    
    * Turn on install_test in jenkins as well
    
    * Update MKL exporter to IDEEP ops (#8228)
    
    IDEEP exporter support
    
    * [ideep] Add IDEEP Squeeze op (#8227)
    
    Similar to MKLSqueezeOp at caffe2/mkl/operators/squeeze_op.cc
    
    * [auto] Update onnx to 62e63e9 - Fix build errors inside protobuf-bench (onnx/onnx#1084)
    https://github.com/onnx/onnx/commit/62e63e9de8f8a3bb8e30c5f7f7f87fb94364ec17
    
    * Use .cc since some downstream libraries are configured for C++ only. (#8234)
    
    * Rename SparseTensor to SparseTensorRef. (#8237)
    
    I want to introduce using SparseTensor = Tensor (as a documentary
    type alias for Tensor), but the name is already taken.
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * [caffe2] Build Android tests and binaries in CI (#7593)
    
    Update benchmark submodule to version with fixed Android/GNUSTL build
    
    * Remove core and util warnings (#8239)
    
    * Fix some signed/unsigned mismatches
    
    * Skip unused result warning
    
    * Explict fallthrough for murmur hash
    
    * Enable aligned new support to eliminate warning
    
    * Switch to int instead of unsigned in some cases
    
    * Remove .gitmodules.aten since it is in .gitmodules now (#8232)
    
    * Fix: gradcheck forced float32 (#8230)
    
    * Print requires_grad and grad_fn in string repr of tensor (#8211)
    
    For example:
    
      >>> torch.ones(3).requires_grad_()
      tensor([ 1.,  1.,  1.], requires_grad=True)
    
      >>> torch.ones(3).requires_grad_() * 5
      tensor([ 5.,  5.,  5.], grad_fn=<MulBackward0>)
    
    The suffix (dtype, requires_grad, grad_fn) wraps to a new line if
    it would cause the the line to exceed the linewidth.
    
      >>> torch.ones(10).double().requires_grad_()
      tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],
             dtype=torch.float64, requires_grad=True)
    
    * Fix TEST_CUDA import in test_cuda (#8246)
    
    * Fix lifting cat into its constant version (#8174)
    
    This fixes a bug where schema including varargs lists did not lift
    properly blocking correct ONNX export.
    
    * Don't override Tensor, Storage macros defined outside torch/csrc in t… (#8243)
    
    * Don't override Tensor, Storage macros defined outside torch/csrc in torch/csrc.
    
    This PR does the following:
    1) Removes THSTensor macros in torch/csrc, which aren't used.
    2) For macros defined outside of torch/csrc (THTensor, THTensor_, THStorage, THStorage_):
    a) No longer override them, i.e. previously THTensor could actually be THCTensor if a generic file was included from a file including THCP.h.
    b) Instead, introduce new macros THW* (e.g. THWTensor) to represent a (potentially empty) wildcard character.
    
    In addition to making this code easier to read and codemod, this allows us to more freely change TH/THC; for example:
    currently in the THC random code, the state is casted to THByteTensor*; this happens to work because the macros don't happen to override THByteTensor.
    But if THByteTensor just becomes an alias of THTensor (which is the plan for a single tensor type), then this no longer works.
    The whole thing is a bit of a mess previously because you really have to understand which macros and redefined and which aren't.
    
    We could also rename the macros that live in torch/csrc (e.g. the THPTensor macros), but since that is more self contained, I punted for now.
    
    * Don't change the plugin.
    
    * [auto] Update onnx to 3a035f4 - Add retry logic to model downloading (onnx/onnx#1077)
    https://github.com/onnx/onnx/commit/3a035f439799de3568c364f3f87014841037708e
    
    * Fully genericize THC/THCUNN (except for TensorUtils and DeviceTensorUtils). (#8251)
    
    * [cmake] Use CAFFE2_USE_* for public/cuda.cmake (#8248)
    
    * Fix app size check (#8256)
    
    Fix app size check
    
    * wip on CPU impl
    
    * Stop BCELoss from returning negative results (#8147)
    
    * Stop BCELoss from returning negative results
    
    * check explicitly for 0 before taking log
    
    * add tests
    
    * fix lint
    
    * address comments
    
    * Relax CUDA_HOME detection logic, to build when libraries are found. (#8244)
    
    Log when no cuda runtime is found, but CUDA is found
    
    * Added backward function for kl_div target (#7839)
    
    * added backward fn for target
    
    * added module test for kl_div target, and assuming targets are probabilities
    
    * Change the output format of caffe2 observers (#8261)
    
    as title
    
    * Remove TensorUtils<T>::getData, provide data<T>() in TH(C)Tensor. (#8247)
    
    * Remove TensorUtils<T>::getData, provide data<T>() in TH(C)Tensor.
    
    * Fix template parameter.
    
    * [caffe2] Move submodule onnx-tensorrt forward (#7659)
    
    Commit 82106f833dcb0070446a150e658e60ca9428f89b is essential.
    
    * [ideep] Add IDEEP fallbacks for Faster-RCNN ops (#8260)
    
    TSIA
    
    * un-genericize THCDeviceTensorUtils. (#8258)
    
    * provide data<T>() in TH(C)Tensor.
    
    * un-genericize THCDeviceTensorUtils.
    
    This is used outside of generic context, so we need to un-genericize it to have a single THCTensor type.
    
    * [caffe2] Fix ATen dispatch for ops with TensorList arg (#8226)
    
    * [cmake] Add and export Modules_CUDA_fix (#8271)
    
    * Add and export Modules_CUDA_fix
    
    * actually, need to include before finding cuda
    
    * [auto] Update onnx to 2508156 - Make error message more verbose (onnx/onnx#1097)
    https://github.com/onnx/onnx/commit/2508156135c67f2097aaac42153f641e55fd6c68
    
    * [auto] Update onnx to 39e4668 - fix optimizer does not set ir_version bug (onnx/onnx#1098)
    https://github.com/onnx/onnx/commit/39e46687eafd34c78dd59a1218171371aa3679f1
    
    * [cmake] Make cudnn optional (#8265)
    
    * Make cudnn optional
    
    * Remove cudnn file from cpu file
    
    * Move signal window functions to ATen; add Blackman window (#8130)
    
    * Move signal window functions to ATen; add Blackman window
    
    * fix cuda test not checking scipy
    
    * [ideep] Fuse Conv-Relu after IDEEP graph rewrite, skip group conv (#8233)
    
    IDEEP supports fusion for non-group conv
    
    * [c10d] NCCL Process Group implementation (#8182)
    
    * [c10d] Process Group NCCL implementation
    
    * Addressed comments
    
    * Added one missing return and clang format again
    
    * Use cmake/Modules for everything and fix gloo build
    
    * Fixed compiler warnings
    
    * Deleted duplicated FindNCCL
    
    * Set up CI build for CUDA 9.2 + macOS (#8274)
    
    * Add macOS CUDA build to CI
    
    * Fix undefined symbols issue
    
    * Use sccache for CUDA build
    
    * Fix sccache issues
    
    * clean up
    
    * c10 build setup (#8264)
    
    * Move c10/ to caffe2/dispatch/
    
    * Set up caffe2/utils directory
    
    * Remove remaining TensorTypeUtils functions. (#8286)
    
    Mostly what's remaining is copy utilities -- these are now provided in THCTensorCopy.hpp and templatized on the ScalarType rather than the TensorType.
    
    * Create initial Python bindings for c10d (#8119)
    
    * Build and install c10d from tools/build_pytorch_libs.sh
    
    * Create initial Python bindings for c10d
    
    * clang-format
    
    * Switch link order to include more symbols
    
    * Add bindings and tests for ProcessGroupGloo
    
    * Add broadcast test
    
    * Separate build flag for c10d
    
    * Explicit PIC property
    
    * Skip c10d tests if not available
    
    * Remove c10d from Windows blacklist
    
    Let it skip by itself because it won't be available anyway.
    
    * Make lint happy
    
    * Comments
    
    * Move c10d module into torch.distributed
    
    * Close tempfile such that it is deleted
    
    * Add option USE_NVRTC which defaults to off (#8289)
    
    * [build] Remove /torch/lib/THD/cmake in favor of /cmake (#7159)
    
    * Remove /torch/lib/THD/cmake in favor of /cmake
    
    * path fix
    
    * Explicitly marking gloo to use cuda
    
    * Fix gloo path in THD
    
    * Have a single THTensor / THCTensor type. (#8288)
    
    * Remove remaining TensorTypeUtils functions.
    
    Mostly what's remaining is copy utilities -- these are now provided in THCTensorCopy.hpp and templatized on the ScalarType rather than the TensorType.
    
    * Have a single THTensor / THCTensor type.
    
    As was previously done with Storages, have only a single (dtype-independent) THTensor / THCTensor.
    
    For documentation and backwards compatibility purposes, the old names, e.g. TH(Cuda)LongTensor alias the new TH(C)Tensor type.
    
    * undef GENERATE_SPARSE.
    
    * [auto] Update onnx to 58efe0a - add float16 support back for math and reduction ops (onnx/onnx#1102)
    https://github.com/onnx/onnx/commit/58efe0a9ca6228942d3f7e955babe44459343347
    
    * Some utils for compile-time programming (#7778)
    
    * Add some C++17 features, implemented with C++14
    
    * Add some type traits
    
    * Compile-time type list abstraction
    
    * Some utils for compile-time programming
    
    * Fix compatibility with a larger range of compilers
    
    * Use guts::array instead of std::array because of std::array shortcomings
    
    * code review comments
    
    * Use quotes for includes
    
    * Remove THC's FindMAGMA (#8299)
    
    * Entries for torch.distributed in CODEOWNERS (#8293)
    
    * Add depthwise convolution test for IDEEP (#8301)
    
    * Fix dividing by zero segfault in Reshape (#8302)
    
    when infer a dimension of zero size new shape
    
    * Removes unused THCTensorConv (#8229)
    
    * Replace Variables to Tensors (#8309)
    
    * Clean up old sccache log before build (#8305)
    
    * Remove unused grad ops on mobile to reduce app size (#8297)
    
    Remove unused grad ops on mobile to reduce app size
    
    * Small fixes (#8296)
    
    * [auto] Update onnx to 5ed684e - Remove/replace /MX with /WX for MSVC build. Was typo in a previous ch… (onnx/onnx#1104)
    https://github.com/onnx/onnx/commit/5ed684ebe5fd2c1fa1b79aeb7bbacf2844a6cb01
    
    * Fix sample code for cuda stream (#8319)
    
    * [auto] Update onnx to 4b4085c - Add missing warning ignoring flags to onnx_proto CMake target (onnx/onnx#1105)
    https://github.com/onnx/onnx/commit/4b4085c2e9d5a944651a2dd0dfdd20ef452bdcdf
    
    * [THD] fix broken THD build with NCCL (#8323)
    
    * Add docstring for `torch.sparse_coo_tensor` (#8152)
    
    * add sparse_coo_tensor docstring
    
    * update empty tensor example
    
    * whitespace
    
    * whitespace again
    
    * add error when backend is not supported by DDP (#8325)
    
    * Fix collect_env.py for Windows (#8326)
    
    * Fix collect_env.py for Windows
    
    * Fix expect file for Win machine
    
    * Fix the script doesn't stop eariler on error for MSVC and Ninja (#8277)
    
    * Simplify the solution
    
    * Remove the usage of set errorlevel
    
    * Skip test_multinomial_invalid_probs_cuda on Windows (#8324)
    
    * Support printing sparse tensors in ATen, fixes #8333. (#8334)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>
    
    * [C++ API] Cursors (#8190)
    
    * Add cursors to C++ API
    
    * Small self nits
    
    * s/struct/class
    
    * Use more STL like names for cursors
    
    * Implement dim_arange operator (#8266)
    
    * Implement arange_like operator
    
    * add ONNX symbolic
    
    * lint
    
    * change name
    
    * Comment the hack
    
    * 1. fixed flip CPU impl for non-continuous flip dims; 2. added more tests; 3. using TensorInfo and collapseDims to speed up CUDA impl for cases where flip dim is the 1st or last dim
    
    * nits
    
    * 1. removed for loop in pointwise CUDA kernel; 2. using templated (int64_t) IndexType for indices in pointwise CUDA kernel
    
    * added torch.flip.__doc__
    
    * nits

commit 4485ce66c293935d36745b9b11d8927aba16d64d
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Tue Jun 12 20:06:24 2018 -0400

    Fix flaky RoiAlignTest, fixes #8084. (#8312)
    
    * Fix flaky RoiAlignTest, fixes #8084.
    
    Signed-off-by: Edward Z. Yang <ezyang@cs.stanford.edu>
    
    * Increase tolerance
    
    * more...

commit 9243b64bff082307da33a418b61d27f3d2a9e449
Author: Xiaomeng Yang <bit.yangxm@gmail.com>
Date:   Tue Jun 5 15:49:16 2018 -0700

    [Caffe2] Update elementwise ops to support numpy style boradcast (#8070)
    
    * Update elementwise ops to support numpy style boradcast
    
    Update elementwise ops to support numpy style boradcast
    
    * Fix sqrt_op
    
    * Fix compare ops
    
    * Fix gradient test
    
    * Fix optimizer legacy broadcast
    
    * Fix legacy broadcast for elementwise ops
    
    * Skip flaky test
    
    * Fix eigen simple binary op
    
    * Fix attention test
    
    * Fix rnn test
    
    * Fix LSTM test
    
    * Fix tan grad
    
    * Fix schema check

commit bc4feab3e37eac576e89f2b0f3c45a87f47bf8a6
Author: bddppq <bai@in.tum.de>
Date:   Thu May 17 21:17:29 2018 -0700

    Fix flaky atomic iter test (#7649)

commit b875fb281c92dbe88da279a0ff7ad342e7b28b9e
Author: Paul Jesse Hellemn <jesse.hellemn@gmail.com>
Date:   Thu May 10 23:14:27 2018 -0700

    Update from facebook (#7451)
    
    * [bootcamp] Improve "Shape" operator to support axes specification
    
    To improve .shape operator of Caffe2 to support x.shape(tensor, axes), which takes an optional int array "axes" as input. For example, x.shape(tensor, [1, 0]) will return the dimension for axis 1 and 0 following the specified order. For current version, "axes" input allows duplications and can have arbitrary length.
    
    * Back out "Add barrier net that runs before training nets"
    
    Original commit changeset: b373fdc9c30f. Need additional changes to some callers to support barrier failures.
    
    * Change warning to verbose log to reduce log spam
    
    The `LOG(WARNING)` was a bit spammy for regular use so lets just make it a `VLOG`.
    
    * Extract the shared code from different caffe2_benchmark binaries
    
    The OSS benchmark and Internal benchmark will share most functions in the benchmark.
    
    * Support MFR in sequence training
    
    As titled.
    
    * Make knowledge distillation work with using logged prediction feature as teacher label.
    
    1) Add loading raw dense feature as teacher label.
    2) Optional calibration function for teacher label
    3) Add teacher label into generic unit test
    4) Deprecated TTSN workflow version using feature_options to config teacher label
    
    * [C2/CUDA]: unjoined cross entropy sigmoid
    
    as desc
    
    * Add async_scheduling executor into deferrable_net_exec_test
    
    Add async_scheduling into tests and fix some exception cases
    
    * Fix Event disabled error
    
    When disabling event in RNN ops make sure we don't call Finish on disabled
    event from op's RunAsync
    
    * cuda ensure cpu output op can handle both TensorCPU and TensorCUDA
    
    as desc.
    
    * [C2 Core] Infer input device option in C2 hypothesis_test checkers
    
    Improve how we default input blob device options.
    Previously it defaults as where op lives but it is not necessarily the case.
    
    For example:
    CopyCPUToGPU
    
    * [C2 Op]SplitByLengthsOp CPU/GPU implementation
    
    [C2 Op]SplitByLengthsOp CPU/GPU implementation
    
    * fix undefined symbol error
    
    not sure why we're getting undefined symbol even with link_whole = True
    Need to figure out why but need this workaround for now
    
    * Add tools in DAIPlayground platform to help debugging models
    
    Add additional tools to allow Plauground override individual method defined in AnyExp.  This will allow user to create module that specificly change certain default method behavior.  An example included in this diff is deactivating test model and checkpointing.  When debugging any model problems, switching off components helps me quickly narrow down the location of the bug.  The technique is extensively used in task T27038712 (Steady memory increase in EDPM, eventually resulting in gloo/cuda.cu:34: out of memory)
    
    * add shape and type inference for int8 conversion operator
    
    * Fix flaky test for group_norm
    
    Fix flaky test for group_norm
    
    * Fix group_norm_op_test flaky
    
    Fix group_norm_op_test flaky
    
    * Implementation of composite learning rate policy
    
    In many state-of-the-arts deep learning works, people use a simple trick to
    schedule the learning rate: use a fixed learning rate until error plateaus
    and then switch to a different fixed learning rate, and so on. In this diff,
    we implemented a simple version of the composite learning rate. The user gives
    a set of learning rates policies and corresponding iteration nums, and the
    optimizer will change the learning rate policy based on the number of iterations so far.
    
    For example, the user give two learning rate policies, one is FixedLearningRate
    and PolyLearningRate, with an iteration number of 1k. Then the first 1k iteration,
    we use FixedLearningRate. For the following iterations, we use PolyLearningRate.
    
    * Split two use cases of CachedReader into two classes, DBFileReader and CachedReader
    
    # Use Cases:
    
    1). input: DB file -> output: DatasetReader.
    
    Use DBFileReader.
    
    2). input: Reader -> build cache DB file -> output: DatasetReader.
    
    Use CachedReader.
    
    # Changes to CachedReader:
    
    1). Move db_path to the constructor.
    Because in mock reader. cache will always be built ahead.
    
    # Changes to tests:
    
    1). Make a separate TestCase class for CachedReader and DBFileReader.
    
    2). Make it possible to add more test functions by adding setUp, tearDown and _make_temp_path.
    
    3). Make delete db_path more general. `db_path` could be a file for `log_file_db`, but could also be a directory for `leveldb`.
    
    * Back out "On Mobile phones, call GlobalInit with no arguments in predictor in case we need to perform initialization"
    
    Original commit changeset: 4489c6133f11
    
    * Fix LARS bug
    
    Fixed a bug in the LARS implementation which caused all subsequent blobs not using LARS to have the LARS learning rate multiplier applied to them.
    
    * [tum] support sparse init & add uniformFill option
    
    as title
    
    * Propagate exception for async nets
    
    Capture the exception when an exception is thrown in async nets and re-throw it after wait().  This allows exceptions to be propagated up to the caller.
    
    This diff was a part of D7752068.  We split the diff so that C2 core files changes are in a separate diff.
    
    * Automatic update of fbcode/onnx to 69894f207dfcd72d1e70497d387201cec327efbc
    
    Previous import was 403ccfbd0161c38f0834413d790bad0874afbf9a
    
    Included changes:
    - **[69894f2](https://github.com/onnx/onnx/commit/69894f2)**: Use op schema.all tensor types in random like definitions (#865) <Scott McKay>
    - **[b9d6b90](https://github.com/onnx/onnx/commit/b9d6b90)**: Clarify random like operators (#846) <Scott McKay>
    - **[fc6b5fb](https://github.com/onnx/onnx/commit/fc6b5fb)**: Refactor shape inference implementation (#855) <anderspapitto>
    - **[b7d8dc8](https://github.com/onnx/onnx/commit/b7d8dc8)**: fix cmake warning message (#863) <Eric S. Yu>
    - **[f585c5d](https://github.com/onnx/onnx/commit/f585c5d)**: add pytorch-operator test for tile (#831) <Wenhao Hu>
    - **[993fe70](https://github.com/onnx/onnx/commit/993fe70)**: add install step (#832) <Eric S. Yu>
    - **[68bc26c](https://github.com/onnx/onnx/commit/68bc26c)**: add type inference for traditional ml ops except classifier ops. (#857) <Ke Zhang>
    - **[9cc0cda](https://github.com/onnx/onnx/commit/9cc0cda)**: fix string representation of scalar types (#858) <G. Ramalingam>
    - **[1078925](https://github.com/onnx/onnx/commit/1078925)**: fix y in pow test case to scalar (#852) <Wenhao Hu>
    - **[c66fb6f](https://github.com/onnx/onnx/commit/c66fb6f)**: Add some math function shape inference (#845) <anderspapitto>
    - **[ff667d1](https://github.com/onnx/onnx/commit/ff667d1)**: Refactor return type and docs for ONNXIFI_BACKEND_DIRECTX_ID (#853) <Marat Dukhan>
    - **[11c6876](https://github.com/onnx/onnx/commit/11c6876)**: clear initializer names when clear initializer (#849) <Wenhao Hu>
    - **[73c34ae](https://github.com/onnx/onnx/commit/73c34ae)**: Clarify FeatureVectorizer description. (#843) <Scott McKay>
    - **[1befb9b](https://github.com/onnx/onnx/commit/1befb9b)**: Remove useless text in docs (#850) <Lu Fang>
    - **[e84788f](https://github.com/onnx/onnx/commit/e84788f)**: Fix SELU attributes' default values (#839) <Lu Fang>
    - **[ebac046](https://github.com/onnx/onnx/commit/ebac046)**: Add tile test case (#823) <Wenhao Hu>
    - **[8b7a925](https://github.com/onnx/onnx/commit/8b7a925)**: a few more shape inference functions (#772) <anderspapitto>
    - **[9718f42](https://github.com/onnx/onnx/commit/9718f42)**: Make the coefficient non optional for LinearClassifier (#836) <Jaliya Ekanayake>
    - **[ef083d0](https://github.com/onnx/onnx/commit/ef083d0)**: Add save_tensor and load_tensor functions for Protos (#770) <Lu Fang>
    - **[45ceb55](https://github.com/onnx/onnx/commit/45ceb55)**: Check if CMAKE_BUILD_TYPE set before project(). (#812) <Sergii Dymchenko>
    - **[4b3d2b0](https://github.com/onnx/onnx/commit/4b3d2b0)**: [WIP] reenable shape inference tests (#834) <anderspapitto>
    - **[22d17ee](https://github.com/onnx/onnx/commit/22d17ee)**: RNN tests: LSTM, GRU, SimpleRNN (#739) <Peyman Manikashani>
    - **[de65b95](https://github.com/onnx/onnx/commit/de65b95)**: dimension denotation (#443) <Tian Jin>
    - **[eccc76e](https://github.com/onnx/onnx/commit/eccc76e)**: fix field number issue in onnx operator proto and enable its build (#829) <Ke Zhang>
    - **[d582beb](https://github.com/onnx/onnx/commit/d582beb)**: disable shape inference test to unbreak ci (#830) <Lu Fang>
    - **[485b787](https://github.com/onnx/onnx/commit/485b787)**: function proto for composite op. (#802) <Ke Zhang>
    - **[cd58928](https://github.com/onnx/onnx/commit/cd58928)**: specify defaults for attributes of Affine op (#820) <G. Ramalingam>
    - **[7ee2cf9](https://github.com/onnx/onnx/commit/7ee2cf9)**: merge the dummy backend back into the main one (#743) <anderspapitto>
    - **[1c03a5a](https://github.com/onnx/onnx/commit/1c03a5a)**: [Proposal] ONNX Interface for Framework Integration (previously ONNX Backend API) header and docs (#551) <Marat Dukhan>
    - **[3769a98](https://github.com/onnx/onnx/commit/3769a98)**: Rename real model test case from VGG-16 to ZFNet (#821) <Lu Fang>
    
    * [C2]ReluN Op
    
    relu n op.
    
    tf reference: https://www.tensorflow.org/api_docs/python/tf/nn/relu6
    
    * Call destructor when assigning a blob value
    
    * Add executor overrides
    
    Add executor overrides flag to enable migration to async_scheduling executor
    
    * Add barrier net that runs before training nets - attempt #2
    
    Add a synchonize barrier net that is run before training nets.  With this net, shards that are faster will wait for other shards before start training.  This reduce chances of the faster shards timing out during GLOO AllReduce.
    Removed explicit data_parallel_model.py.synchronize call in holmes workflow.
    
    This change was landed previously but caused errors for some EDPM workflows - See https://fb.facebook.com/groups/1426530000692545/permalink/1906766366002237/ - because EDPM assumes any call to CreateOrCloneCommonWorld and Gloo ops are wrapped in exception handlers but in this case exception thrown in the barrier init net is not handled.
    
    To address this issue, we add _CreateOrCloneCommonWorld to the param_init_net instead of a new barrier init net.  Since errors for param_init_net run is handled gracefully and re-rendezvous, it should fixes the problem.
    
    * Handle empty nets in async_scheduling
    
    Make sure we don't get stuck on empty nets
    
    * use CUDA_ARCH for conditional compile
    
    * [C2 fix] infer function for ensure_cpu_output_op
    
    * Update group_norm test to reduce flaky test
    
    * Fix lr_multiplier for GPU

commit 157d7499e73f791d940690f8871e8cc0e3377f90
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Fri May 4 13:23:52 2018 -0400

    Disable two flaky C++ API tests. (#7290)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

commit 6223bfdb1d3273a57b58b2a04c25c6114eaf3911
Author: Orion Reblitz-Richardson <orionr@gmail.com>
Date:   Tue Apr 17 23:36:40 2018 -0700

    Update from Facebook (#6692)
    
    * [GanH][Easy]: Add assertion to adaptive weighting layer
    
    0 weight causes numeric instability and exploding ne
    
    * [Easy] Add cast op before computing norm in diagnose options
    
    As LpNorm only takes floats we add a manual casting here.
    
    * Introduce a new caching device allocator
    
    `cudaMalloc` and `cudaFree` calls are slow, and become slower the
    more GPUs there are. Essentially, they grab a host-wide (not device-wide) lock
    because GPU memory is transparently shared across all GPUs. Normally, this
    isn't much of a concern since workloads allocate memory upfront, and reuse it
    during later computation.
    
    However, under some computation models (specifically, memory conserving
    approaches like checkpoint-and-recompute, see
    https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9)
    this assumption is no longer true. In these situations, `cudaMalloc` and
    `cudaFree` are common and frequent. Furthermore, in data parallel contexts,
    these calls happen at nearly the same time from all GPUs worsening lock
    contention.
    
    A common solution to this problem is to add a custom allocator. In fact,
    nVIDIA provides one out of the box: CUB, which Caffe2 already supports.
    Unfortunately, the CUB allocator suffers from very high fragmentation. This is
    primarily because it is a "buddy" allocator which neither splits nor merges
    free cached blocks. Study
    https://github.com/NVlabs/cub/blob/1.8.0/cub/util_allocator.cuh#L357 if you
    want to convince yourself.
    
    This diff adapts a caching allocator from the Torch codebase
    https://github.com/torch/cutorch/blob/master/lib/THC/THCCachingAllocator.cpp
    which does splitting and merging and ends up working really well, at least for
    workloads like the checkpoint-and-recompute computation models noted above.
    
    I simplified the implementation a little bit, made it a bit more C++-like. I
    also removed a bunch of stream synchronization primitives for this diff. I
    plan to add them back in subsequent diffs.
    
    * Report reader progress in fblearner workflows
    
    Integrate with fblearner progress reporting API and add support to report training progress from reader nodes.
    If reader is constructed with batch limits, report based on finished batch vs total batch. The finished batch may be more than total batch because we evaludate if we should stop processing everytime we dequeue a split.
    If no limit for the reader, report based on finished splits (Hive files) vs total splits. This is fairly accurate.
    
    * [GanH][Diagnose]: fix plotting
    
    1. ganh diagnose needs to set plot options
    2. modifier's blob name is used for metric field can need to be fixed before
    generating net
    
    * Automatic update of fbcode/onnx to 985af3f5a0f7e7d29bc0ee6b13047e7ead9c90c8
    
    * Make CompositeReader stops as soon as one reader finishes
    
    Previously, CompositeReader calls all readers before stopping. It results in flaky test since the last batch may be read by different threads; resulting in dropped data.
    
    * [dper] make sure loss is not nan
    
    as desc.
    
    * [rosetta2] [mobile-vision] Option to export NHWC order for RoIWarp/RoIAlign
    
    Thanks for finding this @stzpz and @wangyanghan. Looks like NHWC is more
    optimized. For OCR though it doesn't yet help since NHWC uses more mem b/w but
    will soon become important.
    
    * Intra-op parallel FC operator
    
    Intra-op parallel FC operator
    
    * [C2 Proto] extra info in device option
    
    passing extra information in device option
    
    design doc: https://fb.quip.com/yAiuAXkRXZGx
    
    * Unregister MKL fallbacks for NCHW conversions
    
    * Tracing for more executors
    
    Modified Tracer to work with other executors and add more tracing
    
    * Remove ShiftActivationDevices()
    
    * Check for blob entry iff it is present
    
    When processing the placeholders ops, ignore if the blob is not present in the blob_to_device.
    
    * Internalize use of eigen tensor
    
    Move use of eigen tensor out of the header file so we don't get template partial specialization errors when building other libraries.
    
    * feature importance for transformed features.
    
    * - Fix unused parameter warnings
    
    The changes in this diff comments out unused parameters.
    This will allow us to enable -Wunused-parameter as error.
    
    #accept2ship
    
    * add opencv dependencies to caffe2
    
    The video input op requires additional opencv packages. This is to add them to
    cmake so that it can build
    
    * Add clip_by_value option in gradient clipping
    
    Add clip_by_value option in gradient clipping
    
    when the value is bigger than max or smaller than min, do the clip
    
    * std::round compat

commit bc6243cb4a977323bbc6770af1a79bc5bb92ce4a
Author: costin-eseanu <19394655+costin-eseanu@users.noreply.github.com>
Date:   Tue Apr 17 00:40:58 2018 -0700

    Explicitly define all caffe2 reducer ops by name (#6513)
    
    * Explicitly define all caffe2 reducer ops by name instead of string concatenating them
    
    Explicitly define all caffe2 reducer ops by name instead of string concatenating them.
    
    * Use recursion to make the equal() function compatible with C++11.
    
    * Trivial change.
    
    * Trivial change.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Addressed @dzhulgakov's comments.
    
    * Addressed @dzhulgakov's comments.
    
    * Trivial change to force the flaky build system to rebuild.
    
    * Trivial change to force the flaky build system to rebuild.

commit 40ea24cc542ac3192809be5181d418979885ad29
Author: Edward Z. Yang <ezyang@mit.edu>
Date:   Sat Mar 17 10:40:27 2018 -0400

    Skip test_backwards_fork test as flaky. (#5839)
    
    Signed-off-by: Edward Z. Yang <ezyang@fb.com>

commit db9a700cb7498bec5fa0a94adf9e17f35358c918
Author: Soumith Chintala <soumith@gmail.com>
Date:   Thu Feb 8 09:46:22 2018 -0800

    skip flaky test

commit c261b9ce706755230b84e05080a37ea817f30a5e
Author: Huan Gui <huangui@fb.com>
Date:   Wed Jan 24 18:30:47 2018 -0800

    Fix NGram from categorical test
    
    Summary: Fix the flaky test for ngram from categorical test
    
    Reviewed By: dragonxlwang
    
    Differential Revision: D6801152
    
    fbshipit-source-id: dcbae17b1d3737a41fb2f5c794c1146a02c542bb

commit 4db89e6890e9c59663cad0c97df74edf1214854a
Author: Ravindra Rathi <ravirathi@fb.com>
Date:   Fri Jan 12 17:56:49 2018 -0800

    Check for result in queue only after background process is terminated
    
    Summary:
    Gloo test was waiting only for 10sec for processes
    to terminate causing tests to be flaky.
    
    Reviewed By: pietern
    
    Differential Revision: D6672990
    
    fbshipit-source-id: c58ba512396a0e45fa6ea4d14534ab0ccd54f2a9

commit 3d1135c842f5bc4a6e3cfcbbb5c46776c74b05a9
Author: Pieter Noordhuis <pietern@fb.com>
Date:   Fri Dec 1 09:36:51 2017 -0800

    Skip remove_padding test because it is flaky
    
    Summary:
    Must be fixed in #1547
    Closes https://github.com/caffe2/caffe2/pull/1548
    
    Reviewed By: jhcross
    
    Differential Revision: D6456373
    
    Pulled By: pietern
    
    fbshipit-source-id: 484a58e31506acfc8b8a0954f76796d14dfdfda3

commit aaa74b49298b30e107cbf7ce7b200e40e89e85ca
Author: Sam Gross <colesbury@gmail.com>
Date:   Fri Oct 6 20:05:48 2017 -0400

    Fix flaky erfinv autograd test (#3015)

commit 8a5bdc383e0e0e7dfc5128636d515f710efed1b5
Author: Bor-Yiing Su <boryiingsu@fb.com>
Date:   Fri Aug 11 18:44:44 2017 -0700

    Fixes the flaky upload test
    
    Summary:
    The LocalSession does not work with the multi-node definitions.
    The test becomes flaky because of that. The fix is to create
    different LocalSession for each Node(), and run each node
    sequentially.
    
    Differential Revision: D5617857
    
    fbshipit-source-id: a8079a90291b4c8b5aa6b471c33c06d18e59976c

commit ac76ab5fca3734a94b41006969621039e8b72387
Author: Christian Sarofeen <csarofeen@nvidia.com>
Date:   Thu Jul 27 10:59:39 2017 -0700

    Increase tol. for float tensor qr big test.
    
    test_FloatTensor_qr_big test is still a bit flaky on K80. Increasing tolerance to improve reliability as tests are moved around and results change for this test.

commit 40b783b746b4f5775c97c7fe41dfb011b545665a
Author: Ahmed Taei <ataei@fb.com>
Date:   Wed Jul 26 18:43:46 2017 -0700

    Fix flaky test due to numerical gradient approximation error.
    
    Summary:
    Use smaller step size for GradientChecks and pass seed to help reproducing the
    test from logged inputs.
    
    Reviewed By: Yangqing
    
    Differential Revision: D5505698
    
    fbshipit-source-id: fc308efe72d535695ba628944aee1913ba16b2f1

commit 3c275fe7a08411e23557107d00b358c304148e75
Author: ngimel <ngimelshein@nvidia.com>
Date:   Sat Jul 22 08:37:34 2017 -0700

    Increase flaky test tolerance (#2185)

commit 699d1ec7fb7ba690ed4ae37af328a74d4daaf839
Author: Gregory Chanan <gchanan@fb.com>
Date:   Mon Jul 3 13:12:58 2017 -0700

    Address flaky Norm test issues:
    1) Add a correction for 1.5 norms to ensure input can't be zero.
    2) Increase test tolerance.

commit d46fe736c8b0ad6b89c9bd2684d69f71a5cef45e
Author: Luke Yeager <lukeyeager@users.noreply.github.com>
Date:   Wed Jun 21 05:22:04 2017 -0700

    Fix flaky test in dataset_ops_test.py
    
    Summary:
    ```
    while pytest caffe2/python/operator_test/dataset_ops_test.py::TestDatasetOps::test_collect_tensor_ops; do sleep 0.1; done
    ```
    Run this long enough and you'll see an error like this:
    ```
    Sample histogram: [ 92 109  65 103  99 104  99 125 100 104]
    ...
    >       self.assertTrue(all(hist > 0.7 * (num_to_collect / 10)))
    E       AssertionError: False is not true
    ```
    I've seen values like 65, 68, 69, 70. Setting the cutoff at 60 instead of 70 seems safe enough.
    
    /cc Yangqing (or whoever authored https://github.com/caffe2/caffe2/commit/a56b881c4a4fd8a4e7554d0b31e6e0326d26e718).
    Closes https://github.com/caffe2/caffe2/pull/840
    
    Differential Revision: D5292120
    
    Pulled By: akyrola
    
    fbshipit-source-id: 2ea4cbb58e206268759bd9d3639e8921623f519c

commit 005156f6b4344315fce5976773247d98c61e7b0a
Author: Luke Yeager <lukeyeager@users.noreply.github.com>
Date:   Wed Jun 21 05:19:17 2017 -0700

    Fix gradient checking for softplus op
    
    Summary:
    kmatzen why did you set the stepsize in https://github.com/caffe2/caffe2/commit/ff84e7dea6e118710859d62a7207c06b87ae992e?
    
    The test is flaky before this change. Solid afterwards.
    Closes https://github.com/caffe2/caffe2/pull/841
    
    Differential Revision: D5292112
    
    Pulled By: akyrola
    
    fbshipit-source-id: c84715261194ff047606d4ec659b7f89dac3cbb1

commit 77c481c40c825a7ae161481dadee3e89a245f213
Author: Ben Zhang <benz@fb.com>
Date:   Wed Jun 7 13:22:00 2017 -0700

    Fixed flaky observerTest.TestNotifyAfterDetach
    
    Summary: Not resetting counter matters when running binary directly.
    
    Reviewed By: bwasti
    
    Differential Revision: D5202723
    
    fbshipit-source-id: 7cc5c9e4d5c6db0f79fa3950454556bc26ea4914

commit f2303ccb77f47b44f7a55807a30aa121651e30aa
Author: Aapo Kyrola <akyrola@fb.com>
Date:   Wed May 24 18:22:28 2017 -0700

    fix tileop test
    
    Summary: Gradient test for tile op was flaky because i had made the dimensions too large. This caused push blocking errors. Also I noticed my test_grad_tile was incorrect.
    
    Reviewed By: asaadaldien
    
    Differential Revision: D5126476
    
    fbshipit-source-id: ae9ce5d9041648d7a4535fc88d4013e669bd6f02

commit 0e6413f8ea5fc5e18339fdaa010966c1f3202e33
Author: Bor-Yiing Su <boryiingsu@fb.com>
Date:   Wed Mar 29 16:42:58 2017 -0700

    Fix flaky test
    
    Summary:
    Somehow the stress-runs flag does not work as what I expected.
    Now the test finally passes.
    
    Reviewed By: azzolini
    
    Differential Revision: D4797559
    
    fbshipit-source-id: 1e46844e9ae55c331c2e265a59dc550983274213

commit a03d956b56d3a3f82fd67165940151f32c2664b6
Author: Bor-Yiing Su <boryiingsu@fb.com>
Date:   Tue Mar 28 13:35:13 2017 -0700

    Fixes the flaky test. Although we create nets in three different nodes,
    
    Reviewed By: azzolini
    
    Differential Revision: D4788418
    
    fbshipit-source-id: bdf90c5674b5dbb8b3bda21cf85ea33fedb36fa6

commit 014d1fe5c4c581f7c75399f7ecde6ef64aa3c84a
Author: Luke Yeager <lukeyeager@users.noreply.github.com>
Date:   Tue Mar 14 18:12:12 2017 -0700

    Allow test discovery in caffe2/python/
    
    Summary:
    These are all essentially no-op changes which allow for nose-style (or pytest-style) test discovery.
    
    With this patch, you can use any of these methods to discover and run tests under `caffe2/python`:
    ```
    python -m unittest discover -p '*test*.py' caffe2/python/
    python -m nose caffe2/python/
    python -m pytest caffe2/python/
    ```
    
    Future work:
    
    * Get all of the tests to pass
      * Some seem to be testing operations which don't have GPU implementations
      * I get a segfault unless I set `CUDA_VISIBLE_DEVICES=0`
      * Some tests are flaky
    * Allow test discovery throughout the whole project (e.g. the `experiments/` dir)
    Closes https://github.com/caffe2/caffe2/pull/199
    
    Reviewed By: pietern
    
    Differential Revision: D4704504
    
    Pulled By: Yangqing
    
    fbshipit-source-id: 8f5687ec9c8aa873dfaff30dbf44272bc38a206b

commit 48f48b6ff2e5cd71a1e0762d9102fe4786874fa3
Author: Soumith Chintala <soumith@gmail.com>
Date:   Mon Mar 13 11:33:42 2017 -0400

    fix more flaky VolumetricMaxPooling tests

commit 7117a9012ee33f812334c21ae389942986d24a32
Author: Adam Paszke <adam.paszke@gmail.com>
Date:   Thu Feb 16 09:34:31 2017 -0800

    Fix flaky non-contig test

commit 28220134371bf9944412e6d232ca20b827a849b4
Author: Adam Paszke <adam.paszke@gmail.com>
Date:   Mon Feb 13 15:29:06 2017 -0800

    Fix flaky tests

commit f1d0d73ed72ed6e7208a5c52aa849dca94772666
Author: Adam Paszke <adam.paszke@gmail.com>
Date:   Sat Jan 28 00:45:38 2017 +0100

    Fix flaky Sqrt test

commit 8229adc4959b5c81211c820b5083be4a5fbfa6f9
Author: David Saxton <saxton@google.com>
Date:   Fri May 20 10:37:11 2016 +0100

    Fix SpatialSubSampling (was doing non-atomic writes in backprop).
    
    Also some changes to test to make it less flaky:
    - Decrease some output counts to reduce out-of-memory issues.
    - Decrease some tolerances.
    - Increase precision of random seed so that multiple test launches at
      the same time start with different seeds.

commit 0d5d16b3e6798b724565e00facfd49f77f151629
Author: Yangqing Jia <me@daggerfs.com>
Date:   Tue Feb 2 16:05:04 2016 -0800

    race condition fix: since Memcpy is now async, we will make sure that the python interface syncs before returning the content. Otherwise it makes things flaky.
